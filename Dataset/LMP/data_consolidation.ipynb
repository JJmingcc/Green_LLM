{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The following files are missing: ['SPP_LMP.csvLOUISIANA_LMP.csv', 'MISO_LMP.csv']\n",
      "Processing 8 ISO data files...\n",
      "Processing CAISO_LMP.csv...\n",
      "Added 576 rows from CAISO_LMP.csv\n",
      "Processing ERCOT_LMP.csv...\n",
      "Added 576 rows from ERCOT_LMP.csv\n",
      "Processing ILLINOIS_MISO_LMP.csv...\n",
      "Added 573 rows from ILLINOIS_MISO_LMP.csv\n",
      "Processing ISONE_LMP.csv...\n",
      "Added 575 rows from ISONE_LMP.csv\n",
      "Processing NYISO_LMP.csv...\n",
      "Added 576 rows from NYISO_LMP.csv\n",
      "Processing Ohio_LMP.csv...\n",
      "Added 575 rows from Ohio_LMP.csv\n",
      "Processing PJM_LMP.csv...\n",
      "Added 575 rows from PJM_LMP.csv\n",
      "Processing NJ_LMP.csv...\n",
      "Added 575 rows from NJ_LMP.csv\n",
      "Combined dataset has 4601 rows and 8 ISOs\n",
      "\n",
      "Time range by ISO:\n",
      "                          min_time                  max_time  count\n",
      "iso                                                                \n",
      "CAISO    2025-03-24 07:00:00+00:00 2025-03-26 06:55:00+00:00    576\n",
      "ERCOT    2025-03-24 05:00:00+00:00 2025-03-26 04:55:00+00:00    576\n",
      "ILLINOIS 2025-03-24 05:00:00+00:00 2025-03-26 04:55:00+00:00    573\n",
      "ISONE    2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    575\n",
      "NJ       2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    575\n",
      "NYISO    2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    576\n",
      "OHIO     2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    575\n",
      "PJM      2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    575\n",
      "\n",
      "Creating wide format dataset...\n",
      "Wide format dataset has 612 rows and 9 columns\n",
      "\n",
      "Files saved:\n",
      "- long_format: .\\consolidated_iso_lmp_long_20250329_154002.csv\n",
      "- wide_format: .\\consolidated_iso_lmp_wide_20250329_154002.csv\n",
      "- statistics: .\\iso_lmp_statistics_20250329_154002.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: create a function to extract the ISO name from the filename\n",
    "# Step 2: create a function to read and preprocess data from a single ISO CSV file\n",
    "# Step 3: create a function to consolidate data from multiple ISO CSV files\n",
    "# Step 4: create a function to convert the consolidated data from long to wide format\n",
    "# Step 5: create a function to save both long and wide format datasets\n",
    "\n",
    "def extract_iso_name(filename):\n",
    "    \"\"\"Extract the ISO name from the filename\"\"\"\n",
    "    match = re.match(r'([A-Z]+)_?.*\\.csv', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # Handle special cases like Ohio\n",
    "        if filename.startswith('OHIO'):\n",
    "            return 'OHIO'\n",
    "        elif filename.startswith('ILLINOIS'):\n",
    "            return 'MISO'\n",
    "        return filename.split('.')[0]\n",
    "\n",
    "def read_and_preprocess_iso_data(filepath):\n",
    "    \"\"\"Read and preprocess data from a single ISO CSV file\"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Clean up column names (remove whitespace, lowercase)\n",
    "    df.columns = [col.strip().lower() for col in df.columns]\n",
    "    \n",
    "    # Ensure critical columns are present\n",
    "    required_cols = ['interval_start_utc', 'location', 'lmp']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        raise ValueError(f\"Missing required columns: {missing} in file {filepath}\")\n",
    "    \n",
    "    # Convert timestamps to datetime\n",
    "    df['interval_start_utc'] = pd.to_datetime(df['interval_start_utc'])\n",
    "    \n",
    "    # Extract the ISO name from the filepath\n",
    "    iso_name = extract_iso_name(os.path.basename(filepath))\n",
    "    df['iso'] = iso_name\n",
    "    \n",
    "    # Select and rename columns for consistency\n",
    "    cols_to_keep = ['interval_start_utc', 'iso', 'location', 'lmp']\n",
    "    \n",
    "    # Add optional columns if they exist\n",
    "    optional_cols = ['energy', 'congestion', 'loss']\n",
    "    for col in optional_cols:\n",
    "        if col in df.columns:\n",
    "            cols_to_keep.append(col)\n",
    "    \n",
    "    # Drop rows with missing values in critical columns\n",
    "    df = df.dropna(subset=['interval_start_utc', 'location', 'lmp'])\n",
    "    \n",
    "    return df[cols_to_keep]\n",
    "\n",
    "def consolidate_iso_data(file_list):\n",
    "    \"\"\"Consolidate data from multiple ISO CSV files\"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        try:\n",
    "            print(f\"Processing {file}...\")\n",
    "            df = read_and_preprocess_iso_data(file)\n",
    "            dfs.append(df)\n",
    "            print(f\"Added {len(df)} rows from {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if not dfs:\n",
    "        raise ValueError(\"No data was successfully processed\")\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Fix ISO names\n",
    "    combined_df['iso'] = combined_df['iso'].replace({'O': 'OHIO'})\n",
    "\n",
    "\n",
    "    # Sort by timestamp and ISO\n",
    "    combined_df = combined_df.sort_values(['interval_start_utc', 'iso', 'location'])\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def create_wide_format(df):\n",
    "    \"\"\"Convert the consolidated data from long to wide format\"\"\"\n",
    "    # Create a column name by combining ISO and location\n",
    "    df['iso_location'] = df['iso'] + '_' + df['location']\n",
    "    \n",
    "    # Pivot the dataframe to get timestamps as rows and ISO_location as columns\n",
    "    pivot_df = df.pivot(index='interval_start_utc', columns='iso_location', values='lmp')\n",
    "    \n",
    "    # Reset index to make interval_start_utc a column\n",
    "    pivot_df = pivot_df.reset_index()\n",
    "    \n",
    "    return pivot_df\n",
    "\n",
    "def save_outputs(df_long, df_wide, output_dir='.'):\n",
    "    \"\"\"Save both long and wide format datasets\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp for filenames\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save long format (all data)\n",
    "    long_path = os.path.join(output_dir, f'consolidated_iso_lmp_long_{timestamp}.csv')\n",
    "    df_long.to_csv(long_path, index=False)\n",
    "    \n",
    "    # Save wide format (pivoted)\n",
    "    wide_path = os.path.join(output_dir, f'consolidated_iso_lmp_wide_{timestamp}.csv')\n",
    "    df_wide.to_csv(wide_path, index=False)\n",
    "    \n",
    "    # Save ISO summary statistics\n",
    "    iso_stats = df_long.groupby('iso')['lmp'].agg([\n",
    "        ('min', 'min'),\n",
    "        ('max', 'max'), \n",
    "        ('mean', 'mean'),\n",
    "        ('median', 'median'),\n",
    "        ('std', 'std'),\n",
    "        ('count', 'count')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    stats_path = os.path.join(output_dir, f'iso_lmp_statistics_{timestamp}.csv')\n",
    "    iso_stats.to_csv(stats_path, index=False)\n",
    "    \n",
    "    return {\n",
    "        'long_format': long_path,\n",
    "        'wide_format': wide_path,\n",
    "        'statistics': stats_path\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process ISO data files\"\"\"\n",
    "    # List of ISO files to process\n",
    "    iso_files = [\n",
    "        'CAISO_LMP.csv',\n",
    "        'ERCOT_LMP.csv',\n",
    "        'ILLINOIS_MISO_LMP.csv',\n",
    "        'ISONE_LMP.csv',\n",
    "        'NYISO_LMP.csv',\n",
    "        'Ohio_LMP.csv',\n",
    "        'PJM_LMP.csv',\n",
    "        'SPP_LMP.csv'\n",
    "        'LOUISIANA_LMP.csv',\n",
    "        'MISO_LMP.csv',\n",
    "        'NJ_LMP.csv',\n",
    "    ]\n",
    "    \n",
    "    # Verify all files exist\n",
    "    missing_files = [f for f in iso_files if not os.path.exists(f)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing: {missing_files}\")\n",
    "        iso_files = [f for f in iso_files if os.path.exists(f)]\n",
    "    \n",
    "    # Process the data\n",
    "    print(f\"Processing {len(iso_files)} ISO data files...\")\n",
    "    df_long = consolidate_iso_data(iso_files)\n",
    "    print(f\"Combined dataset has {len(df_long)} rows and {df_long['iso'].nunique()} ISOs\")\n",
    "    \n",
    "    # Generate time interval statistics\n",
    "    time_stats = df_long.groupby('iso')['interval_start_utc'].agg([\n",
    "        ('min_time', 'min'),\n",
    "        ('max_time', 'max'),\n",
    "        ('count', 'count')\n",
    "    ])\n",
    "    print(\"\\nTime range by ISO:\")\n",
    "    print(time_stats)\n",
    "    \n",
    "    # Create wide format\n",
    "    print(\"\\nCreating wide format dataset...\")\n",
    "    df_wide = create_wide_format(df_long)\n",
    "    print(f\"Wide format dataset has {len(df_wide)} rows and {len(df_wide.columns)} columns\")\n",
    "    \n",
    "    # Save outputs\n",
    "    output_paths = save_outputs(df_long, df_wide)\n",
    "    print(\"\\nFiles saved:\")\n",
    "    for format_type, path in output_paths.items():\n",
    "        print(f\"- {format_type}: {path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 ISO data files...\n",
      "Processing CAISO_LMP.csv...\n",
      "Added 576 rows from CAISO_LMP.csv\n",
      "Processing ERCOT_LMP.csv...\n",
      "Added 576 rows from ERCOT_LMP.csv\n",
      "Processing ILLINOIS_MISO_LMP.csv...\n",
      "Warning: ILLINOIS_MISO_LMP.csv contains 3 rows with missing location values\n",
      "Added 573 rows from ILLINOIS_MISO_LMP.csv\n",
      "Processing ISONE_LMP.csv...\n",
      "Warning: ISONE_LMP.csv contains 1 rows with missing location values\n",
      "Added 575 rows from ISONE_LMP.csv\n",
      "Processing NYISO_LMP.csv...\n",
      "Added 576 rows from NYISO_LMP.csv\n",
      "Processing Ohio_LMP.csv...\n",
      "Warning: Ohio_LMP.csv contains 1 rows with missing location values\n",
      "Added 575 rows from Ohio_LMP.csv\n",
      "Processing PJM_LMP.csv...\n",
      "Warning: PJM_LMP.csv contains 1 rows with missing location values\n",
      "Added 575 rows from PJM_LMP.csv\n",
      "Processing SPP_LMP.csv...\n",
      "Added 576 rows from SPP_LMP.csv\n",
      "Processing LOUISIANA_LMP.csv...\n",
      "Warning: LOUISIANA_LMP.csv contains 3 rows with missing location values\n",
      "Added 573 rows from LOUISIANA_LMP.csv\n",
      "Processing NJ_LMP.csv...\n",
      "Warning: NJ_LMP.csv contains 1 rows with missing location values\n",
      "Added 576 rows from NJ_LMP.csv\n",
      "\n",
      "Checking for missing time points in each ISO-location combination...\n",
      "  CAISO-TH_NP15_GEN-APND: Missing 36 time points out of 612\n",
      "  ERCOT-HB_HOUSTON: Missing 36 time points out of 612\n",
      "  ILLINOIS-ILLINOIS.HUB: Missing 39 time points out of 612\n",
      "  ISONE-.H.INTERNAL_HUB: Missing 37 time points out of 612\n",
      "  LOUISIANA-LOUISIANA.HUB: Missing 39 time points out of 612\n",
      "  NJ-NEW JERSEY HUB: Missing 37 time points out of 612\n",
      "  NJ-NJ_DEFAULT: Missing 611 time points out of 612\n",
      "  NYISO-CAPITL: Missing 36 time points out of 612\n",
      "  O-OHIO HUB: Missing 37 time points out of 612\n",
      "  PJM-AECO: Missing 37 time points out of 612\n",
      "  SPP-SPPSOUTH_HUB: Missing 36 time points out of 612\n",
      "Total missing time points across all ISO-location combinations: 981\n",
      "Combined dataset has 5751 rows and 10 ISOs\n",
      "\n",
      "Time range by ISO:\n",
      "                           min_time                  max_time  count\n",
      "iso                                                                 \n",
      "CAISO     2025-03-24 07:00:00+00:00 2025-03-26 06:55:00+00:00    576\n",
      "ERCOT     2025-03-24 05:00:00+00:00 2025-03-26 04:55:00+00:00    576\n",
      "ILLINOIS  2025-03-24 05:00:00+00:00 2025-03-26 04:55:00+00:00    573\n",
      "ISONE     2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    575\n",
      "LOUISIANA 2025-03-24 05:00:00+00:00 2025-03-26 04:55:00+00:00    573\n",
      "NJ        2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    576\n",
      "NYISO     2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    576\n",
      "O         2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    575\n",
      "PJM       2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    575\n",
      "SPP       2025-03-24 05:00:00+00:00 2025-03-26 04:55:00+00:00    576\n",
      "\n",
      "Creating wide format dataset with 'interpolate' fill method...\n",
      "Wide format dataset has 612 rows and 12 columns\n",
      "Data completeness in wide format: 100.00%\n",
      "\n",
      "Files saved:\n",
      "- long_format: .\\consolidated_iso_lmp_long_20250329_160704.csv\n",
      "- wide_format: .\\consolidated_iso_lmp_wide_20250329_160704.csv\n",
      "- statistics: .\\iso_lmp_statistics_20250329_160704.csv\n",
      "- completeness: .\\iso_location_completeness_20250329_160704.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_iso_name(filename):\n",
    "    \"\"\"Extract the ISO name from the filename\"\"\"\n",
    "    match = re.match(r'([A-Z]+)_?.*\\.csv', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # Handle special cases like Ohio\n",
    "        if filename.startswith('Ohio'):\n",
    "            return 'OHIO'\n",
    "        elif filename.startswith('ILLINOIS'):\n",
    "            return 'MISO'\n",
    "        elif filename.startswith('LOUISIANA'):\n",
    "            return 'LOUISIANA'\n",
    "        elif filename.startswith('NJ'):\n",
    "            return 'NJ'\n",
    "        return filename.split('.')[0]\n",
    "\n",
    "def read_and_preprocess_iso_data(filepath):\n",
    "    \"\"\"Read and preprocess data from a single ISO CSV file\"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Clean up column names (remove whitespace, lowercase)\n",
    "        df.columns = [col.strip().lower() for col in df.columns]\n",
    "        \n",
    "        # Ensure critical columns are present\n",
    "        required_cols = ['interval_start_utc', 'location', 'lmp']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols} in file {filepath}\")\n",
    "        \n",
    "        # Convert timestamps to datetime\n",
    "        df['interval_start_utc'] = pd.to_datetime(df['interval_start_utc'])\n",
    "        \n",
    "        # Extract the ISO name from the filepath\n",
    "        iso_name = extract_iso_name(os.path.basename(filepath))\n",
    "        df['iso'] = iso_name\n",
    "        \n",
    "        # Replace null location values with a default value based on the ISO\n",
    "        if df['location'].isna().any():\n",
    "            print(f\"Warning: {filepath} contains {df['location'].isna().sum()} rows with missing location values\")\n",
    "            default_location = f\"{iso_name}_DEFAULT\"\n",
    "            df['location'] = df['location'].fillna(default_location)\n",
    "        \n",
    "        # Clean location strings (remove special characters, trim whitespace)\n",
    "        df['location'] = df['location'].astype(str).str.strip()\n",
    "        \n",
    "        # Select and rename columns for consistency\n",
    "        cols_to_keep = ['interval_start_utc', 'iso', 'location', 'lmp']\n",
    "        \n",
    "        # Add optional columns if they exist\n",
    "        optional_cols = ['energy', 'congestion', 'loss']\n",
    "        for col in optional_cols:\n",
    "            if col in df.columns:\n",
    "                cols_to_keep.append(col)\n",
    "        \n",
    "        # Drop rows with missing values in critical columns (except location which we fixed)\n",
    "        df = df.dropna(subset=['interval_start_utc', 'lmp'])\n",
    "        \n",
    "        return df[cols_to_keep]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def consolidate_iso_data(file_list):\n",
    "    \"\"\"Consolidate data from multiple ISO CSV files\"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        try:\n",
    "            if not os.path.exists(file):\n",
    "                print(f\"Warning: File not found: {file}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing {file}...\")\n",
    "            df = read_and_preprocess_iso_data(file)\n",
    "            dfs.append(df)\n",
    "            print(f\"Added {len(df)} rows from {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if not dfs:\n",
    "        raise ValueError(\"No data was successfully processed\")\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Sort by timestamp and ISO\n",
    "    combined_df = combined_df.sort_values(['interval_start_utc', 'iso', 'location'])\n",
    "    \n",
    "    # Identify all unique time points and ISO-location combinations\n",
    "    all_timestamps = combined_df['interval_start_utc'].unique()\n",
    "    iso_locations = combined_df.groupby(['iso', 'location']).size().reset_index()[['iso', 'location']]\n",
    "    \n",
    "    # Check for missing time points in each ISO-location combination\n",
    "    print(\"\\nChecking for missing time points in each ISO-location combination...\")\n",
    "    missing_count = 0\n",
    "    \n",
    "    for _, row in iso_locations.iterrows():\n",
    "        iso = row['iso']\n",
    "        location = row['location']\n",
    "        \n",
    "        # Get timestamps for this ISO-location\n",
    "        location_data = combined_df[(combined_df['iso'] == iso) & (combined_df['location'] == location)]\n",
    "        location_times = set(location_data['interval_start_utc'])\n",
    "        \n",
    "        # Find missing timestamps\n",
    "        missing_times = set(all_timestamps) - location_times\n",
    "        \n",
    "        if missing_times:\n",
    "            missing_count += len(missing_times)\n",
    "            print(f\"  {iso}-{location}: Missing {len(missing_times)} time points out of {len(all_timestamps)}\")\n",
    "    \n",
    "    print(f\"Total missing time points across all ISO-location combinations: {missing_count}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def create_wide_format(df, fill_method='interpolate'):\n",
    "    \"\"\"\n",
    "    Convert the consolidated data from long to wide format\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Consolidated data in long format\n",
    "    fill_method : str\n",
    "        Method to fill missing values: 'interpolate', 'forward', 'backward', or 'none'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Data in wide format with timestamps as rows and ISO_location as columns\n",
    "    \"\"\"\n",
    "    # Create a column name by combining ISO and location\n",
    "    df['iso_location'] = df['iso'] + '_' + df['location']\n",
    "    \n",
    "    # Pivot the dataframe to get timestamps as rows and ISO_location as columns\n",
    "    pivot_df = df.pivot(index='interval_start_utc', columns='iso_location', values='lmp')\n",
    "    \n",
    "    # Fill missing values based on the specified method\n",
    "    if fill_method == 'interpolate':\n",
    "        # Use linear interpolation to fill gaps\n",
    "        pivot_df = pivot_df.interpolate(method='linear')\n",
    "        \n",
    "        # For any remaining NaNs at the beginning/end, use forward and backward fill\n",
    "        pivot_df = pivot_df.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "    elif fill_method == 'forward':\n",
    "        # Use forward fill (last observation carried forward)\n",
    "        pivot_df = pivot_df.fillna(method='ffill')\n",
    "        \n",
    "    elif fill_method == 'backward':\n",
    "        # Use backward fill (next observation carried backward)\n",
    "        pivot_df = pivot_df.fillna(method='bfill')\n",
    "    \n",
    "    # Reset index to make interval_start_utc a column\n",
    "    pivot_df = pivot_df.reset_index()\n",
    "    \n",
    "    # Count how many missing values remain after filling\n",
    "    missing_values = pivot_df.isna().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Warning: {missing_values} missing values remain in the wide format dataset after applying {fill_method} fill\")\n",
    "    \n",
    "    return pivot_df\n",
    "\n",
    "def save_outputs(df_long, df_wide, output_dir='.'):\n",
    "    \"\"\"Save both long and wide format datasets\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp for filenames\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save long format (all data)\n",
    "    long_path = os.path.join(output_dir, f'consolidated_iso_lmp_long_{timestamp}.csv')\n",
    "    df_long.to_csv(long_path, index=False)\n",
    "    \n",
    "    # Save wide format (pivoted)\n",
    "    wide_path = os.path.join(output_dir, f'consolidated_iso_lmp_wide_{timestamp}.csv')\n",
    "    df_wide.to_csv(wide_path, index=False)\n",
    "    \n",
    "    # Save ISO summary statistics\n",
    "    iso_stats = df_long.groupby('iso')['lmp'].agg([\n",
    "        ('min', 'min'),\n",
    "        ('max', 'max'), \n",
    "        ('mean', 'mean'),\n",
    "        ('median', 'median'),\n",
    "        ('std', 'std'),\n",
    "        ('count', 'count')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    stats_path = os.path.join(output_dir, f'iso_lmp_statistics_{timestamp}.csv')\n",
    "    iso_stats.to_csv(stats_path, index=False)\n",
    "    \n",
    "    # Save ISO-Location completeness report\n",
    "    completeness_df = df_long.groupby(['iso', 'location']).agg(\n",
    "        time_points=('interval_start_utc', 'count'),\n",
    "        min_time=('interval_start_utc', 'min'),\n",
    "        max_time=('interval_start_utc', 'max'),\n",
    "        min_lmp=('lmp', 'min'),\n",
    "        max_lmp=('lmp', 'max'),\n",
    "        avg_lmp=('lmp', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate expected number of time points (from min to max time with 5-min intervals)\n",
    "    for idx, row in completeness_df.iterrows():\n",
    "        min_time = row['min_time']\n",
    "        max_time = row['max_time']\n",
    "        \n",
    "        # Calculate time difference in minutes and divide by 5 to get expected number of intervals\n",
    "        time_diff_minutes = (max_time - min_time).total_seconds() / 60\n",
    "        expected_points = time_diff_minutes / 5 + 1  # +1 because we include both endpoints\n",
    "        \n",
    "        completeness_df.loc[idx, 'expected_points'] = expected_points\n",
    "        completeness_df.loc[idx, 'completeness_pct'] = min(100, (row['time_points'] / expected_points) * 100)\n",
    "    \n",
    "    completeness_path = os.path.join(output_dir, f'iso_location_completeness_{timestamp}.csv')\n",
    "    completeness_df.to_csv(completeness_path, index=False)\n",
    "    \n",
    "    return {\n",
    "        'long_format': long_path,\n",
    "        'wide_format': wide_path,\n",
    "        'statistics': stats_path,\n",
    "        'completeness': completeness_path\n",
    "    }\n",
    "\n",
    "def main(fill_method='interpolate', output_dir='.'):\n",
    "    \"\"\"\n",
    "    Main function to process ISO data files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fill_method : str\n",
    "        Method to fill missing values in wide format: 'interpolate', 'forward', 'backward', or 'none'\n",
    "    output_dir : str\n",
    "        Directory to save output files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (df_long, df_wide) DataFrames containing the consolidated data\n",
    "    \"\"\"\n",
    "    # List of ISO files to process\n",
    "    iso_files = [\n",
    "        'CAISO_LMP.csv',\n",
    "        'ERCOT_LMP.csv',\n",
    "        'ILLINOIS_MISO_LMP.csv',\n",
    "        'ISONE_LMP.csv',\n",
    "        'NYISO_LMP.csv',\n",
    "        'Ohio_LMP.csv',\n",
    "        'PJM_LMP.csv',\n",
    "        'SPP_LMP.csv',\n",
    "        'LOUISIANA_LMP.csv',\n",
    "        'NJ_LMP.csv'\n",
    "    ]\n",
    "    \n",
    "    # Verify files exist and report missing files\n",
    "    missing_files = [f for f in iso_files if not os.path.exists(f)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing: {missing_files}\")\n",
    "        iso_files = [f for f in iso_files if os.path.exists(f)]\n",
    "        \n",
    "    if not iso_files:\n",
    "        raise ValueError(\"No ISO data files found to process\")\n",
    "    \n",
    "    # Process the data\n",
    "    print(f\"Processing {len(iso_files)} ISO data files...\")\n",
    "    df_long = consolidate_iso_data(iso_files)\n",
    "    print(f\"Combined dataset has {len(df_long)} rows and {df_long['iso'].nunique()} ISOs\")\n",
    "    \n",
    "    # Generate time interval statistics\n",
    "    time_stats = df_long.groupby('iso')['interval_start_utc'].agg([\n",
    "        ('min_time', 'min'),\n",
    "        ('max_time', 'max'),\n",
    "        ('count', 'count')\n",
    "    ])\n",
    "    print(\"\\nTime range by ISO:\")\n",
    "    print(time_stats)\n",
    "    \n",
    "    # Check for duplicate records\n",
    "    duplicate_check = df_long.duplicated(subset=['interval_start_utc', 'iso', 'location']).sum()\n",
    "    if duplicate_check > 0:\n",
    "        print(f\"\\nWarning: Found {duplicate_check} duplicate records. Removing duplicates...\")\n",
    "        df_long = df_long.drop_duplicates(subset=['interval_start_utc', 'iso', 'location'])\n",
    "    \n",
    "    # Create wide format\n",
    "    print(f\"\\nCreating wide format dataset with '{fill_method}' fill method...\")\n",
    "    df_wide = create_wide_format(df_long, fill_method=fill_method)\n",
    "    print(f\"Wide format dataset has {len(df_wide)} rows and {len(df_wide.columns)} columns\")\n",
    "    \n",
    "    # Check data completeness in wide format\n",
    "    missing_pct = (df_wide.isnull().sum().sum() / (df_wide.shape[0] * df_wide.shape[1])) * 100\n",
    "    print(f\"Data completeness in wide format: {100 - missing_pct:.2f}%\")\n",
    "    \n",
    "    # Save outputs\n",
    "    output_paths = save_outputs(df_long, df_wide, output_dir)\n",
    "    print(\"\\nFiles saved:\")\n",
    "    for format_type, path in output_paths.items():\n",
    "        print(f\"- {format_type}: {path}\")\n",
    "    \n",
    "    return df_long, df_wide\n",
    "\n",
    "def run_from_notebook(iso_files=None, fill_method='interpolate', output_dir='.'):\n",
    "    \"\"\"\n",
    "    Function to run data consolidation from a Jupyter notebook\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    iso_files : list\n",
    "        List of ISO data files to process. If None, use default list\n",
    "    fill_method : str\n",
    "        Method to fill missing values in wide format: 'interpolate', 'forward', 'backward', or 'none'\n",
    "    output_dir : str\n",
    "        Directory to save output files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (df_long, df_wide) DataFrames containing the consolidated data\n",
    "    \"\"\"\n",
    "    if iso_files is None:\n",
    "        # Default ISO files list\n",
    "        iso_files = [\n",
    "            'CAISO_LMP.csv',\n",
    "            'ERCOT_LMP.csv',\n",
    "            'ILLINOIS_MISO_LMP.csv',\n",
    "            'ISONE_LMP.csv',\n",
    "            'NYISO_LMP.csv',\n",
    "            'Ohio_LMP.csv',\n",
    "            'PJM_LMP.csv',\n",
    "            'SPP_LMP.csv',\n",
    "            'LOUISIANA_LMP.csv',\n",
    "            'NJ_LMP.csv'\n",
    "        ]\n",
    "    \n",
    "    # Filter to only existing files\n",
    "    existing_files = [f for f in iso_files if os.path.exists(f)]\n",
    "    if not existing_files:\n",
    "        raise ValueError(\"No specified ISO data files found\")\n",
    "    \n",
    "    # Process the data\n",
    "    print(f\"Processing {len(existing_files)} ISO data files...\")\n",
    "    df_long = consolidate_iso_data(existing_files)\n",
    "    \n",
    "    # Create wide format\n",
    "    df_wide = create_wide_format(df_long, fill_method=fill_method)\n",
    "    \n",
    "    # Save outputs\n",
    "    output_paths = save_outputs(df_long, df_wide, output_dir)\n",
    "    \n",
    "    return df_long, df_wide\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    import sys\n",
    "    \n",
    "      # Set up command line argument parsing\n",
    "    parser = argparse.ArgumentParser(description='Process ISO LMP data files and create consolidated datasets')\n",
    "    parser.add_argument('--fill', choices=['interpolate', 'forward', 'backward', 'none'], \n",
    "                        default='interpolate', help='Method to fill missing values in the wide format')\n",
    "    parser.add_argument('--output', type=str, default='.', \n",
    "                        help='Directory to save output files')\n",
    "    parser.add_argument('--files', nargs='+', type=str, \n",
    "                        help='Specific ISO files to process (optional, uses default list if not provided)')\n",
    "    \n",
    "    try:\n",
    "        # When running in Jupyter, ignore sys.argv and use empty args\n",
    "        if 'ipykernel_launcher' in sys.argv[0]:\n",
    "            args = parser.parse_args([])\n",
    "        else:\n",
    "            # If run as a standalone script with command line arguments\n",
    "            args = parser.parse_args()\n",
    "            \n",
    "        # Process specific files if provided\n",
    "        if hasattr(args, 'files') and args.files:\n",
    "            df_long, df_wide = run_from_notebook(args.files, args.fill, args.output)\n",
    "        else:\n",
    "            # Use the main function with default file list\n",
    "            df_long, df_wide = main(fill_method=args.fill, output_dir=args.output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        # For environments where argparse might not be available\n",
    "        print(\"Running with default parameters\")\n",
    "        df_long, df_wide = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data center location: \n",
    "    # \\datacenter_info = {\n",
    "    #     0: {\"name\": \"us-central1\", \"location\": \"Iowa\", \"renewable\": 0.95, \"co2\": 430},\n",
    "    #     1: {\"name\": \"us-east1\", \"location\": \"South Carolina\", \"renewable\": 0.29, \"co2\": 560},\n",
    "    #     2: {\"name\": \"us-east4\", \"location\": \"Northern Virginia\", \"renewable\": 0.52, \"co2\": 322},\n",
    "    #     3: {\"name\": \"us-east5\", \"location\": \"Columbus\", \"renewable\": 0.52, \"co2\": 322},\n",
    "    #     4: {\"name\": \"us-south1\", \"location\": \"Dallas\", \"renewable\": 0.79, \"co2\": 321},\n",
    "    #     5: {\"name\": \"us-west1\", \"location\": \"Oregon\", \"renewable\": 0.84, \"co2\": 94},\n",
    "    #     6: {\"name\": \"us-west2\", \"location\": \"Los Angeles\", \"renewable\": 0.55, \"co2\": 198},\n",
    "    #     7: {\"name\": \"us-west3\", \"location\": \"Salt Lake City\", \"renewable\": 0.29, \"co2\": 588},\n",
    "    #     8: {\"name\": \"us-west4\", \"location\": \"Las Vegas\", \"renewable\": 0.26, \"co2\": 373}\n",
    "    # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Cloud Datacenter to ISO Region Mapping:\n",
      "--------------------------------------------------------------------------------\n",
      "ID   Name            Location             ISO Region      Price File           Match Type     \n",
      "--------------------------------------------------------------------------------\n",
      "0    us-central1     Iowa                 MISO            ILLINOIS_MISO_LMP.csv datacenter_name\n",
      "1    us-east1        South Carolina       SOUTHEAST       PJM_LMP.csv          datacenter_name\n",
      "2    us-east4        Northern Virginia    PJM             PJM_LMP.csv          datacenter_name\n",
      "3    us-east5        Columbus             PJM             PJM_LMP.csv          datacenter_name\n",
      "4    us-south1       Dallas               ERCOT           ERCOT_LMP.csv        datacenter_name\n",
      "5    us-west1        Oregon               CAISO           CAISO_LMP.csv        datacenter_name\n",
      "6    us-west2        Los Angeles          CAISO           CAISO_LMP.csv        datacenter_name\n",
      "7    us-west3        Salt Lake City       WEST            CAISO_LMP.csv        datacenter_name\n",
      "8    us-west4        Las Vegas            WEST            CAISO_LMP.csv        datacenter_name\n",
      "Mapping saved to datacenter_iso_mapping.json\n",
      "Error getting price: Timestamp subtraction must have the same timezones or no timezones\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from difflib import get_close_matches\n",
    "import re\n",
    "\n",
    "class DataCenterISOMapper:\n",
    "    \"\"\"\n",
    "    A class to map data centers to their corresponding ISO regions\n",
    "    based on geographical information\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # State to ISO region mapping (primary)\n",
    "        self.state_to_iso = {\n",
    "            # CAISO (California Independent System Operator)\n",
    "            \"California\": \"CAISO\",\n",
    "            \"CA\": \"CAISO\",\n",
    "            \n",
    "            # ERCOT (Electric Reliability Council of Texas)\n",
    "            \"Texas\": \"ERCOT\",\n",
    "            \"TX\": \"ERCOT\",\n",
    "            \n",
    "            # MISO (Midcontinent Independent System Operator)\n",
    "            \"Iowa\": \"MISO\",  \n",
    "            \"IA\": \"MISO\",\n",
    "            \"Illinois\": \"MISO\",\n",
    "            \"IL\": \"MISO\",\n",
    "            \"Indiana\": \"MISO\",\n",
    "            \"IN\": \"MISO\",\n",
    "            \"Michigan\": \"MISO\",\n",
    "            \"MI\": \"MISO\",\n",
    "            \"Minnesota\": \"MISO\",\n",
    "            \"MN\": \"MISO\",\n",
    "            \"Louisiana\": \"MISO\",\n",
    "            \"LA\": \"MISO\",\n",
    "            \"Arkansas\": \"MISO\",\n",
    "            \"AR\": \"MISO\",\n",
    "            \"Mississippi\": \"MISO\",\n",
    "            \"MS\": \"MISO\",\n",
    "            \"Missouri\": \"MISO\",\n",
    "            \"MO\": \"MISO\",\n",
    "            \"North Dakota\": \"MISO\",\n",
    "            \"ND\": \"MISO\",\n",
    "            \"South Dakota\": \"MISO\",\n",
    "            \"SD\": \"MISO\",\n",
    "            \"Wisconsin\": \"MISO\",\n",
    "            \"WI\": \"MISO\",\n",
    "            \n",
    "            # ISO-NE (ISO New England)\n",
    "            \"Connecticut\": \"ISONE\",\n",
    "            \"CT\": \"ISONE\",\n",
    "            \"Maine\": \"ISONE\",\n",
    "            \"ME\": \"ISONE\",\n",
    "            \"Massachusetts\": \"ISONE\",\n",
    "            \"MA\": \"ISONE\",\n",
    "            \"New Hampshire\": \"ISONE\",\n",
    "            \"NH\": \"ISONE\",\n",
    "            \"Rhode Island\": \"ISONE\",\n",
    "            \"RI\": \"ISONE\",\n",
    "            \"Vermont\": \"ISONE\",\n",
    "            \"VT\": \"ISONE\",\n",
    "            \n",
    "            # NYISO (New York Independent System Operator)\n",
    "            \"New York\": \"NYISO\",\n",
    "            \"NY\": \"NYISO\",\n",
    "            \n",
    "            # PJM (Pennsylvania-New Jersey-Maryland Interconnection)\n",
    "            \"Delaware\": \"PJM\",\n",
    "            \"DE\": \"PJM\",\n",
    "            \"Kentucky\": \"PJM\",\n",
    "            \"KY\": \"PJM\",\n",
    "            \"Maryland\": \"PJM\",\n",
    "            \"MD\": \"PJM\",\n",
    "            \"New Jersey\": \"PJM\",\n",
    "            \"NJ\": \"PJM\",\n",
    "            \"North Carolina\": \"PJM\",\n",
    "            \"NC\": \"PJM\",\n",
    "            \"Ohio\": \"PJM\",\n",
    "            \"OH\": \"PJM\",\n",
    "            \"Pennsylvania\": \"PJM\",\n",
    "            \"PA\": \"PJM\",\n",
    "            \"Virginia\": \"PJM\",\n",
    "            \"VA\": \"PJM\",\n",
    "            \"West Virginia\": \"PJM\",\n",
    "            \"WV\": \"PJM\",\n",
    "            \"Washington DC\": \"PJM\",\n",
    "            \"District of Columbia\": \"PJM\",\n",
    "            \"DC\": \"PJM\",\n",
    "            \n",
    "            # SPP (Southwest Power Pool)\n",
    "            \"Kansas\": \"SPP\",\n",
    "            \"KS\": \"SPP\",\n",
    "            \"Oklahoma\": \"SPP\",\n",
    "            \"OK\": \"SPP\",\n",
    "            \"Nebraska\": \"SPP\",\n",
    "            \"NE\": \"SPP\",\n",
    "            \"New Mexico\": \"SPP\",\n",
    "            \"NM\": \"SPP\",\n",
    "            \"Colorado\": \"SPP\",\n",
    "            \"CO\": \"SPP\",\n",
    "            \"Wyoming\": \"SPP\",\n",
    "            \"WY\": \"SPP\",\n",
    "            \n",
    "            # Southeast (not a formal ISO but grouped for reference)\n",
    "            \"Alabama\": \"SOUTHEAST\",\n",
    "            \"AL\": \"SOUTHEAST\",\n",
    "            \"Florida\": \"SOUTHEAST\",\n",
    "            \"FL\": \"SOUTHEAST\",\n",
    "            \"Georgia\": \"SOUTHEAST\",\n",
    "            \"GA\": \"SOUTHEAST\",\n",
    "            \"South Carolina\": \"SOUTHEAST\",\n",
    "            \"SC\": \"SOUTHEAST\",\n",
    "            \"Tennessee\": \"SOUTHEAST\",\n",
    "            \"TN\": \"SOUTHEAST\",\n",
    "            \n",
    "            # West (not covered by major ISOs)\n",
    "            \"Arizona\": \"WEST\",\n",
    "            \"AZ\": \"WEST\",\n",
    "            \"Idaho\": \"WEST\",\n",
    "            \"ID\": \"WEST\",\n",
    "            \"Montana\": \"WEST\",\n",
    "            \"MT\": \"WEST\",\n",
    "            \"Nevada\": \"WEST\",\n",
    "            \"NV\": \"WEST\",\n",
    "            \"Oregon\": \"WEST\",\n",
    "            \"OR\": \"WEST\",\n",
    "            \"Utah\": \"WEST\",\n",
    "            \"UT\": \"WEST\",\n",
    "            \"Washington\": \"WEST\",\n",
    "            \"WA\": \"WEST\",\n",
    "        }\n",
    "        \n",
    "        # City to ISO region mapping (secondary, used when state mapping is ambiguous)\n",
    "        self.city_to_iso = {\n",
    "            # CAISO\n",
    "            \"Los Angeles\": \"CAISO\",\n",
    "            \"San Francisco\": \"CAISO\",\n",
    "            \"San Diego\": \"CAISO\",\n",
    "            \"Sacramento\": \"CAISO\",\n",
    "            \"San Jose\": \"CAISO\",\n",
    "            \n",
    "            # ERCOT\n",
    "            \"Dallas\": \"ERCOT\",\n",
    "            \"Houston\": \"ERCOT\",\n",
    "            \"Austin\": \"ERCOT\",\n",
    "            \"San Antonio\": \"ERCOT\",\n",
    "            \"Fort Worth\": \"ERCOT\",\n",
    "            \n",
    "            # MISO\n",
    "            \"Chicago\": \"MISO\",\n",
    "            \"Detroit\": \"MISO\",\n",
    "            \"Minneapolis\": \"MISO\",\n",
    "            \"New Orleans\": \"MISO\",\n",
    "            \"Des Moines\": \"MISO\",\n",
    "            \n",
    "            # ISO-NE\n",
    "            \"Boston\": \"ISONE\",\n",
    "            \"Providence\": \"ISONE\",\n",
    "            \"Hartford\": \"ISONE\",\n",
    "            \"Portland ME\": \"ISONE\",\n",
    "            \n",
    "            # NYISO\n",
    "            \"New York City\": \"NYISO\",\n",
    "            \"Buffalo\": \"NYISO\",\n",
    "            \"Albany\": \"NYISO\",\n",
    "            \n",
    "            # PJM\n",
    "            \"Philadelphia\": \"PJM\",\n",
    "            \"Pittsburgh\": \"PJM\",\n",
    "            \"Columbus\": \"PJM\",\n",
    "            \"Cincinnati\": \"PJM\",\n",
    "            \"Baltimore\": \"PJM\",\n",
    "            \"Washington\": \"PJM\",\n",
    "            \"Richmond\": \"PJM\",\n",
    "            \"Northern Virginia\": \"PJM\",\n",
    "            \n",
    "            # SPP\n",
    "            \"Kansas City\": \"SPP\",\n",
    "            \"Oklahoma City\": \"SPP\",\n",
    "            \"Omaha\": \"SPP\",\n",
    "            \"Tulsa\": \"SPP\",\n",
    "            \n",
    "            # West\n",
    "            \"Phoenix\": \"WEST\",\n",
    "            \"Las Vegas\": \"WEST\",\n",
    "            \"Salt Lake City\": \"WEST\",\n",
    "            \"Portland OR\": \"WEST\",\n",
    "            \"Seattle\": \"WEST\",\n",
    "            \"Boise\": \"WEST\",\n",
    "            \"Oregon\": \"WEST\",\n",
    "        }\n",
    "        \n",
    "        # Specific datacenter location override (for unique cases)\n",
    "        self.datacenter_specific_mapping = {\n",
    "            \"us-west1\": \"CAISO\",  # Oregon datacenter is actually in CAISO territory\n",
    "            \"us-west2\": \"CAISO\",  # Los Angeles\n",
    "            \"us-west3\": \"WEST\",   # Salt Lake City\n",
    "            \"us-west4\": \"WEST\",   # Las Vegas\n",
    "            \"us-central1\": \"MISO\", # Iowa\n",
    "            \"us-east1\": \"SOUTHEAST\", # South Carolina\n",
    "            \"us-east4\": \"PJM\",    # Northern Virginia\n",
    "            \"us-east5\": \"PJM\",    # Columbus\n",
    "            \"us-south1\": \"ERCOT\"  # Dallas\n",
    "        }\n",
    "        \n",
    "        # ISO to price file mapping\n",
    "        self.iso_to_price_file = {\n",
    "            \"CAISO\": \"CAISO_LMP.csv\",\n",
    "            \"ERCOT\": \"ERCOT_LMP.csv\",\n",
    "            \"MISO\": \"ILLINOIS_MISO_LMP.csv\",  # Using Illinois MISO as default\n",
    "            \"ISONE\": \"ISONE_LMP.csv\",\n",
    "            \"NYISO\": \"NYISO_LMP.csv\",\n",
    "            \"PJM\": \"PJM_LMP.csv\",\n",
    "            \"SPP\": \"SPP_LMP.csv\",\n",
    "            \"LOUISIANA\": \"LOUISIANA_LMP.csv\",\n",
    "            \"NJ\": \"NJ_LMP.csv\",\n",
    "            \"OHIO\": \"Ohio_LMP.csv\",\n",
    "            \"SOUTHEAST\": \"PJM_LMP.csv\",  # Fallback to PJM for Southeast\n",
    "            \"WEST\": \"CAISO_LMP.csv\"      # Fallback to CAISO for Western states\n",
    "        }\n",
    "        \n",
    "        # State abbreviation to full name mapping (for reference)\n",
    "        self.state_abbr_to_name = {\n",
    "            \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\",\n",
    "            \"CA\": \"California\", \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\",\n",
    "            \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\",\n",
    "            \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\",\n",
    "            \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\",\n",
    "            \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\",\n",
    "            \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\",\n",
    "            \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\", \"NY\": \"New York\",\n",
    "            \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\",\n",
    "            \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\",\n",
    "            \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\",\n",
    "            \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\",\n",
    "            \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\", \"DC\": \"District of Columbia\"\n",
    "        }\n",
    "        \n",
    "        # Initialize lookup map for more flexible searching\n",
    "        self._initialize_lookup_maps()\n",
    "    \n",
    "    def _initialize_lookup_maps(self):\n",
    "        \"\"\"Initialize lookup maps for more flexible searching\"\"\"\n",
    "        # Create case-insensitive maps\n",
    "        self.state_lookup = {k.lower(): v for k, v in self.state_to_iso.items()}\n",
    "        self.city_lookup = {k.lower(): v for k, v in self.city_to_iso.items()}\n",
    "        self.datacenter_lookup = {k.lower(): v for k, v in self.datacenter_specific_mapping.items()}\n",
    "    \n",
    "    def get_iso_for_datacenter(self, datacenter_info):\n",
    "        \"\"\"\n",
    "        Determine the ISO region for a datacenter based on its information\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        datacenter_info : dict\n",
    "            Dictionary containing datacenter information with at least 'name' and 'location' keys\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with ISO information including 'iso_name', 'price_file', and 'match_type'\n",
    "        \"\"\"\n",
    "        iso_name = None\n",
    "        match_type = None\n",
    "        \n",
    "        # Try to get a direct match from datacenter name first (highest priority)\n",
    "        datacenter_name = datacenter_info.get('name', '').lower()\n",
    "        if datacenter_name in self.datacenter_lookup:\n",
    "            iso_name = self.datacenter_lookup[datacenter_name]\n",
    "            match_type = \"datacenter_name\"\n",
    "        \n",
    "        # If no match yet, try to extract location information\n",
    "        if not iso_name:\n",
    "            location = datacenter_info.get('location', '')\n",
    "            \n",
    "            # Check for direct state match\n",
    "            state_match = self._find_state_match(location)\n",
    "            if state_match:\n",
    "                iso_name = self.state_to_iso.get(state_match)\n",
    "                match_type = \"state\"\n",
    "            \n",
    "            # If no state match, try city match\n",
    "            if not iso_name:\n",
    "                city_match = self._find_city_match(location)\n",
    "                if city_match:\n",
    "                    iso_name = self.city_to_iso.get(city_match)\n",
    "                    match_type = \"city\"\n",
    "        \n",
    "        # Default to PJM if no match found (most extensive coverage)\n",
    "        if not iso_name:\n",
    "            iso_name = \"PJM\"\n",
    "            match_type = \"default\"\n",
    "        \n",
    "        # Get the appropriate price file\n",
    "        price_file = self.iso_to_price_file.get(iso_name)\n",
    "        \n",
    "        return {\n",
    "            \"datacenter_name\": datacenter_info.get('name', ''),\n",
    "            \"location\": datacenter_info.get('location', ''),\n",
    "            \"iso_name\": iso_name,\n",
    "            \"price_file\": price_file,\n",
    "            \"match_type\": match_type\n",
    "        }\n",
    "    \n",
    "    def _find_state_match(self, location):\n",
    "        \"\"\"Find state match in a location string\"\"\"\n",
    "        if not location:\n",
    "            return None\n",
    "            \n",
    "        location_lower = location.lower()\n",
    "        \n",
    "        # Direct lookup\n",
    "        if location_lower in self.state_lookup:\n",
    "            return location\n",
    "        \n",
    "        # Check for state abbreviation or full name in string\n",
    "        for abbr, name in self.state_abbr_to_name.items():\n",
    "            if abbr.lower() in location_lower or name.lower() in location_lower:\n",
    "                return abbr\n",
    "        \n",
    "        # Try fuzzy matching\n",
    "        possible_states = list(self.state_to_iso.keys())\n",
    "        matches = get_close_matches(location_lower, [s.lower() for s in possible_states], n=1, cutoff=0.6)\n",
    "        \n",
    "        if matches:\n",
    "            # Find the original case version\n",
    "            for state in possible_states:\n",
    "                if state.lower() == matches[0]:\n",
    "                    return state\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _find_city_match(self, location):\n",
    "        \"\"\"Find city match in a location string\"\"\"\n",
    "        if not location:\n",
    "            return None\n",
    "            \n",
    "        location_lower = location.lower()\n",
    "        \n",
    "        # Direct lookup\n",
    "        if location_lower in self.city_lookup:\n",
    "            return location\n",
    "        \n",
    "        # Check for city name in string\n",
    "        for city in self.city_to_iso.keys():\n",
    "            if city.lower() in location_lower:\n",
    "                return city\n",
    "        \n",
    "        # Try fuzzy matching\n",
    "        possible_cities = list(self.city_to_iso.keys())\n",
    "        matches = get_close_matches(location_lower, [c.lower() for c in possible_cities], n=1, cutoff=0.6)\n",
    "        \n",
    "        if matches:\n",
    "            # Find the original case version\n",
    "            for city in possible_cities:\n",
    "                if city.lower() == matches[0]:\n",
    "                    return city\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def map_datacenters(self, datacenter_dict):\n",
    "        \"\"\"\n",
    "        Map all datacenters in a dictionary to their ISO regions\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        datacenter_dict : dict\n",
    "            Dictionary of datacenters with their information\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary mapping datacenter IDs to ISO information\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        for dc_id, dc_info in datacenter_dict.items():\n",
    "            result[dc_id] = self.get_iso_for_datacenter(dc_info)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_best_price_column(self, iso_name, price_df):\n",
    "        \"\"\"\n",
    "        Determine the best price column to use from a given ISO's price dataframe\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        iso_name : str\n",
    "            Name of the ISO region\n",
    "        price_df : DataFrame\n",
    "            DataFrame containing price data for the ISO\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Name of the most appropriate price column to use\n",
    "        \"\"\"\n",
    "        # Default to 'lmp' column if available\n",
    "        if 'lmp' in price_df.columns:\n",
    "            return 'lmp'\n",
    "        \n",
    "        # Look for other price-related columns\n",
    "        price_columns = [col for col in price_df.columns if any(\n",
    "            term in col.lower() for term in ['price', 'cost', 'lmp', 'rate']\n",
    "        )]\n",
    "        \n",
    "        if price_columns:\n",
    "            return price_columns[0]\n",
    "        \n",
    "        # Last resort: return the first numeric column that's not a timestamp\n",
    "        for col in price_df.columns:\n",
    "            if price_df[col].dtype in ['int64', 'float64']:\n",
    "                return col\n",
    "        \n",
    "        # If no suitable column found\n",
    "        raise ValueError(f\"No suitable price column found in price data for ISO: {iso_name}\")\n",
    "    \n",
    "    def update_datacenter_dict_with_iso(self, datacenter_dict):\n",
    "        \"\"\"\n",
    "        Update a datacenter dictionary with ISO region information\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        datacenter_dict : dict\n",
    "            Dictionary of datacenters with their information\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Updated dictionary with ISO information added to each datacenter\n",
    "        \"\"\"\n",
    "        updated_dict = {}\n",
    "        \n",
    "        for dc_id, dc_info in datacenter_dict.items():\n",
    "            # Create a copy of the original info\n",
    "            updated_info = dc_info.copy()\n",
    "            \n",
    "            # Get ISO information\n",
    "            iso_info = self.get_iso_for_datacenter(dc_info)\n",
    "            \n",
    "            # Add ISO information to datacenter info\n",
    "            updated_info['iso_region'] = iso_info['iso_name']\n",
    "            updated_info['iso_price_file'] = iso_info['price_file']\n",
    "            \n",
    "            # Add to updated dictionary\n",
    "            updated_dict[dc_id] = updated_info\n",
    "        \n",
    "        return updated_dict\n",
    "\n",
    "def map_google_cloud_datacenters():\n",
    "    \"\"\"Map the Google Cloud datacenters to their ISO regions\"\"\"\n",
    "    # Google Cloud datacenter information\n",
    "    datacenter_info = {\n",
    "        0: {\"name\": \"us-central1\", \"location\": \"Iowa\", \"renewable\": 0.95, \"co2\": 430},\n",
    "        1: {\"name\": \"us-east1\", \"location\": \"South Carolina\", \"renewable\": 0.29, \"co2\": 560},\n",
    "        2: {\"name\": \"us-east4\", \"location\": \"Northern Virginia\", \"renewable\": 0.52, \"co2\": 322},\n",
    "        3: {\"name\": \"us-east5\", \"location\": \"Columbus\", \"renewable\": 0.52, \"co2\": 322},\n",
    "        4: {\"name\": \"us-south1\", \"location\": \"Dallas\", \"renewable\": 0.79, \"co2\": 321},\n",
    "        5: {\"name\": \"us-west1\", \"location\": \"Oregon\", \"renewable\": 0.84, \"co2\": 94},\n",
    "        6: {\"name\": \"us-west2\", \"location\": \"Los Angeles\", \"renewable\": 0.55, \"co2\": 198},\n",
    "        7: {\"name\": \"us-west3\", \"location\": \"Salt Lake City\", \"renewable\": 0.29, \"co2\": 588},\n",
    "        8: {\"name\": \"us-west4\", \"location\": \"Las Vegas\", \"renewable\": 0.26, \"co2\": 373}\n",
    "    }\n",
    "    \n",
    "    # Create mapper and map datacenters\n",
    "    mapper = DataCenterISOMapper()\n",
    "    dc_iso_mapping = mapper.map_datacenters(datacenter_info)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Google Cloud Datacenter to ISO Region Mapping:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'ID':<4} {'Name':<15} {'Location':<20} {'ISO Region':<15} {'Price File':<20} {'Match Type':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for dc_id, iso_info in dc_iso_mapping.items():\n",
    "        print(f\"{dc_id:<4} {iso_info['datacenter_name']:<15} {iso_info['location']:<20} \"\n",
    "              f\"{iso_info['iso_name']:<15} {iso_info['price_file']:<20} {iso_info['match_type']:<15}\")\n",
    "    \n",
    "    # Return both the original datacenter info and the mapping\n",
    "    return datacenter_info, dc_iso_mapping\n",
    "\n",
    "def get_electricity_price(datacenter_id, timestamp, datacenter_info, dc_iso_mapping, price_data_dir='.'):\n",
    "    \"\"\"\n",
    "    Get the electricity price for a datacenter at a specific timestamp\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    datacenter_id : int or str\n",
    "        ID of the datacenter\n",
    "    timestamp : datetime or str\n",
    "        Timestamp to get price for\n",
    "    datacenter_info : dict\n",
    "        Dictionary of datacenter information\n",
    "    dc_iso_mapping : dict\n",
    "        Dictionary mapping datacenter IDs to ISO information\n",
    "    price_data_dir : str\n",
    "        Directory containing price data files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Electricity price for the datacenter at the specified timestamp\n",
    "    \"\"\"\n",
    "    # Convert datacenter_id to string for lookup\n",
    "    dc_id = str(datacenter_id)\n",
    "    \n",
    "    # Get ISO mapping for this datacenter\n",
    "    iso_mapping = dc_iso_mapping.get(int(dc_id) if dc_id.isdigit() else dc_id)\n",
    "    \n",
    "    if not iso_mapping:\n",
    "        raise ValueError(f\"No ISO mapping found for datacenter ID: {datacenter_id}\")\n",
    "    \n",
    "    # Get price file path\n",
    "    price_file = iso_mapping.get('price_file')\n",
    "    if not price_file:\n",
    "        raise ValueError(f\"No price file specified for datacenter ID: {datacenter_id}\")\n",
    "    \n",
    "    price_file_path = os.path.join(price_data_dir, price_file)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(price_file_path):\n",
    "        raise FileNotFoundError(f\"Price file not found: {price_file_path}\")\n",
    "    \n",
    "    # Load price data\n",
    "    price_df = pd.read_csv(price_file_path)\n",
    "    \n",
    "    # Convert timestamp column to datetime\n",
    "    timestamp_col = 'interval_start_utc'\n",
    "    if timestamp_col in price_df.columns:\n",
    "        price_df[timestamp_col] = pd.to_datetime(price_df[timestamp_col])\n",
    "    \n",
    "    # Convert input timestamp to datetime if it's a string\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = pd.to_datetime(timestamp)\n",
    "    \n",
    "    # Get the best price column to use\n",
    "    mapper = DataCenterISOMapper()\n",
    "    price_col = mapper.get_best_price_column(iso_mapping['iso_name'], price_df)\n",
    "    \n",
    "    # Find the closest timestamp in the data\n",
    "    closest_row = price_df.iloc[price_df[timestamp_col].sub(timestamp).abs().idxmin()]\n",
    "    \n",
    "    # Return the price\n",
    "    return closest_row[price_col]\n",
    "\n",
    "def save_mapping_to_json(datacenter_info, dc_iso_mapping, output_file='datacenter_iso_mapping.json'):\n",
    "    \"\"\"Save datacenter to ISO mapping to a JSON file\"\"\"\n",
    "    # Create a combined dictionary with all information\n",
    "    combined_data = {\n",
    "        'datacenters': {},\n",
    "        'iso_regions': {}\n",
    "    }\n",
    "    \n",
    "    # Add datacenter information with ISO mapping\n",
    "    for dc_id, dc_info in datacenter_info.items():\n",
    "        dc_id_str = str(dc_id)\n",
    "        iso_mapping = dc_iso_mapping.get(int(dc_id) if dc_id_str.isdigit() else dc_id)\n",
    "        \n",
    "        combined_data['datacenters'][dc_id_str] = {\n",
    "            **dc_info,\n",
    "            'iso_region': iso_mapping['iso_name'],\n",
    "            'iso_price_file': iso_mapping['price_file'],\n",
    "            'iso_match_type': iso_mapping['match_type']\n",
    "        }\n",
    "    \n",
    "    # Add unique ISO regions information\n",
    "    unique_isos = set(mapping['iso_name'] for mapping in dc_iso_mapping.values())\n",
    "    for iso_name in unique_isos:\n",
    "        combined_data['iso_regions'][iso_name] = {\n",
    "            'price_file': next(mapping['price_file'] for mapping in dc_iso_mapping.values() \n",
    "                           if mapping['iso_name'] == iso_name),\n",
    "            'datacenters': [dc_id for dc_id, mapping in dc_iso_mapping.items() \n",
    "                          if mapping['iso_name'] == iso_name]\n",
    "        }\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(combined_data, f, indent=4)\n",
    "    \n",
    "    print(f\"Mapping saved to {output_file}\")\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    datacenter_info, dc_iso_mapping = map_google_cloud_datacenters()\n",
    "    \n",
    "    # Save mapping to JSON\n",
    "    combined_data = save_mapping_to_json(datacenter_info, dc_iso_mapping)\n",
    "    \n",
    "    # Example price lookup\n",
    "    try:\n",
    "        # Get price for us-central1 for a specific timestamp\n",
    "        timestamp = \"2025-03-24 10:00:00\"\n",
    "        dc_id = 0  # us-central1\n",
    "        \n",
    "        price = get_electricity_price(dc_id, timestamp, datacenter_info, dc_iso_mapping)\n",
    "        print(f\"\\nElectricity price for {datacenter_info[dc_id]['name']} at {timestamp}: ${price:.2f}/MWh\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting price: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DataCenterPowerManager:\n",
    "    \"\"\"\n",
    "    Class for managing data center power pricing and analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, datacenter_info=None, iso_mapping_file=None, price_data_dir='.'):\n",
    "        \"\"\"\n",
    "        Initialize the DataCenterPowerManager\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        datacenter_info : dict\n",
    "            Dictionary containing datacenter information\n",
    "        iso_mapping_file : str\n",
    "            Path to the JSON file containing ISO mapping information\n",
    "        price_data_dir : str\n",
    "            Directory containing price data files\n",
    "        \"\"\"\n",
    "        self.price_data_dir = price_data_dir\n",
    "        \n",
    "        # Initialize datacenter info and mappings\n",
    "        if datacenter_info:\n",
    "            self.datacenter_info = datacenter_info\n",
    "            # Create mapper and generate ISO mappings\n",
    "            self.mapper = DataCenterISOMapper()\n",
    "            self.dc_iso_mapping = self.mapper.map_datacenters(self.datacenter_info)\n",
    "        elif iso_mapping_file and os.path.exists(iso_mapping_file):\n",
    "            # Load from mapping file\n",
    "            with open(iso_mapping_file, 'r') as f:\n",
    "                mapping_data = json.load(f)\n",
    "            \n",
    "            self.datacenter_info = {int(k) if k.isdigit() else k: v \n",
    "                                   for k, v in mapping_data['datacenters'].items()}\n",
    "            \n",
    "            # Extract ISO mappings\n",
    "            self.dc_iso_mapping = {}\n",
    "            for dc_id, dc_info in self.datacenter_info.items():\n",
    "                dc_id_normalized = int(dc_id) if isinstance(dc_id, str) and dc_id.isdigit() else dc_id\n",
    "                self.dc_iso_mapping[dc_id_normalized] = {\n",
    "                    'datacenter_name': dc_info['name'],\n",
    "                    'location': dc_info['location'],\n",
    "                    'iso_name': dc_info['iso_region'],\n",
    "                    'price_file': dc_info['iso_price_file'],\n",
    "                    'match_type': dc_info.get('iso_match_type', 'loaded')\n",
    "                }\n",
    "            \n",
    "            self.mapper = DataCenterISOMapper()\n",
    "        else:\n",
    "            raise ValueError(\"Either datacenter_info or iso_mapping_file must be provided\")\n",
    "        \n",
    "        # Cache for price data\n",
    "        self.price_data_cache = {}\n",
    "    \n",
    "    def load_price_data(self, iso_name, price_file):\n",
    "        \"\"\"\n",
    "        Load price data for an ISO region\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        iso_name : str\n",
    "            Name of the ISO region\n",
    "        price_file : str\n",
    "            Name of the price file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            DataFrame containing price data\n",
    "        \"\"\"\n",
    "        # Check if data is already in cache\n",
    "        cache_key = f\"{iso_name}_{price_file}\"\n",
    "        if cache_key in self.price_data_cache:\n",
    "            return self.price_data_cache[cache_key]\n",
    "        \n",
    "        # Load data\n",
    "        price_file_path = os.path.join(self.price_data_dir, price_file)\n",
    "        if not os.path.exists(price_file_path):\n",
    "            raise FileNotFoundError(f\"Price file not found: {price_file_path}\")\n",
    "        \n",
    "        df = pd.read_csv(price_file_path)\n",
    "        \n",
    "        # Convert timestamp column to datetime\n",
    "        timestamp_col = 'interval_start_utc'\n",
    "        if timestamp_col in df.columns:\n",
    "            df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "        \n",
    "        # Store in cache\n",
    "        self.price_data_cache[cache_key] = df\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_price(self, datacenter_id, timestamp):\n",
    "        \"\"\"\n",
    "        Get the electricity price for a datacenter at a specific timestamp\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        datacenter_id : int or str\n",
    "            ID of the datacenter\n",
    "        timestamp : datetime or str\n",
    "            Timestamp to get price for\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Electricity price for the datacenter at the specified timestamp\n",
    "        \"\"\"\n",
    "        # Normalize datacenter_id\n",
    "        dc_id = int(datacenter_id) if str(datacenter_id).isdigit() else datacenter_id\n",
    "        \n",
    "        # Get ISO mapping for this datacenter\n",
    "        iso_mapping = self.dc_iso_mapping.get(dc_id)\n",
    "        if not iso_mapping:\n",
    "            raise ValueError(f\"No ISO mapping found for datacenter ID: {datacenter_id}\")\n",
    "        \n",
    "        # Get price file path\n",
    "        price_file = iso_mapping.get('price_file')\n",
    "        if not price_file:\n",
    "            raise ValueError(f\"No price file specified for datacenter ID: {datacenter_id}\")\n",
    "        \n",
    "        # Load price data\n",
    "        try:\n",
    "            price_df = self.load_price_data(iso_mapping['iso_name'], price_file)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load price data for datacenter {datacenter_id}: {str(e)}\")\n",
    "        \n",
    "        # Convert input timestamp to datetime if it's a string\n",
    "        if isinstance(timestamp, str):\n",
    "            timestamp = pd.to_datetime(timestamp)\n",
    "        \n",
    "        # Get the best price column to use\n",
    "        price_col = self.mapper.get_best_price_column(iso_mapping['iso_name'], price_df)\n",
    "        \n",
    "        # Find the timestamp column\n",
    "        timestamp_col = 'interval_start_utc'\n",
    "        if timestamp_col not in price_df.columns:\n",
    "            # Try to find an alternative timestamp column\n",
    "            timestamp_candidates = [col for col in price_df.columns \n",
    "                                   if any(term in col.lower() \n",
    "                                         for term in ['time', 'date', 'timestamp'])]\n",
    "            if timestamp_candidates:\n",
    "                timestamp_col = timestamp_candidates[0]\n",
    "            else:\n",
    "                raise ValueError(f\"No timestamp column found in price data for ISO: {iso_mapping['iso_name']}\")\n",
    "        \n",
    "        # Find the closest timestamp in the data\n",
    "        closest_idx = price_df[timestamp_col].sub(timestamp).abs().idxmin()\n",
    "        closest_row = price_df.iloc[closest_idx]\n",
    "        \n",
    "        # Return the price\n",
    "        return closest_row[price_col]\n",
    "    \n",
    "    def get_price_series(self, datacenter_id, start_time, end_time):\n",
    "        \"\"\"\n",
    "        Get a time series of electricity prices for a datacenter\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        datacenter_id : int or str\n",
    "            ID of the datacenter\n",
    "        start_time : datetime or str\n",
    "            Start timestamp\n",
    "        end_time : datetime or str\n",
    "            End timestamp\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            DataFrame containing timestamp and price data\n",
    "        \"\"\"\n",
    "        # Normalize datacenter_id\n",
    "        dc_id = int(datacenter_id) if str(datacenter_id).isdigit() else datacenter_id\n",
    "        \n",
    "        # Get ISO mapping for this datacenter\n",
    "        iso_mapping = self.dc_iso_mapping.get(dc_id)\n",
    "        if not iso_mapping:\n",
    "            raise ValueError(f\"No ISO mapping found for datacenter ID: {datacenter_id}\")\n",
    "        \n",
    "        # Get price file\n",
    "        price_file = iso_mapping.get('price_file')\n",
    "        if not price_file:\n",
    "            raise ValueError(f\"No price file specified for datacenter ID: {datacenter_id}\")\n",
    "        \n",
    "        # Load price data\n",
    "        try:\n",
    "            price_df = self.load_price_data(iso_mapping['iso_name'], price_file)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load price data for datacenter {datacenter_id}: {str(e)}\")\n",
    "        \n",
    "        # Convert input timestamps to datetime if they're strings\n",
    "        if isinstance(start_time, str):\n",
    "            start_time = pd.to_datetime(start_time)\n",
    "        if isinstance(end_time, str):\n",
    "            end_time = pd.to_datetime(end_time)\n",
    "        \n",
    "        # Get the best price column to use\n",
    "        price_col = self.mapper.get_best_price_column(iso_mapping['iso_name'], price_df)\n",
    "        \n",
    "        # Find the timestamp column\n",
    "        timestamp_col = 'interval_start_utc'\n",
    "        if timestamp_col not in price_df.columns:\n",
    "            # Try to find an alternative timestamp column\n",
    "            timestamp_candidates = [col for col in price_df.columns \n",
    "                                   if any(term in col.lower() \n",
    "                                         for term in ['time', 'date', 'timestamp'])]\n",
    "            if timestamp_candidates:\n",
    "                timestamp_col = timestamp_candidates[0]\n",
    "            else:\n",
    "                raise ValueError(f\"No timestamp column found in price data for ISO: {iso_mapping['iso_name']}\")\n",
    "        \n",
    "        # Filter to the requested time range\n",
    "        filtered_df = price_df[(price_df[timestamp_col] >= start_time) & \n",
    "                              (price_df[timestamp_col] <= end_time)].copy()\n",
    "        \n",
    "        # Select only the relevant columns\n",
    "        result_df = filtered_df[[timestamp_col, price_col]]\n",
    "        \n",
    "        # Add datacenter information\n",
    "        result_df['datacenter_id'] = dc_id\n",
    "        result_df['datacenter_name'] = self.datacenter_info[dc_id]['name']\n",
    "        result_df['location'] = self.datacenter_info[dc_id]['location']\n",
    "        result_df['iso_region'] = iso_mapping['iso_name']\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def compare_datacenter_prices(self, datacenter_ids, timestamp):\n",
    "        \"\"\"\n",
    "        Compare electricity prices across multiple datacenters at a specific timestamp\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        datacenter_ids : list\n",
    "            List of datacenter IDs to compare\n",
    "        timestamp : datetime or str\n",
    "            Timestamp to compare prices at\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            DataFrame containing price comparison\n",
    "        \"\"\"\n",
    "        # Initialize results\n",
    "        results = []\n",
    "        \n",
    "        # Get price for each datacenter\n",
    "        for dc_id in datacenter_ids:\n",
    "            try:\n",
    "                price = self.get_price(dc_id, timestamp)\n",
    "                \n",
    "                # Add to results\n",
    "                results.append({\n",
    "                    'datacenter_id': dc_id,\n",
    "                    'datacenter_name': self.datacenter_info[dc_id]['name'],\n",
    "                    'location': self.datacenter_info[dc_id]['location'],\n",
    "                    'iso_region': self.dc_iso_mapping[dc_id]['iso_name'],\n",
    "                    'price': price,\n",
    "                    'renewable': self.datacenter_info[dc_id].get('renewable', None),\n",
    "                    'co2': self.datacenter_info[dc_id].get('co2', None)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to get price for datacenter {dc_id}: {str(e)}\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        if results:\n",
    "            result_df = pd.DataFrame(results)\n",
    "            \n",
    "            # Sort by price (ascending)\n",
    "            result_df = result_df.sort_values('price')\n",
    "            \n",
    "            return result_df\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\n",
    "                'datacenter_id', 'datacenter_name', 'location', 'iso_region', 'price', 'renewable', 'co2'\n",
    "            ])\n",
    "    \n",
    "    def find_cheapest_datacenter(self, timestamp):\n",
    "        \"\"\"\n",
    "        Find the datacenter with the lowest electricity price at a specific timestamp\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        timestamp : datetime or str\n",
    "            Timestamp to check prices at\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Information about the cheapest datacenter\n",
    "        \"\"\"\n",
    "        # Get price comparison for all datacenters\n",
    "        comparison = self.compare_datacenter_prices(list(self.datacenter_info.keys()), timestamp)\n",
    "        \n",
    "        # Return the cheapest (first row after sorting)\n",
    "        if len(comparison) > 0:\n",
    "            cheapest = comparison.iloc[0].to_dict()\n",
    "            return cheapest\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def find_optimal_datacenter(self, timestamp, price_weight=0.5, renewable_weight=0.3, co2_weight=0.2):\n",
    "        \"\"\"\n",
    "        Find the optimal datacenter based on a weighted combination of price, renewable energy, and CO2 emissions\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        timestamp : datetime or str\n",
    "            Timestamp to check prices at\n",
    "        price_weight : float\n",
    "            Weight to assign to price factor (lower is better)\n",
    "        renewable_weight : float\n",
    "            Weight to assign to renewable energy factor (higher is better)\n",
    "        co2_weight : float\n",
    "            Weight to assign to CO2 emissions factor (lower is better)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Information about the optimal datacenter\n",
    "        \"\"\"\n",
    "        # Get price comparison for all datacenters\n",
    "        comparison = self.compare_datacenter_prices(list(self.datacenter_info.keys()), timestamp)\n",
    "        \n",
    "        if len(comparison) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Normalize factors to 0-1 scale\n",
    "        if comparison['price'].max() != comparison['price'].min():\n",
    "            comparison['price_norm'] = 1 - ((comparison['price'] - comparison['price'].min()) / \n",
    "                                         (comparison['price'].max() - comparison['price'].min()))\n",
    "        else:\n",
    "            comparison['price_norm'] = 1\n",
    "        \n",
    "        if comparison['renewable'].max() != comparison['renewable'].min():\n",
    "            comparison['renewable_norm'] = (comparison['renewable'] - comparison['renewable'].min()) / \\\n",
    "                                        (comparison['renewable'].max() - comparison['renewable'].min())\n",
    "        else:\n",
    "            comparison['renewable_norm'] = 1\n",
    "        \n",
    "        if comparison['co2'].max() != comparison['co2'].min():\n",
    "            comparison['co2_norm'] = 1 - ((comparison['co2'] - comparison['co2'].min()) / \n",
    "                                       (comparison['co2'].max() - comparison['co2'].min()))\n",
    "        else:\n",
    "            comparison['co2_norm'] = 1\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        comparison['score'] = (comparison['price_norm'] * price_weight + \n",
    "                             comparison['renewable_norm'] * renewable_weight + \n",
    "                             comparison['co2_norm'] * co2_weight)\n",
    "        \n",
    "        # Sort by score (descending)\n",
    "        comparison = comparison.sort_values('score', ascending=False)\n",
    "        \n",
    "        # Return the optimal datacenter\n",
    "        optimal = comparison.iloc[0].to_dict()\n",
    "        return optimal\n",
    "    \n",
    "    def plot_price_comparison(self, start_time, end_time, datacenter_ids=None, figsize=(12, 8)):\n",
    "        \"\"\"\n",
    "        Plot electricity price comparison across datacenters\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_time : datetime or str\n",
    "            Start timestamp\n",
    "        end_time : datetime or str\n",
    "            End timestamp\n",
    "        datacenter_ids : list\n",
    "            List of datacenter IDs to compare (if None, use all)\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            The created figure\n",
    "        \"\"\"\n",
    "        # Default to all datacenters if none specified\n",
    "        if datacenter_ids is None:\n",
    "            datacenter_ids = list(self.datacenter_info.keys())\n",
    "        \n",
    "        # Get price series for each datacenter\n",
    "        all_data = []\n",
    "        \n",
    "        for dc_id in datacenter_ids:\n",
    "            try:\n",
    "                data = self.get_price_series(dc_id, start_time, end_time)\n",
    "                all_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to get price data for datacenter {dc_id}: {str(e)}\")\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"No price data available for the specified datacenters and time range\")\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        for data in all_data:\n",
    "            # Get datacenter name and price column\n",
    "            dc_name = data['datacenter_name'].iloc[0]\n",
    "            timestamp_col = 'interval_start_utc'\n",
    "            price_col = next(col for col in data.columns if col not in [\n",
    "                'datacenter_id', 'datacenter_name', 'location', 'iso_region', 'interval_start_utc'\n",
    "            ])\n",
    "            \n",
    "            # Plot price series\n",
    "            ax.plot(data[timestamp_col], data[price_col], label=f\"{dc_name} ({data['iso_region'].iloc[0]})\")\n",
    "        \n",
    "        # Add labels and legend\n",
    "        ax.set_xlabel('Time (UTC)', fontsize=12)\n",
    "        ax.set_ylabel('Price ($/MWh)', fontsize=12)\n",
    "        ax.set_title('Electricity Price Comparison by Data Center', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        \n",
    "        # Format x-axis to show dates nicely\n",
    "        fig.autofmt_xdate()\n",
    "        \n",
    "        # Format y-axis to show currency\n",
    "        ax.yaxis.set_major_formatter('${x:.2f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_iso_distribution_pie(self, figsize=(10, 8)):\n",
    "        \"\"\"\n",
    "        Plot a pie chart showing the distribution of datacenters across ISO regions\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            The created figure\n",
    "        \"\"\"\n",
    "        # Count datacenters by ISO region\n",
    "        iso_counts = {}\n",
    "        \n",
    "        for dc_id, mapping in self.dc_iso_mapping.items():\n",
    "            iso_name = mapping['iso_name']\n",
    "            iso_counts[iso_name] = iso_counts.get(iso_name, 0) + 1\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Create pie chart\n",
    "        wedges, texts, autotexts = ax.pie(\n",
    "            iso_counts.values(), \n",
    "            labels=iso_counts.keys(),\n",
    "            autopct='%1.1f%%',\n",
    "            shadow=False,\n",
    "            startangle=90\n",
    "        )\n",
    "        \n",
    "        # Style the text\n",
    "        for text in texts:\n",
    "            text.set_fontsize(12)\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_fontsize(10)\n",
    "            autotext.set_color('white')\n",
    "        \n",
    "        ax.set_title('Distribution of Data Centers by ISO Region', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def save_mapping_to_json(self, output_file='datacenter_iso_mapping.json'):\n",
    "        \"\"\"Save datacenter to ISO mapping to a JSON file\"\"\"\n",
    "        # Create a combined dictionary with all information\n",
    "        combined_data = {\n",
    "            'datacenters': {},\n",
    "            'iso_regions': {}\n",
    "        }\n",
    "        \n",
    "        # Add datacenter information with ISO mapping\n",
    "        for dc_id, dc_info in self.datacenter_info.items():\n",
    "            dc_id_str = str(dc_id)\n",
    "            iso_mapping = self.dc_iso_mapping.get(int(dc_id) if dc_id_str.isdigit() else dc_id)\n",
    "            \n",
    "            combined_data['datacenters'][dc_id_str] = {\n",
    "                **dc_info,\n",
    "                'iso_region': iso_mapping['iso_name'],\n",
    "                'iso_price_file': iso_mapping['price_file'],\n",
    "                'iso_match_type': iso_mapping['match_type']\n",
    "            }\n",
    "        \n",
    "        # Add unique ISO regions information\n",
    "        unique_isos = set(mapping['iso_name'] for mapping in self.dc_iso_mapping.values())\n",
    "        for iso_name in unique_isos:\n",
    "            combined_data['iso_regions'][iso_name] = {\n",
    "                'price_file': next(mapping['price_file'] for mapping in self.dc_iso_mapping.values() \n",
    "                               if mapping['iso_name'] == iso_name),\n",
    "                'datacenters': [dc_id for dc_id, mapping in self.dc_iso_mapping.items() \n",
    "                              if mapping['iso_name'] == iso_name]\n",
    "            }\n",
    "        \n",
    "        # Save to JSON file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(combined_data, f, indent=4)\n",
    "        \n",
    "        print(f\"Mapping saved to {output_file}\")\n",
    "        \n",
    "        return combined_data\n",
    "    \n",
    "    def add_new_datacenter(self, dc_id, name, location, renewable=None, co2=None):\n",
    "        \"\"\"\n",
    "        Add a new datacenter to the manager\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dc_id : int or str\n",
    "            ID for the new datacenter\n",
    "        name : str\n",
    "            Name of the datacenter (e.g., \"us-east1\")\n",
    "        location : str\n",
    "            Location of the datacenter (e.g., \"Virginia\")\n",
    "        renewable : float\n",
    "            Renewable energy percentage (0-1)\n",
    "        co2 : float\n",
    "            CO2 emissions (g/kWh)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Information about the newly added datacenter including ISO mapping\n",
    "        \"\"\"\n",
    "        # Create datacenter info\n",
    "        dc_info = {\n",
    "            \"name\": name,\n",
    "            \"location\": location\n",
    "        }\n",
    "        \n",
    "        if renewable is not None:\n",
    "            dc_info[\"renewable\"] = renewable\n",
    "        \n",
    "        if co2 is not None:\n",
    "            dc_info[\"co2\"] = co2\n",
    "        \n",
    "        # Add to datacenter info dictionary\n",
    "        self.datacenter_info[dc_id] = dc_info\n",
    "        \n",
    "        # Create ISO mapping\n",
    "        iso_mapping = self.mapper.get_iso_for_datacenter(dc_info)\n",
    "        self.dc_iso_mapping[dc_id] = iso_mapping\n",
    "        \n",
    "        return {\n",
    "            \"datacenter_id\": dc_id,\n",
    "            \"info\": dc_info,\n",
    "            \"iso_mapping\": iso_mapping\n",
    "        }\n",
    "    \n",
    "    def get_datacenter_summary(self):\n",
    "        \"\"\"\n",
    "        Get a summary of all datacenters and their ISO mappings\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Summary of all datacenters\n",
    "        \"\"\"\n",
    "        # Initialize results\n",
    "        results = []\n",
    "        \n",
    "        # Process each datacenter\n",
    "        for dc_id, dc_info in self.datacenter_info.items():\n",
    "            iso_mapping = self.dc_iso_mapping.get(dc_id)\n",
    "            \n",
    "            results.append({\n",
    "                'datacenter_id': dc_id,\n",
    "                'datacenter_name': dc_info['name'],\n",
    "                'location': dc_info['location'],\n",
    "                'iso_region': iso_mapping['iso_name'],\n",
    "                'price_file': iso_mapping['price_file'],\n",
    "                'match_type': iso_mapping['match_type'],\n",
    "                'renewable': dc_info.get('renewable', None),\n",
    "                'co2': dc_info.get('co2', None)\n",
    "            })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        result_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Sort by datacenter ID\n",
    "        result_df = result_df.sort_values('datacenter_id')\n",
    "        \n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Center Power Management Example\n",
      "\n",
      "Mapping Google Cloud datacenters to ISO regions...\n",
      "\n",
      "Google Cloud Datacenter to ISO Region Mapping:\n",
      "--------------------------------------------------------------------------------\n",
      "ID   Name            Location             ISO Region      Price File           Match Type     \n",
      "--------------------------------------------------------------------------------\n",
      "0    us-central1     Iowa                 MISO            ILLINOIS_MISO_LMP.csv datacenter_name\n",
      "1    us-east1        South Carolina       SOUTHEAST       PJM_LMP.csv          datacenter_name\n",
      "2    us-east4        Northern Virginia    PJM             PJM_LMP.csv          datacenter_name\n",
      "3    us-east5        Columbus             PJM             PJM_LMP.csv          datacenter_name\n",
      "4    us-south1       Dallas               ERCOT           ERCOT_LMP.csv        datacenter_name\n",
      "5    us-west1        Oregon               CAISO           CAISO_LMP.csv        datacenter_name\n",
      "6    us-west2        Los Angeles          CAISO           CAISO_LMP.csv        datacenter_name\n",
      "7    us-west3        Salt Lake City       WEST            CAISO_LMP.csv        datacenter_name\n",
      "8    us-west4        Las Vegas            WEST            CAISO_LMP.csv        datacenter_name\n",
      "\n",
      "Data Center Summary:\n",
      "   datacenter_id datacenter_name           location iso_region  \\\n",
      "0              0     us-central1               Iowa       MISO   \n",
      "1              1        us-east1     South Carolina  SOUTHEAST   \n",
      "2              2        us-east4  Northern Virginia        PJM   \n",
      "3              3        us-east5           Columbus        PJM   \n",
      "4              4       us-south1             Dallas      ERCOT   \n",
      "5              5        us-west1             Oregon      CAISO   \n",
      "6              6        us-west2        Los Angeles      CAISO   \n",
      "7              7        us-west3     Salt Lake City       WEST   \n",
      "8              8        us-west4          Las Vegas       WEST   \n",
      "\n",
      "              price_file       match_type  renewable  co2  \n",
      "0  ILLINOIS_MISO_LMP.csv  datacenter_name       0.95  430  \n",
      "1            PJM_LMP.csv  datacenter_name       0.29  560  \n",
      "2            PJM_LMP.csv  datacenter_name       0.52  322  \n",
      "3            PJM_LMP.csv  datacenter_name       0.52  322  \n",
      "4          ERCOT_LMP.csv  datacenter_name       0.79  321  \n",
      "5          CAISO_LMP.csv  datacenter_name       0.84   94  \n",
      "6          CAISO_LMP.csv  datacenter_name       0.55  198  \n",
      "7          CAISO_LMP.csv  datacenter_name       0.29  588  \n",
      "8          CAISO_LMP.csv  datacenter_name       0.26  373  \n",
      "\n",
      "Electricity Price Comparison at 2025-03-24 10:00:00:\n",
      "Warning: Failed to get price for datacenter 0: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 1: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 2: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 3: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 4: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 5: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 6: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 7: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 8: Timestamp subtraction must have the same timezones or no timezones\n",
      "Empty DataFrame\n",
      "Columns: [datacenter_name, location, iso_region, price, renewable, co2]\n",
      "Index: []\n",
      "Warning: Failed to get price for datacenter 0: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 1: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 2: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 3: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 4: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 5: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 6: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 7: Timestamp subtraction must have the same timezones or no timezones\n",
      "Warning: Failed to get price for datacenter 8: Timestamp subtraction must have the same timezones or no timezones\n",
      "\n",
      "Cheapest datacenter at 2025-03-24 10:00:00:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30500/1787811535.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30500/1787811535.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mcheapest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpower_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_cheapest_datacenter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nCheapest datacenter at {timestamp}:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"  {cheapest['datacenter_name']} ({cheapest['location']}): ${cheapest['price']:.2f}/MWh\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Example 3: Find the optimal datacenter considering price, renewable energy, and CO2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the DataCenterPowerManager class\"\"\"\n",
    "    print(\"Data Center Power Management Example\\n\")\n",
    "    \n",
    "    # Get Google Cloud datacenter information and ISO mapping\n",
    "    print(\"Mapping Google Cloud datacenters to ISO regions...\\n\")\n",
    "    datacenter_info, dc_iso_mapping = map_google_cloud_datacenters()\n",
    "    \n",
    "    # Create power manager\n",
    "    power_manager = DataCenterPowerManager(datacenter_info=datacenter_info)\n",
    "    \n",
    "    # Display datacenter summary\n",
    "    print(\"\\nData Center Summary:\")\n",
    "    summary = power_manager.get_datacenter_summary()\n",
    "    print(summary)\n",
    "    \n",
    "    # Example 1: Compare prices at a specific time\n",
    "    timestamp = \"2025-03-24 10:00:00\"\n",
    "    print(f\"\\nElectricity Price Comparison at {timestamp}:\")\n",
    "    comparison = power_manager.compare_datacenter_prices(list(datacenter_info.keys()), timestamp)\n",
    "    print(comparison[['datacenter_name', 'location', 'iso_region', 'price', 'renewable', 'co2']])\n",
    "    \n",
    "    # Example 2: Find the cheapest datacenter\n",
    "    cheapest = power_manager.find_cheapest_datacenter(timestamp)\n",
    "    print(f\"\\nCheapest datacenter at {timestamp}:\")\n",
    "    print(f\"  {cheapest['datacenter_name']} ({cheapest['location']}): ${cheapest['price']:.2f}/MWh\")\n",
    "    \n",
    "    # Example 3: Find the optimal datacenter considering price, renewable energy, and CO2\n",
    "    optimal = power_manager.find_optimal_datacenter(timestamp, \n",
    "                                                   price_weight=0.4, \n",
    "                                                   renewable_weight=0.4, \n",
    "                                                   co2_weight=0.2)\n",
    "    print(f\"\\nOptimal datacenter at {timestamp} (balancing price, renewable energy, and CO2):\")\n",
    "    print(f\"  {optimal['datacenter_name']} ({optimal['location']})\")\n",
    "    print(f\"  Price: ${optimal['price']:.2f}/MWh\")\n",
    "    print(f\"  Renewable: {optimal['renewable']*100:.1f}%\")\n",
    "    print(f\"  CO2: {optimal['co2']} g/kWh\")\n",
    "    print(f\"  Score: {optimal['score']:.3f}\")\n",
    "    \n",
    "    # Example 4: Add a new datacenter\n",
    "    print(\"\\nAdding a new datacenter...\")\n",
    "    new_dc = power_manager.add_new_datacenter(\n",
    "        dc_id=9,\n",
    "        name=\"us-east-new\",\n",
    "        location=\"Atlanta, Georgia\",\n",
    "        renewable=0.45,\n",
    "        co2=410\n",
    "    )\n",
    "    print(f\"  Added {new_dc['info']['name']} in {new_dc['info']['location']}\")\n",
    "    print(f\"  Mapped to ISO region: {new_dc['iso_mapping']['iso_name']}\")\n",
    "    \n",
    "    # Example 5: Plot price comparison over time\n",
    "    print(\"\\nGenerating price comparison plot...\")\n",
    "    start_time = \"2025-03-24 08:00:00\"\n",
    "    end_time = \"2025-03-24 20:00:00\"\n",
    "    \n",
    "    # Select a subset of datacenters to compare\n",
    "    compare_dcs = [0, 4, 6]  # us-central1, us-south1, us-west2\n",
    "    \n",
    "    try:\n",
    "        fig = power_manager.plot_price_comparison(start_time, end_time, compare_dcs)\n",
    "        fig.savefig('datacenter_price_comparison.png')\n",
    "        print(\"  Plot saved as 'datacenter_price_comparison.png'\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error generating plot: {str(e)}\")\n",
    "    \n",
    "    # Example 6: Plot ISO distribution pie chart\n",
    "    print(\"\\nGenerating ISO distribution pie chart...\")\n",
    "    try:\n",
    "        fig = power_manager.plot_iso_distribution_pie()\n",
    "        fig.savefig('datacenter_iso_distribution.png')\n",
    "        print(\"  Plot saved as 'datacenter_iso_distribution.png'\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error generating plot: {str(e)}\")\n",
    "    \n",
    "    # Example 7: Get price time series for a specific datacenter\n",
    "    print(\"\\nGetting price time series for us-west2 (Los Angeles)...\")\n",
    "    try:\n",
    "        dc_id = 6  # us-west2\n",
    "        price_series = power_manager.get_price_series(dc_id, start_time, end_time)\n",
    "        print(f\"  Retrieved {len(price_series)} price points\")\n",
    "        print(\"  Sample of price data:\")\n",
    "        print(price_series.head())\n",
    "    except Exception as e:\n",
    "        print(f\"  Error getting price series: {str(e)}\")\n",
    "    \n",
    "    # Example 8: Save mapping to JSON for future use\n",
    "    print(\"\\nSaving datacenter ISO mapping to JSON...\")\n",
    "    try:\n",
    "        output_file = 'gcp_datacenter_iso_mapping.json'\n",
    "        power_manager.save_mapping_to_json(output_file)\n",
    "        print(f\"  Mapping saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving mapping: {str(e)}\")\n",
    "    \n",
    "    # Example 9: Demonstration of handling missing data\n",
    "    print(\"\\nDemonstrating missing data handling:\")\n",
    "    try:\n",
    "        # Intentionally use a timestamp that might be outside the data range\n",
    "        future_time = \"2025-03-27 10:00:00\"\n",
    "        print(f\"  Attempting to get price for {future_time} (might be outside data range)\")\n",
    "        \n",
    "        # Try to get price with error handling\n",
    "        try:\n",
    "            price = power_manager.get_price(0, future_time)\n",
    "            print(f\"  Successfully got price: ${price:.2f}/MWh\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Handled error: {str(e)}\")\n",
    "            print(\"  Using nearest available timestamp instead...\")\n",
    "            \n",
    "            # Get the timestamp range for this datacenter\n",
    "            iso_name = power_manager.dc_iso_mapping[0]['iso_name']\n",
    "            price_file = power_manager.dc_iso_mapping[0]['price_file']\n",
    "            df = power_manager.load_price_data(iso_name, price_file)\n",
    "            \n",
    "            if 'interval_start_utc' in df.columns:\n",
    "                last_time = df['interval_start_utc'].max()\n",
    "                print(f\"  Last available timestamp: {last_time}\")\n",
    "                \n",
    "                # Get price for the last available timestamp\n",
    "                price = power_manager.get_price(0, last_time)\n",
    "                print(f\"  Price at last available timestamp: ${price:.2f}/MWh\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error in demonstration: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nExample complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
