{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8 ISO data files...\n",
      "Processing CAISO_LMP.csv...\n",
      "Added 573 rows from CAISO_LMP.csv\n",
      "Processing ERCOT_LMP.csv...\n",
      "Added 576 rows from ERCOT_LMP.csv\n",
      "Processing ILLINOIS_MISO_LMP.csv...\n",
      "Added 573 rows from ILLINOIS_MISO_LMP.csv\n",
      "Processing ISONE_LMP.csv...\n",
      "Added 575 rows from ISONE_LMP.csv\n",
      "Processing NYISO_LMP.csv...\n",
      "Added 576 rows from NYISO_LMP.csv\n",
      "Processing Ohio_LMP.csv...\n",
      "Added 575 rows from Ohio_LMP.csv\n",
      "Processing PJM_LMP.csv...\n",
      "Added 575 rows from PJM_LMP.csv\n",
      "Processing SPP_LMP.csv...\n",
      "Added 562 rows from SPP_LMP.csv\n",
      "Combined dataset has 4585 rows and 8 ISOs\n",
      "\n",
      "Time range by ISO:\n",
      "                          min_time                  max_time  count\n",
      "iso                                                                \n",
      "CAISO    2025-03-24 07:00:00+00:00 2025-03-26 06:55:00+00:00    573\n",
      "ERCOT    2025-03-24 05:00:00+00:00 2025-03-26 04:55:00+00:00    576\n",
      "ILLINOIS 2025-03-24 05:00:00+00:00 2025-03-26 04:55:00+00:00    573\n",
      "ISONE    2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    575\n",
      "NYISO    2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    576\n",
      "OHIO     2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    575\n",
      "PJM      2025-03-24 04:00:00+00:00 2025-03-26 03:55:00+00:00    575\n",
      "SPP      2025-03-24 05:00:00+00:00 2025-03-26 04:55:00+00:00    562\n",
      "\n",
      "Creating wide format dataset...\n",
      "Wide format dataset has 612 rows and 9 columns\n",
      "\n",
      "Files saved:\n",
      "- long_format: .\\consolidated_iso_lmp_long_20250326_235401.csv\n",
      "- wide_format: .\\consolidated_iso_lmp_wide_20250326_235401.csv\n",
      "- statistics: .\\iso_lmp_statistics_20250326_235401.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: create a function to extract the ISO name from the filename\n",
    "# Step 2: create a function to read and preprocess data from a single ISO CSV file\n",
    "# Step 3: create a function to consolidate data from multiple ISO CSV files\n",
    "# Step 4: create a function to convert the consolidated data from long to wide format\n",
    "# Step 5: create a function to save both long and wide format datasets\n",
    "\n",
    "def extract_iso_name(filename):\n",
    "    \"\"\"Extract the ISO name from the filename\"\"\"\n",
    "    match = re.match(r'([A-Z]+)_?.*\\.csv', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # Handle special cases like Ohio\n",
    "        if filename.startswith('OHIO'):\n",
    "            return 'OHIO'\n",
    "        elif filename.startswith('ILLINOIS'):\n",
    "            return 'MISO'\n",
    "        return filename.split('.')[0]\n",
    "\n",
    "def read_and_preprocess_iso_data(filepath):\n",
    "    \"\"\"Read and preprocess data from a single ISO CSV file\"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Clean up column names (remove whitespace, lowercase)\n",
    "    df.columns = [col.strip().lower() for col in df.columns]\n",
    "    \n",
    "    # Ensure critical columns are present\n",
    "    required_cols = ['interval_start_utc', 'location', 'lmp']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        raise ValueError(f\"Missing required columns: {missing} in file {filepath}\")\n",
    "    \n",
    "    # Convert timestamps to datetime\n",
    "    df['interval_start_utc'] = pd.to_datetime(df['interval_start_utc'])\n",
    "    \n",
    "    # Extract the ISO name from the filepath\n",
    "    iso_name = extract_iso_name(os.path.basename(filepath))\n",
    "    df['iso'] = iso_name\n",
    "    \n",
    "    # Select and rename columns for consistency\n",
    "    cols_to_keep = ['interval_start_utc', 'iso', 'location', 'lmp']\n",
    "    \n",
    "    # Add optional columns if they exist\n",
    "    optional_cols = ['energy', 'congestion', 'loss']\n",
    "    for col in optional_cols:\n",
    "        if col in df.columns:\n",
    "            cols_to_keep.append(col)\n",
    "    \n",
    "    # Drop rows with missing values in critical columns\n",
    "    df = df.dropna(subset=['interval_start_utc', 'location', 'lmp'])\n",
    "    \n",
    "    return df[cols_to_keep]\n",
    "\n",
    "def consolidate_iso_data(file_list):\n",
    "    \"\"\"Consolidate data from multiple ISO CSV files\"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        try:\n",
    "            print(f\"Processing {file}...\")\n",
    "            df = read_and_preprocess_iso_data(file)\n",
    "            dfs.append(df)\n",
    "            print(f\"Added {len(df)} rows from {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if not dfs:\n",
    "        raise ValueError(\"No data was successfully processed\")\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Fix ISO names\n",
    "    combined_df['iso'] = combined_df['iso'].replace({'O': 'OHIO'})\n",
    "\n",
    "\n",
    "    # Sort by timestamp and ISO\n",
    "    combined_df = combined_df.sort_values(['interval_start_utc', 'iso', 'location'])\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def create_wide_format(df):\n",
    "    \"\"\"Convert the consolidated data from long to wide format\"\"\"\n",
    "    # Create a column name by combining ISO and location\n",
    "    df['iso_location'] = df['iso'] + '_' + df['location']\n",
    "    \n",
    "    # Pivot the dataframe to get timestamps as rows and ISO_location as columns\n",
    "    pivot_df = df.pivot(index='interval_start_utc', columns='iso_location', values='lmp')\n",
    "    \n",
    "    # Reset index to make interval_start_utc a column\n",
    "    pivot_df = pivot_df.reset_index()\n",
    "    \n",
    "    return pivot_df\n",
    "\n",
    "def save_outputs(df_long, df_wide, output_dir='.'):\n",
    "    \"\"\"Save both long and wide format datasets\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp for filenames\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save long format (all data)\n",
    "    long_path = os.path.join(output_dir, f'consolidated_iso_lmp_long_{timestamp}.csv')\n",
    "    df_long.to_csv(long_path, index=False)\n",
    "    \n",
    "    # Save wide format (pivoted)\n",
    "    wide_path = os.path.join(output_dir, f'consolidated_iso_lmp_wide_{timestamp}.csv')\n",
    "    df_wide.to_csv(wide_path, index=False)\n",
    "    \n",
    "    # Save ISO summary statistics\n",
    "    iso_stats = df_long.groupby('iso')['lmp'].agg([\n",
    "        ('min', 'min'),\n",
    "        ('max', 'max'), \n",
    "        ('mean', 'mean'),\n",
    "        ('median', 'median'),\n",
    "        ('std', 'std'),\n",
    "        ('count', 'count')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    stats_path = os.path.join(output_dir, f'iso_lmp_statistics_{timestamp}.csv')\n",
    "    iso_stats.to_csv(stats_path, index=False)\n",
    "    \n",
    "    return {\n",
    "        'long_format': long_path,\n",
    "        'wide_format': wide_path,\n",
    "        'statistics': stats_path\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process ISO data files\"\"\"\n",
    "    # List of ISO files to process\n",
    "    iso_files = [\n",
    "        'CAISO_LMP.csv',\n",
    "        'ERCOT_LMP.csv',\n",
    "        'ILLINOIS_MISO_LMP.csv',\n",
    "        'ISONE_LMP.csv',\n",
    "        'NYISO_LMP.csv',\n",
    "        'Ohio_LMP.csv',\n",
    "        'PJM_LMP.csv',\n",
    "        'SPP_LMP.csv'\n",
    "    ]\n",
    "    \n",
    "    # Verify all files exist\n",
    "    missing_files = [f for f in iso_files if not os.path.exists(f)]\n",
    "    if missing_files:\n",
    "        print(f\"Warning: The following files are missing: {missing_files}\")\n",
    "        iso_files = [f for f in iso_files if os.path.exists(f)]\n",
    "    \n",
    "    # Process the data\n",
    "    print(f\"Processing {len(iso_files)} ISO data files...\")\n",
    "    df_long = consolidate_iso_data(iso_files)\n",
    "    print(f\"Combined dataset has {len(df_long)} rows and {df_long['iso'].nunique()} ISOs\")\n",
    "    \n",
    "    # Generate time interval statistics\n",
    "    time_stats = df_long.groupby('iso')['interval_start_utc'].agg([\n",
    "        ('min_time', 'min'),\n",
    "        ('max_time', 'max'),\n",
    "        ('count', 'count')\n",
    "    ])\n",
    "    print(\"\\nTime range by ISO:\")\n",
    "    print(time_stats)\n",
    "    \n",
    "    # Create wide format\n",
    "    print(\"\\nCreating wide format dataset...\")\n",
    "    df_wide = create_wide_format(df_long)\n",
    "    print(f\"Wide format dataset has {len(df_wide)} rows and {len(df_wide.columns)} columns\")\n",
    "    \n",
    "    # Save outputs\n",
    "    output_paths = save_outputs(df_long, df_wide)\n",
    "    print(\"\\nFiles saved:\")\n",
    "    for format_type, path in output_paths.items():\n",
    "        print(f\"- {format_type}: {path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
