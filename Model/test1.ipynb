{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'electricity_prices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# Generate parameters\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_green_llm_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Summary of key parameters\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# print(f\"Number of user areas: {len(params['I'])}\")\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# print(f\"Number of data centers: {len(params['J'])}\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# Sample of demand\u001b[39;00m\n\u001b[0;32m    402\u001b[0m total_demand_by_hour \u001b[38;5;241m=\u001b[39m {t: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mgenerate_green_llm_parameters\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m c \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m J:\n\u001b[1;32m--> 168\u001b[0m     base_price \u001b[38;5;241m=\u001b[39m \u001b[43melectricity_prices\u001b[49m[j] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100.0\u001b[39m  \u001b[38;5;66;03m# Convert cents to dollars\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m T:\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;66;03m# Apply time-of-day pricing (peak during business hours)\u001b[39;00m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m t \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m20\u001b[39m:  \u001b[38;5;66;03m# Peak hours\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'electricity_prices' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_green_llm_parameters():\n",
    "    \"\"\"\n",
    "    Generate realistic parameters for the Green-LLM optimization model based on\n",
    "    Google data center locations and real-world energy data.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define data center locations\n",
    "    datacenter_info = {\n",
    "        0: {\"name\": \"us-central1\", \"location\": \"Iowa\", \"renewable\": 0.95, \"co2\": 430},\n",
    "        1: {\"name\": \"us-east1\", \"location\": \"South Carolina\", \"renewable\": 0.29, \"co2\": 560},\n",
    "        2: {\"name\": \"us-east4\", \"location\": \"Northern Virginia\", \"renewable\": 0.52, \"co2\": 322},\n",
    "        3: {\"name\": \"us-east5\", \"location\": \"Columbus\", \"renewable\": 0.52, \"co2\": 322},\n",
    "        4: {\"name\": \"us-south1\", \"location\": \"Dallas\", \"renewable\": 0.79, \"co2\": 321},\n",
    "        5: {\"name\": \"us-west1\", \"location\": \"Oregon\", \"renewable\": 0.84, \"co2\": 94},\n",
    "        6: {\"name\": \"us-west2\", \"location\": \"Los Angeles\", \"renewable\": 0.55, \"co2\": 198},\n",
    "        7: {\"name\": \"us-west3\", \"location\": \"Salt Lake City\", \"renewable\": 0.29, \"co2\": 588},\n",
    "        8: {\"name\": \"us-west4\", \"location\": \"Las Vegas\", \"renewable\": 0.26, \"co2\": 373}\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "# Define electricity prices (cents/kWh) based on real average prices by state \n",
    "    population_by_region = {\n",
    "        0: 3.2,    # Iowa\n",
    "        1: 5.1,    # South Carolina \n",
    "        2: 8.6,    # Northern Virginia (using Virginia population)\n",
    "        3: 11.8,   # Ohio (Columbus)\n",
    "        4: 29.0,   # Texas (Dallas)\n",
    "        5: 4.2,    # Oregon\n",
    "        6: 39.5,   # California (Los Angeles)\n",
    "        7: 3.3,    # Utah (Salt Lake City)\n",
    "        8: 3.1     # Nevada (Las Vegas)\n",
    "    }\n",
    "\n",
    "# Define query popularity by type\n",
    "    query_popularity = {\n",
    "        0: 1.5,    # Completion - very common\n",
    "        1: 2.0,    # Chat - most common\n",
    "        2: 0.7,    # Embedding - less common\n",
    "        3: 0.3,    # Fine-tuning - rare\n",
    "        4: 0.8     # Inference - moderately common\n",
    "    }\n",
    "    \n",
    "    # Define spatial and time dimensions\n",
    "    I = list(range(9))  # 12 user areas\n",
    "    J = list(range(9))   # 9 data centers (from Google)\n",
    "    K = list(range(5))   # 5 query types (e.g., completion, chat, embedding, fine-tuning, inference)\n",
    "    T = list(range(24))  # 24 time slots (hours in a day)\n",
    "    R = list(range(3))   # 3 resource types (CPU, GPU, Memory)\n",
    "    DeltaT = list(range(6)) # 6 time slots for water usage\n",
    "\n",
    "    params = {}\n",
    "    \n",
    "    # Add sets\n",
    "    params['I'] = I\n",
    "    params['J'] = J\n",
    "    params['K'] = K\n",
    "    params['T'] = T\n",
    "    params['R'] = R\n",
    "    params['DeltaT'] = DeltaT\n",
    "    \n",
    "    # Create query demand distributions - make these time-of-day dependent\n",
    "    # Different geographical areas will have different peak times\n",
    "    \n",
    "    # Base demand patterns by hour (24-hour cycle)\n",
    "    hourly_patterns = np.array([\n",
    "        0.3, 0.2, 0.15, 0.1, 0.1, 0.15, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95,  # 0-11\n",
    "        1.0, 0.95, 0.9, 0.85, 0.8, 0.85, 0.9, 0.85, 0.75, 0.6, 0.5, 0.4  # 12-23\n",
    "    ])\n",
    "    \n",
    "    # Create time zone offsets for different regions (simplified)\n",
    "    # Assume I = 0-4: East, 5-9: Central, 10-14: Mountain, 15-19: Pacific\n",
    "    time_offsets = np.zeros(len(I), dtype=int)\n",
    "    time_offsets[0:5] = 0      # Eastern\n",
    "    time_offsets[5:10] = 1     # Central \n",
    "    time_offsets[10:15] = 2    # Mountain\n",
    "    time_offsets[15:20] = 3    # Pacific\n",
    "    \n",
    "    # Query parameters - using consistent ordering (i,k,t)\n",
    "    lmbda = {}\n",
    "    total_population = sum(population_by_region.values())\n",
    "    base_demand = 30  # Base demand for a typical region\n",
    "\n",
    "    for i in params['I']:\n",
    "    # Calculate population factor (normalized to keep values in desired range)\n",
    "        pop_factor = population_by_region[i] / total_population * len(params['I'])\n",
    "    \n",
    "        for k in params['K']:\n",
    "            # Get popularity factor for this query type\n",
    "            type_factor = query_popularity[k]\n",
    "        \n",
    "            for t in params['T']:\n",
    "                # Time of day factor - business hours have more traffic\n",
    "                local_hour = t\n",
    "                if 9 <= local_hour < 18:  # Business hours\n",
    "                    time_factor = 1.2\n",
    "                elif 18 <= local_hour < 22:  # Evening\n",
    "                    time_factor = 1.0\n",
    "                else:  # Night/early morning\n",
    "                    time_factor = 0.8\n",
    "                \n",
    "            # Calculate demand with small random variation\n",
    "                raw_demand = base_demand * pop_factor * type_factor * time_factor\n",
    "            \n",
    "            # Add some randomness (Â±15%)\n",
    "                variation = 0.85 + 0.3 * np.random.random()\n",
    "            \n",
    "            # Set final demand, ensuring it stays within 30-50 range\n",
    "                final_demand = max(30, min(50, int(raw_demand * variation)))\n",
    "                lmbda[(i, k, t)] = final_demand\n",
    "    \n",
    "    \n",
    "    # Create a DataFrame for demand analysis\n",
    "    demand_data = []\n",
    "    for i in params['I']:\n",
    "        for k in params['K']:\n",
    "            for t in params['T']:\n",
    "                demand_data.append({\n",
    "                    'region': i, \n",
    "                    'region_name': list(datacenter_info.values())[i]['location'],\n",
    "                    'query_type': k,\n",
    "                    'hour': t,\n",
    "                    'demand': lmbda[(i, k, t)],\n",
    "                    'population': population_by_region[i]\n",
    "                })\n",
    "\n",
    "\n",
    "    # Token counts for different query types\n",
    "    h = {\n",
    "        0: 50,    # Completion - shorter prompts\n",
    "        1: 150,   # Chat - medium prompts\n",
    "        2: 500,   # Embedding - longer texts\n",
    "        3: 1000,  # Fine-tuning - lots of examples\n",
    "        4: 100    # Inference - medium prompts\n",
    "    }\n",
    "    params['h'] = h\n",
    "    \n",
    "    # Output token counts\n",
    "    f = {\n",
    "        0: 200,   # Completion - medium outputs\n",
    "        1: 300,   # Chat - longer conversations\n",
    "        2: 2000,    # Embedding - just vectors\n",
    "        3: 100,   # Fine-tuning - model updates\n",
    "        4: 500    # Inference - detailed outputs\n",
    "    }\n",
    "    params['f'] = f\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Energy consumption coefficients\n",
    "    tau_in = {k: 0.001 + 0.002 * np.random.random() for k in K}  # Input token energy\n",
    "    tau_out = {k: 0.002 + 0.005 * np.random.random() for k in K}  # Output token energy (more computation)\n",
    "    params['tau_in'] = tau_in\n",
    "    params['tau_out'] = tau_out\n",
    "    \n",
    "    # Electricity prices - vary by location and time (peak/off-peak patterns)\n",
    "    c = {}\n",
    "    for j in J:\n",
    "        base_price = electricity_prices[j] / 100.0  # Convert cents to dollars\n",
    "        for t in T:\n",
    "            # Apply time-of-day pricing (peak during business hours)\n",
    "            if 8 <= t < 20:  # Peak hours\n",
    "                factor = 1.2 + 0.3 * np.random.random()  # 20-50% premium\n",
    "            else:  # Off-peak\n",
    "                factor = 0.8 + 0.3 * np.random.random()  # 20-50% discount\n",
    "            \n",
    "            c[(j, t)] = base_price * factor\n",
    "    params['c'] = c\n",
    "    \n",
    "    \n",
    "    # Carbon intensity (kg CO2/kWh) - from Google data\n",
    "    theta = {j: datacenter_info[j]['co2'] / 1000.0 for j in J}  # Convert g to kg\n",
    "    params['theta'] = theta\n",
    "    \n",
    "    # Carbon tax ($/kg CO2)\n",
    "    carbon_tax_base = 50.0  # $/ton = 0.05 $/kg\n",
    "    delta = {(j, t): carbon_tax_base * (0.001 + 0.01 * np.random.random()) for j in J for t in T}\n",
    "    params['delta'] = delta\n",
    "    \n",
    "    # Renewable energy availability (kW)\n",
    "    P_w = {}\n",
    "    for j in J:\n",
    "        renewable_percentage = datacenter_info[j]['renewable']\n",
    "        base_capacity = np.random.randint(2000, 5000)  # Base datacenter capacity\n",
    "        \n",
    "        for t in T:\n",
    "            # Solar generation curve (for daytime hours)\n",
    "            solar_factor = 0\n",
    "            if 6 <= t < 18:  # Daylight hours\n",
    "                # Peak at noon, lower at dawn/dusk\n",
    "                hour_factor = 1.0 - abs(t - 12) / 6.0\n",
    "                solar_factor = hour_factor * renewable_percentage\n",
    "                \n",
    "            # Wind generation (less predictable)\n",
    "            wind_factor = 0.5 * renewable_percentage * np.random.random()\n",
    "            \n",
    "            # Total renewable factor\n",
    "            total_factor = solar_factor + wind_factor\n",
    "            \n",
    "            # Final renewable capacity for this data center and time\n",
    "            P_w[(j, t)] = base_capacity * total_factor\n",
    "    params['P_w'] = P_w\n",
    "    \n",
    "    # Maximum power from grid (kW)\n",
    "    P_max = {(j, t): np.random.randint(30000, 50000) for j in J for t in T}\n",
    "    params['P_max'] = P_max\n",
    "    \n",
    "    \n",
    "    # Power Usage Effectiveness\n",
    "    # Modern data centers range from 1.1 (very efficient) to 1.5 (average)\n",
    "    PUE = {j: 1.1 + 0.4 * np.random.random() for j in J}\n",
    "    params['PUE'] = PUE\n",
    "    \n",
    "    # Resource capacity (units)\n",
    "    C = {(r, j): np.random.randint(1000, 2000) for r in R for j in J}\n",
    "    params['C'] = C\n",
    "    \n",
    "    # Resource requirements per query type\n",
    "    alpha = {}\n",
    "    for k in K:\n",
    "        for r in R:\n",
    "            # Different query types need different resources\n",
    "            if r == 0:  # CPU\n",
    "                alpha[(k, r)] = 0.01 + 0.05 * np.random.random()\n",
    "            elif r == 1:  # GPU\n",
    "                # Fine-tuning and large models need more GPU\n",
    "                if k in [2, 3]:\n",
    "                    alpha[(k, r)] = 0.05 + 0.15 * np.random.random()\n",
    "                else:\n",
    "                    alpha[(k, r)] = 0.01 + 0.05 * np.random.random()\n",
    "            else:  # Memory\n",
    "                alpha[(k, r)] = 0.02 + 0.08 * np.random.random()\n",
    "    params['alpha'] = alpha\n",
    "    \n",
    "    # Previous model placement\n",
    "    # Initially have some models placed but not all\n",
    "    z_prev = {}\n",
    "    for j in J:\n",
    "        for k in K:\n",
    "            for t in T:\n",
    "                if t == 0:  # Initial placement\n",
    "                    z_prev[(j, k, t)] = np.random.randint(0, 2)\n",
    "                else:\n",
    "                    # For subsequent time slots, we'd use optimization output\n",
    "                    # But for t=1, just use random placement\n",
    "                    z_prev[(j, k, t)] = z_prev[(j, k, 0)]\n",
    "    params['z_prev'] = z_prev\n",
    "    \n",
    "    # Model download cost (units)\n",
    "    f_download = {j: 10 + 20 * np.random.random() for j in J}\n",
    "    params['f_download'] = f_download\n",
    "    \n",
    "\n",
    "\n",
    "    s = {}\n",
    "    for i in I:\n",
    "        for k in K:\n",
    "            for t in T:\n",
    "                # Random penalty between 10 and 15\n",
    "                s[(i, k, t)] = 10 + 5 * np.random.random()\n",
    "    params['s'] = s\n",
    "\n",
    "    \n",
    "    # Maximum unmet demand ratio\n",
    "    Gamma = {i: 0.2 + 0.2 * np.random.random() for i in I}\n",
    "    params['Gamma'] = Gamma\n",
    "    \n",
    "    # Error rates\n",
    "    e = {\n",
    "        0: 0.02,  # Completion\n",
    "        1: 0.03,  # Chat\n",
    "        2: 0.01,  # Embedding\n",
    "        3: 0.05,  # Fine-tuning\n",
    "        4: 0.02   # Inference\n",
    "    }\n",
    "    params['e'] = e\n",
    "    \n",
    "    # Minimum accuracy\n",
    "    E = {k: 0.60 + 0.08 * np.random.random() for k in K}\n",
    "    params['E'] = E\n",
    "    \n",
    "    # Token size (KB)\n",
    "    beta = {(i, k, t): 0.5 + 1.5 * np.random.random() for i in I for k in K for t in T}\n",
    "    params['beta'] = beta\n",
    "    \n",
    "    # Network bandwidth (Mbps)\n",
    "    B = {}\n",
    "    for i in I:\n",
    "        for j in J:\n",
    "            # Distance factor: closer regions have higher bandwidth\n",
    "            region_i = i // 5  # 0=East, 1=Central, 2=Mountain, 3=West\n",
    "            region_j = 0\n",
    "            if j in [0]:\n",
    "                region_j = 1  # Central\n",
    "            elif j in [1, 2, 3]:\n",
    "                region_j = 0  # East\n",
    "            elif j in [4]:\n",
    "                region_j = 1  # South (treat as Central)\n",
    "            else:\n",
    "                region_j = 3  # West\n",
    "                \n",
    "            distance = abs(region_i - region_j)\n",
    "            base_bw = 20000.0  # 1 Gbps base\n",
    "            \n",
    "            # Further distances have more bandwidth variability and lower average\n",
    "            bandwidth = base_bw * (1.0 - 0.1 * distance) * (0.7 + 0.6 * np.random.random())\n",
    "            B[(i, j)] = max(500, bandwidth)  # Minimum 100 Mbps\n",
    "    params['B'] = B\n",
    "    \n",
    "    # Network delay (s)\n",
    "    d = {}\n",
    "    for i in I:\n",
    "        for j in J:\n",
    "            # Similar to bandwidth, but inverse relationship\n",
    "            region_i = i // 5\n",
    "            region_j = 0\n",
    "            if j in [0]:\n",
    "                region_j = 1\n",
    "            elif j in [1, 2, 3]:\n",
    "                region_j = 0\n",
    "            elif j in [4]:\n",
    "                region_j = 1\n",
    "            else:\n",
    "                region_j = 3\n",
    "                \n",
    "            distance = abs(region_i - region_j)\n",
    "            \n",
    "            # Base latency plus distance factor (in seconds)\n",
    "            delay = 0.01 + 0.01 * distance + 0.02 * np.random.random()\n",
    "            d[(i, j)] = delay\n",
    "    params['d'] = d\n",
    "    \n",
    "    # Processing delay (s/token)\n",
    "    v = {(j, t): 0.001 + 0.002 * np.random.random() for j in J for t in T}\n",
    "    params['v'] = v\n",
    "    \n",
    "    # Delay threshold (s)\n",
    "    Delta = {}\n",
    "    for i in I:\n",
    "        for k in K:\n",
    "            # Different thresholds for different query types\n",
    "            base_threshold = 0.5\n",
    "            if k in [2, 3]:  # Embedding and fine-tuning can take longer\n",
    "                base_threshold = 2.0\n",
    "            elif k == 4:  # Inference\n",
    "                base_threshold = 1.0\n",
    "                \n",
    "            for t in T:\n",
    "                Delta[(i, k, t)] = base_threshold * (0.8 + 0.4 * np.random.random())\n",
    "    params['Delta'] = Delta\n",
    "    \n",
    "    # Water Usage Effectiveness\n",
    "    WUE = {(j, t): 0.2 + 0.3 * np.random.random() for j in J for t in T}\n",
    "    params['WUE'] = WUE\n",
    "    \n",
    "    # Energy-Water Intensity Factor\n",
    "    EWIF = {(j, t): 0.01 + 0.01 * np.random.random() for j in J for t in T}\n",
    "    params['EWIF'] = EWIF\n",
    "    \n",
    "    # Maximum water consumption\n",
    "    Z = np.random.uniform(8000, 12000)\n",
    "    params['Z'] = Z\n",
    "    return params\n",
    "    \n",
    "# Generate parameters\n",
    "params = generate_green_llm_parameters()\n",
    "\n",
    "# Summary of key parameters\n",
    "# print(f\"Number of user areas: {len(params['I'])}\")\n",
    "# print(f\"Number of data centers: {len(params['J'])}\")\n",
    "# print(f\"Number of query types: {len(params['K'])}\")\n",
    "# print(f\"Number of time slots: {len(params['T'])}\")\n",
    "# print(f\"Number of resource types: {len(params['R'])}\")\n",
    "\n",
    "# Sample of electricity prices ($/kWh)\n",
    "# print(\"\\nSample electricity prices ($/kWh):\")\n",
    "# for j in params['J']:\n",
    "#     for t in range(0, 24, 8):\n",
    "#         print(f\"Data center {j} at time {t}: ${params['c'][(j, t)]:.4f}\")\n",
    "\n",
    "# Sample of CO2 intensities\n",
    "# print(\"\\nCO2 intensities (kg/kWh):\")\n",
    "# for j in params['J']:\n",
    "#     print(f\"Data center {j} ({list(params['theta'].values())[j]:.4f} kg/kWh)\")\n",
    "\n",
    "# # Sample of renewable energy availability\n",
    "# print(\"\\nSample renewable energy availability (kW):\")\n",
    "# for j in params['J']:\n",
    "#     for t in range(0, 24, 8):\n",
    "#         print(f\"Data center {j} at time {t}: {params['P_w'][(j, t)]:.2f} kW\")\n",
    "\n",
    "# Sample of demand\n",
    "total_demand_by_hour = {t: 0 for t in params['T']}\n",
    "for i in params['I']:\n",
    "    for k in params['K']:\n",
    "        for t in params['T']:\n",
    "            total_demand_by_hour[t] += params['lambda'][(i, k, t)]\n",
    "\n",
    "total_demand_by_hour\n",
    "# Function to save parameters to file if needed\n",
    "# def save_params_to_file(params, filename='green_llm_params.py'):\n",
    "#     with open(filename, 'w') as f:\n",
    "#         f.write(\"# Auto-generated Green-LLM parameters\\n\\n\")\n",
    "#         f.write(\"def get_parameters():\\n\")\n",
    "#         f.write(\"    params = {}\\n\\n\")\n",
    "        \n",
    "#         # Write basic sets\n",
    "#         for set_name in ['I', 'J', 'K', 'T', 'R']:\n",
    "#             f.write(f\"    params['{set_name}'] = {params[set_name]}\\n\")\n",
    "        \n",
    "#         # Write dictionaries\n",
    "#         for key, value in params.items():\n",
    "#             if key not in ['I', 'J', 'K', 'T', 'R']:\n",
    "#                 f.write(f\"\\n    # {key}\\n\")\n",
    "#                 f.write(f\"    params['{key}'] = {value}\\n\")\n",
    "        \n",
    "#         f.write(\"\\n    return params\\n\")\n",
    "\n",
    "# Uncomment to save to file\n",
    "# save_params_to_file(params)\n",
    "\n",
    "# Return parameters for use in optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "Current working directory: c:\\Users\\jiami\\ASU Dropbox\\Jiaming Cheng\\Network_letter\n",
      "Files in current directory:\n",
      "  - avgprice_annual.xlsx\n",
      "  - clean_data.csv\n",
      "  - clean_data.csv.zip\n",
      "  - dataset\n",
      "  - demand_plots.png\n",
      "  - Green_LLM.py\n",
      "  - green_llm_model.py\n",
      "  - Green_LLM_opt.py\n",
      "  - model_iis.ilp\n",
      "  - model_test.ipynb\n",
      "  - test1.ipynb\n",
      "Found clean_data.csv in current directory, trying with relative path...\n",
      "Reading data from clean_data.csv...\n",
      "Filtered data to 15190 rows from 2020-2024.\n",
      "Calculating state summaries...\n",
      "Calculating yearly trends by state...\n",
      "Saving data in JSON format...\n",
      "Data extraction complete. Files saved to extracted_data/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def extract_electricity_data(filepath, output_dir='extracted_data'):\n",
    "    \"\"\"\n",
    "    Extract electricity price data from 2020-2024 and save in various formats.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the CSV file containing electricity data\n",
    "    output_dir : str\n",
    "        Directory to save extracted data files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the extracted DataFrames\n",
    "    \"\"\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    print(f\"Reading data from {filepath}...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Filter for 2020-2024\n",
    "    df_filtered = df[(df['year'] >= 2020) & (df['year'] <= 2024)].copy()\n",
    "    print(f\"Filtered data to {len(df_filtered)} rows from 2020-2024.\")\n",
    "    \n",
    "    # Create output dictionary\n",
    "    output_data = {}\n",
    "    \n",
    "    # 1. All sectors data (main dataset for analysis)\n",
    "    all_sectors = df_filtered[df_filtered['sectorName'] == 'all sectors'].copy()\n",
    "    output_data['all_sectors'] = all_sectors\n",
    "    \n",
    "    # Save all sectors data\n",
    "    all_sectors.to_csv(f\"{output_dir}/electricity_all_sectors_2020_2024.csv\", index=False)\n",
    "    all_sectors.to_excel(f\"{output_dir}/electricity_all_sectors_2020_2024.xlsx\", index=False)\n",
    "    \n",
    "    # 2. Calculate state summaries\n",
    "    print(\"Calculating state summaries...\")\n",
    "    state_summaries = []\n",
    "    \n",
    "    for state in all_sectors['stateDescription'].unique():\n",
    "        state_data = all_sectors[all_sectors['stateDescription'] == state]\n",
    "        \n",
    "        # Calculate average price, total revenue, etc.\n",
    "        avg_price = state_data['price'].mean()\n",
    "        total_revenue = state_data['revenue'].sum()\n",
    "        total_sales = state_data['sales'].sum()\n",
    "        avg_customers = state_data['customers'].mean()\n",
    "        \n",
    "        state_summaries.append({\n",
    "            'state': state,\n",
    "            'avg_price': round(avg_price, 2),\n",
    "            'total_revenue': round(total_revenue, 2),\n",
    "            'total_sales': round(total_sales, 2),\n",
    "            'avg_customers': round(avg_customers) if not np.isnan(avg_customers) else None\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame and sort by price\n",
    "    state_summary_df = pd.DataFrame(state_summaries)\n",
    "    state_summary_df = state_summary_df.sort_values('avg_price', ascending=False)\n",
    "    output_data['state_summary'] = state_summary_df\n",
    "    \n",
    "    # Save state summary\n",
    "    state_summary_df.to_csv(f\"{output_dir}/state_summary_2020_2024.csv\", index=False)\n",
    "    state_summary_df.to_excel(f\"{output_dir}/state_summary_2020_2024.xlsx\", index=False)\n",
    "    \n",
    "    # 3. Calculate yearly trends by state\n",
    "    print(\"Calculating yearly trends by state...\")\n",
    "    yearly_trends = []\n",
    "    \n",
    "    for state in all_sectors['stateDescription'].unique():\n",
    "        state_data = all_sectors[all_sectors['stateDescription'] == state]\n",
    "        \n",
    "        for year in range(2020, 2025):\n",
    "            year_data = state_data[state_data['year'] == year]\n",
    "            \n",
    "            if not year_data.empty:\n",
    "                avg_price = year_data['price'].mean()\n",
    "                \n",
    "                yearly_trends.append({\n",
    "                    'state': state,\n",
    "                    'year': year,\n",
    "                    'avg_price': round(avg_price, 2)\n",
    "                })\n",
    "    \n",
    "    yearly_trends_df = pd.DataFrame(yearly_trends)\n",
    "    output_data['yearly_trends'] = yearly_trends_df\n",
    "    \n",
    "    # Save yearly trends\n",
    "    yearly_trends_df.to_csv(f\"{output_dir}/yearly_trends_2020_2024.csv\", index=False)\n",
    "    yearly_trends_df.to_excel(f\"{output_dir}/yearly_trends_2020_2024.xlsx\", index=False)\n",
    "    \n",
    "    # 4. Identify top and bottom states\n",
    "    top_states = state_summary_df.head(10).copy()\n",
    "    bottom_states = state_summary_df.tail(10).copy()\n",
    "    \n",
    "    output_data['top_states'] = top_states\n",
    "    output_data['bottom_states'] = bottom_states\n",
    "    \n",
    "    # Save top/bottom states\n",
    "    top_states.to_csv(f\"{output_dir}/top_price_states.csv\", index=False)\n",
    "    bottom_states.to_csv(f\"{output_dir}/bottom_price_states.csv\", index=False)\n",
    "    \n",
    "    # 5. Save data in JSON format (for web applications)\n",
    "    print(\"Saving data in JSON format...\")\n",
    "    state_summary_df.to_json(f\"{output_dir}/state_summary.json\", orient='records')\n",
    "    yearly_trends_df.to_json(f\"{output_dir}/yearly_trends.json\", orient='records')\n",
    "    \n",
    "    # 6. Create a pivot table for easier analysis\n",
    "    pivot_df = yearly_trends_df.pivot(index='state', columns='year', values='avg_price')\n",
    "    pivot_df.to_csv(f\"{output_dir}/price_pivot_table.csv\")\n",
    "    pivot_df.to_excel(f\"{output_dir}/price_pivot_table.xlsx\")\n",
    "    output_data['pivot_table'] = pivot_df\n",
    "    \n",
    "    print(f\"Data extraction complete. Files saved to {output_dir}/\")\n",
    "    return output_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Usage example\n",
    "    # 1. Check if the file exists\n",
    "    file_path = r\"C:\\Users\\jiami\\ASU Dropbox\\Jiaming Cheng\\Network_letter\\clean_data.csv\"\n",
    "    print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "    # 2. Check the current working directory\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "    # 3. List files in the current directory\n",
    "    print(\"Files in current directory:\")\n",
    "    for file in os.listdir():\n",
    "        print(f\"  - {file}\")\n",
    "\n",
    "    # 4. Try with a relative path if the file is in the same folder as your notebook\n",
    "    if \"clean_data.csv\" in os.listdir():\n",
    "        print(\"Found clean_data.csv in current directory, trying with relative path...\")\n",
    "        extracted_data = extract_electricity_data(\"clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from clean_data.csv...\n",
      "Filtered data to 15190 rows from 2020-2024.\n",
      "Organizing monthly data by state...\n",
      "Calculating state summaries...\n",
      "Calculating monthly trends...\n",
      "Calculating yearly trends by state...\n",
      "Saving data in JSON format...\n",
      "Data extraction complete. Files saved to extracted_data/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "def extract_electricity_data(filepath, output_dir='extracted_data'):\n",
    "    \"\"\"\n",
    "    Extract electricity price data from 2020-2024 and save in various formats.\n",
    "    Preserves monthly data for all states.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the CSV file containing electricity data\n",
    "    output_dir : str\n",
    "        Directory to save extracted data files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the extracted DataFrames\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    print(f\"Reading data from {filepath}...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Filter for 2020-2024\n",
    "    df_filtered = df[(df['year'] >= 2020) & (df['year'] <= 2024)].copy()\n",
    "    print(f\"Filtered data to {len(df_filtered)} rows from 2020-2024.\")\n",
    "    \n",
    "    # Create output dictionary\n",
    "    output_data = {}\n",
    "    \n",
    "    # 1. All sectors data (main dataset for analysis)\n",
    "    all_sectors = df_filtered[df_filtered['sectorName'] == 'all sectors'].copy()\n",
    "    output_data['all_sectors'] = all_sectors\n",
    "    \n",
    "    # Save all sectors data (preserves monthly data)\n",
    "    all_sectors.to_csv(f\"{output_dir}/electricity_all_sectors_2020_2024.csv\", index=False)\n",
    "    all_sectors.to_excel(f\"{output_dir}/electricity_all_sectors_2020_2024.xlsx\", index=False)\n",
    "    \n",
    "    # 2. Save monthly data by state (key requirement)\n",
    "    print(\"Organizing monthly data by state...\")\n",
    "    \n",
    "    # Create a subdirectory for state-specific data\n",
    "    state_dir = f\"{output_dir}/states\"\n",
    "    if not os.path.exists(state_dir):\n",
    "        os.makedirs(state_dir)\n",
    "    \n",
    "    # Get list of all states\n",
    "    all_states = sorted(all_sectors['stateDescription'].unique())\n",
    "    \n",
    "    # Create a dictionary to store monthly data by state\n",
    "    monthly_data_by_state = {}\n",
    "    \n",
    "    for state in all_states:\n",
    "        # Filter data for this state\n",
    "        state_data = all_sectors[all_sectors['stateDescription'] == state].copy()\n",
    "        \n",
    "        # Sort by year and month\n",
    "        state_data = state_data.sort_values(['year', 'month'])\n",
    "        \n",
    "        # Save to CSV and Excel\n",
    "        state_data.to_csv(f\"{state_dir}/{state.replace(' ', '_')}_monthly_2020_2024.csv\", index=False)\n",
    "        state_data.to_excel(f\"{state_dir}/{state.replace(' ', '_')}_monthly_2020_2024.xlsx\", index=False)\n",
    "        \n",
    "        # Store in dictionary\n",
    "        monthly_data_by_state[state] = state_data\n",
    "        \n",
    "    output_data['monthly_by_state'] = monthly_data_by_state\n",
    "    \n",
    "    # 3. Create a comprehensive monthly dataset\n",
    "    monthly_all = all_sectors.sort_values(['stateDescription', 'year', 'month']).copy()\n",
    "    monthly_all.to_csv(f\"{output_dir}/monthly_all_states_2020_2024.csv\", index=False)\n",
    "    monthly_all.to_excel(f\"{output_dir}/monthly_all_states_2020_2024.xlsx\", index=False)\n",
    "    output_data['monthly_all'] = monthly_all\n",
    "    \n",
    "    # 4. Calculate state summaries\n",
    "    print(\"Calculating state summaries...\")\n",
    "    state_summaries = []\n",
    "    \n",
    "    for state in all_states:\n",
    "        state_data = all_sectors[all_sectors['stateDescription'] == state]\n",
    "        \n",
    "        # Calculate average price, total revenue, etc.\n",
    "        avg_price = state_data['price'].mean()\n",
    "        total_revenue = state_data['revenue'].sum()\n",
    "        total_sales = state_data['sales'].sum()\n",
    "        avg_customers = state_data['customers'].mean()\n",
    "        \n",
    "        # Calculate monthly statistics\n",
    "        monthly_stats = []\n",
    "        for year in range(2020, 2025):\n",
    "            for month in range(1, 13):\n",
    "                month_data = state_data[(state_data['year'] == year) & (state_data['month'] == month)]\n",
    "                if not month_data.empty:\n",
    "                    monthly_stats.append({\n",
    "                        'year': year,\n",
    "                        'month': month,\n",
    "                        'price': float(month_data['price'].values[0]) if not month_data['price'].isna().all() else None,\n",
    "                        'revenue': float(month_data['revenue'].values[0]) if not month_data['revenue'].isna().all() else None,\n",
    "                        'sales': float(month_data['sales'].values[0]) if not month_data['sales'].isna().all() else None,\n",
    "                        'customers': float(month_data['customers'].values[0]) if not month_data['customers'].isna().all() else None\n",
    "                    })\n",
    "        \n",
    "        state_summaries.append({\n",
    "            'state': state,\n",
    "            'avg_price': round(avg_price, 2),\n",
    "            'total_revenue': round(total_revenue, 2),\n",
    "            'total_sales': round(total_sales, 2),\n",
    "            'avg_customers': round(avg_customers) if not np.isnan(avg_customers) else None,\n",
    "            'monthly_data': monthly_stats\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame (excluding monthly_data for DataFrame version)\n",
    "    state_summary_df = pd.DataFrame([{k: v for k, v in summary.items() if k != 'monthly_data'} \n",
    "                                     for summary in state_summaries])\n",
    "    state_summary_df = state_summary_df.sort_values('avg_price', ascending=False)\n",
    "    output_data['state_summary'] = state_summary_df\n",
    "    \n",
    "    # Save state summary\n",
    "    state_summary_df.to_csv(f\"{output_dir}/state_summary_2020_2024.csv\", index=False)\n",
    "    state_summary_df.to_excel(f\"{output_dir}/state_summary_2020_2024.xlsx\", index=False)\n",
    "    \n",
    "    # Save the full summary with monthly data as JSON\n",
    "    with open(f\"{output_dir}/state_summary_with_monthly.json\", 'w') as f:\n",
    "        json.dump(state_summaries, f, indent=2)\n",
    "    \n",
    "    # 5. Calculate monthly trends for all states\n",
    "    print(\"Calculating monthly trends...\")\n",
    "    monthly_trends = []\n",
    "    \n",
    "    for year in range(2020, 2025):\n",
    "        for month in range(1, 13):\n",
    "            month_data = all_sectors[(all_sectors['year'] == year) & (all_sectors['month'] == month)]\n",
    "            \n",
    "            if not month_data.empty:\n",
    "                avg_price = month_data['price'].mean()\n",
    "                \n",
    "                monthly_trends.append({\n",
    "                    'year': year,\n",
    "                    'month': month,\n",
    "                    'date': f\"{year}-{month:02d}\",\n",
    "                    'avg_price': round(avg_price, 2)\n",
    "                })\n",
    "    \n",
    "    monthly_trends_df = pd.DataFrame(monthly_trends)\n",
    "    output_data['monthly_trends'] = monthly_trends_df\n",
    "    \n",
    "    # Save monthly trends\n",
    "    monthly_trends_df.to_csv(f\"{output_dir}/monthly_trends_2020_2024.csv\", index=False)\n",
    "    monthly_trends_df.to_excel(f\"{output_dir}/monthly_trends_2020_2024.xlsx\", index=False)\n",
    "    \n",
    "    # 6. Calculate yearly trends by state\n",
    "    print(\"Calculating yearly trends by state...\")\n",
    "    yearly_trends = []\n",
    "    \n",
    "    for state in all_states:\n",
    "        state_data = all_sectors[all_sectors['stateDescription'] == state]\n",
    "        \n",
    "        for year in range(2020, 2025):\n",
    "            year_data = state_data[state_data['year'] == year]\n",
    "            \n",
    "            if not year_data.empty:\n",
    "                avg_price = year_data['price'].mean()\n",
    "                \n",
    "                yearly_trends.append({\n",
    "                    'state': state,\n",
    "                    'year': year,\n",
    "                    'avg_price': round(avg_price, 2)\n",
    "                })\n",
    "    \n",
    "    yearly_trends_df = pd.DataFrame(yearly_trends)\n",
    "    output_data['yearly_trends'] = yearly_trends_df\n",
    "    \n",
    "    # Save yearly trends\n",
    "    yearly_trends_df.to_csv(f\"{output_dir}/yearly_trends_2020_2024.csv\", index=False)\n",
    "    yearly_trends_df.to_excel(f\"{output_dir}/yearly_trends_2020_2024.xlsx\", index=False)\n",
    "    \n",
    "    # 7. Identify top and bottom states\n",
    "    top_states = state_summary_df.head(10).copy()\n",
    "    bottom_states = state_summary_df.tail(10).copy()\n",
    "    \n",
    "    output_data['top_states'] = top_states\n",
    "    output_data['bottom_states'] = bottom_states\n",
    "    \n",
    "    # Save top/bottom states\n",
    "    top_states.to_csv(f\"{output_dir}/top_price_states.csv\", index=False)\n",
    "    bottom_states.to_csv(f\"{output_dir}/bottom_price_states.csv\", index=False)\n",
    "    \n",
    "    # 8. Save data in JSON format (for web applications)\n",
    "    print(\"Saving data in JSON format...\")\n",
    "    state_summary_df.to_json(f\"{output_dir}/state_summary.json\", orient='records')\n",
    "    yearly_trends_df.to_json(f\"{output_dir}/yearly_trends.json\", orient='records')\n",
    "    monthly_trends_df.to_json(f\"{output_dir}/monthly_trends.json\", orient='records')\n",
    "    \n",
    "    # 9. Create pivot tables for easier analysis\n",
    "    # Yearly pivot table\n",
    "    yearly_pivot_df = yearly_trends_df.pivot(index='state', columns='year', values='avg_price')\n",
    "    yearly_pivot_df.to_csv(f\"{output_dir}/yearly_price_pivot_table.csv\")\n",
    "    yearly_pivot_df.to_excel(f\"{output_dir}/yearly_price_pivot_table.xlsx\")\n",
    "    output_data['yearly_pivot'] = yearly_pivot_df\n",
    "    \n",
    "    # Monthly pivot table\n",
    "    monthly_pivot = pd.pivot_table(\n",
    "        all_sectors,\n",
    "        values='price',\n",
    "        index=['stateDescription'],\n",
    "        columns=['year', 'month'],\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    monthly_pivot.to_csv(f\"{output_dir}/monthly_price_pivot_table.csv\")\n",
    "    monthly_pivot.to_excel(f\"{output_dir}/monthly_price_pivot_table.xlsx\")\n",
    "    output_data['monthly_pivot'] = monthly_pivot\n",
    "    \n",
    "    print(f\"Data extraction complete. Files saved to {output_dir}/\")\n",
    "    return output_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Usage example\n",
    "    extract_electricity_data('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading electricity data from monthly_all_states_2020_2024.xlsx...\n",
      "Loaded 3038 rows of data\n",
      "Sample of first 3 rows:\n",
      "   year  month stateDescription   sectorName  customers  price    revenue  \\\n",
      "0  2020      1          Alabama  all sectors    2639176   9.53  683.99992   \n",
      "1  2020      2          Alabama  all sectors    2639076   9.44  635.71477   \n",
      "2  2020      3          Alabama  all sectors    2648998   9.34  605.21129   \n",
      "\n",
      "        sales  \n",
      "0  7176.42851  \n",
      "1  6737.74734  \n",
      "2  6479.78199  \n",
      "Adding RTO/ISO information...\n",
      "\n",
      "Number of entries by RTO/ISO:\n",
      "RTO_ISO\n",
      "PJM         588\n",
      "MISO        490\n",
      "WEIM        441\n",
      "Multiple    441\n",
      "Other       343\n",
      "ISO-NE      343\n",
      "SPP         245\n",
      "CAISO        49\n",
      "NYISO        49\n",
      "ERCOT        49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saving modified data to monthly_all_states_with_rto_2020_2024.xlsx...\n",
      "Done!\n",
      "\n",
      "Sample of modified data:\n",
      "   year  month stateDescription RTO_ISO  price\n",
      "0  2020      1          Alabama   Other   9.53\n",
      "1  2020      2          Alabama   Other   9.44\n",
      "2  2020      3          Alabama   Other   9.34\n",
      "3  2020      4          Alabama   Other   9.62\n",
      "4  2020      5          Alabama   Other   9.73\n",
      "5  2020      6          Alabama   Other  10.31\n",
      "6  2020      7          Alabama   Other  10.41\n",
      "7  2020      8          Alabama   Other  10.33\n",
      "8  2020      9          Alabama   Other  10.36\n",
      "9  2020     10          Alabama   Other   9.83\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kaleido\n",
    "def add_rto_to_electricity_data(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Add RTO/ISO information to electricity data based on state location.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to the input Excel file containing electricity data\n",
    "    output_file : str\n",
    "        Path where the modified Excel file will be saved\n",
    "    \"\"\"\n",
    "    print(f\"Reading electricity data from {input_file}...\")\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # Print dataset info\n",
    "    print(f\"Loaded {len(df)} rows of data\")\n",
    "    print(\"Sample of first 3 rows:\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    # Create a mapping of states to RTOs/ISOs\n",
    "    # This mapping is based on the RTO/ISO information provided\n",
    "    state_to_rto_mapping = {\n",
    "        # CAISO (California ISO)\n",
    "        \"California\": \"CAISO\",\n",
    "        \n",
    "        # ERCOT (Electric Reliability Council of Texas)\n",
    "        \"Texas\": \"ERCOT\",\n",
    "        \n",
    "        # PJM Interconnection\n",
    "        \"Delaware\": \"PJM\",\n",
    "        \"Illinois\": \"PJM\",  # Note: Parts of Illinois are in MISO too\n",
    "        \"Indiana\": \"PJM\",   # Note: Parts of Indiana are in MISO too\n",
    "        \"Kentucky\": \"PJM\",  # Note: Parts of Kentucky are in MISO too\n",
    "        \"Maryland\": \"PJM\",\n",
    "        \"Michigan\": \"PJM\",  # Note: Parts of Michigan are in MISO too\n",
    "        \"New Jersey\": \"PJM\",\n",
    "        \"North Carolina\": \"PJM\",  # Note: Only parts of NC are in PJM\n",
    "        \"Ohio\": \"PJM\",\n",
    "        \"Pennsylvania\": \"PJM\",\n",
    "        \"Tennessee\": \"PJM\",  # Note: Only parts of TN are in PJM\n",
    "        \"Virginia\": \"PJM\",\n",
    "        \"West Virginia\": \"PJM\",\n",
    "        \"District of Columbia\": \"PJM\",\n",
    "        \n",
    "        # MISO (Midcontinent Independent System Operator)\n",
    "        \"Minnesota\": \"MISO\",\n",
    "        \"Iowa\": \"MISO\",\n",
    "        \"Missouri\": \"MISO\",\n",
    "        \"Wisconsin\": \"MISO\",\n",
    "        \"Michigan\": \"MISO\",  # Note: Overlap with PJM\n",
    "        \"Illinois\": \"MISO\",  # Note: Overlap with PJM\n",
    "        \"Indiana\": \"MISO\",   # Note: Overlap with PJM\n",
    "        \"Kentucky\": \"MISO\",  # Note: Overlap with PJM\n",
    "        \"Arkansas\": \"MISO\",  # Note: Parts of Arkansas are in SPP\n",
    "        \"Mississippi\": \"MISO\",\n",
    "        \"Louisiana\": \"MISO\",\n",
    "        \"North Dakota\": \"MISO\",\n",
    "        \"South Dakota\": \"MISO\",\n",
    "        \n",
    "        # SPP (Southwest Power Pool)\n",
    "        \"Oklahoma\": \"SPP\",\n",
    "        \"Kansas\": \"SPP\",\n",
    "        \"Nebraska\": \"SPP\",\n",
    "        \"Arkansas\": \"SPP\",    # Note: Overlap with MISO\n",
    "        \"Louisiana\": \"SPP\",   # Note: Overlap with MISO\n",
    "        \"Missouri\": \"SPP\",    # Note: Overlap with MISO\n",
    "        \"New Mexico\": \"SPP\",\n",
    "        \n",
    "        # NYISO (New York ISO)\n",
    "        \"New York\": \"NYISO\",\n",
    "        \n",
    "        # ISO-NE (ISO New England)\n",
    "        \"Connecticut\": \"ISO-NE\",\n",
    "        \"Maine\": \"ISO-NE\",\n",
    "        \"Massachusetts\": \"ISO-NE\",\n",
    "        \"New Hampshire\": \"ISO-NE\",\n",
    "        \"Rhode Island\": \"ISO-NE\",\n",
    "        \"Vermont\": \"ISO-NE\",\n",
    "        \n",
    "        # WEIM (Western Energy Imbalance Market)\n",
    "        \"Arizona\": \"WEIM\",\n",
    "        \"California\": \"WEIM\",  # Note: Overlap with CAISO\n",
    "        \"Idaho\": \"WEIM\",\n",
    "        \"Nevada\": \"WEIM\",\n",
    "        \"Oregon\": \"WEIM\",\n",
    "        \"Utah\": \"WEIM\",\n",
    "        \"Washington\": \"WEIM\",\n",
    "        \"Wyoming\": \"WEIM\",\n",
    "        \"Colorado\": \"WEIM\",\n",
    "        \"Montana\": \"WEIM\"\n",
    "    }\n",
    "    \n",
    "    # Function to handle overlapping RTOs (taking primary RTO for simplicity)\n",
    "    def get_primary_rto(state):\n",
    "        # For states with overlapping RTOs, prioritize as follows:\n",
    "        # CAISO > ERCOT > PJM > MISO > SPP > NYISO > ISO-NE > WEIM\n",
    "        if state == \"California\":\n",
    "            return \"CAISO\"  # Prioritize CAISO over WEIM for California\n",
    "        elif state == \"Texas\":\n",
    "            return \"ERCOT\"  # Most of Texas is ERCOT\n",
    "        # For states in both PJM and MISO, prioritize based on which covers more of the state\n",
    "        elif state in [\"Illinois\", \"Michigan\"]:\n",
    "            return \"MISO\"   # More of these states are in MISO\n",
    "        elif state in [\"Indiana\", \"Kentucky\"]:\n",
    "            return \"PJM\"    # More of these states are in PJM\n",
    "        elif state == \"Arkansas\":\n",
    "            return \"SPP\"    # More of Arkansas is in SPP\n",
    "        elif state == \"Louisiana\":\n",
    "            return \"MISO\"   # More of Louisiana is in MISO\n",
    "        elif state == \"Missouri\":\n",
    "            return \"MISO\"   # More of Missouri is in MISO\n",
    "        else:\n",
    "            return state_to_rto_mapping.get(state, \"Other\")\n",
    "    \n",
    "    # Add RTO/ISO column\n",
    "    print(\"Adding RTO/ISO information...\")\n",
    "    df['RTO_ISO'] = df['stateDescription'].apply(get_primary_rto)\n",
    "    \n",
    "    # Handle regional entries\n",
    "    # For regional entries, set RTO/ISO based on the region name or mark as \"Multiple\"\n",
    "    region_to_rto = {\n",
    "        \"New England\": \"ISO-NE\",\n",
    "        \"Middle Atlantic\": \"Multiple\",  # Covers PJM and NYISO areas\n",
    "        \"East North Central\": \"Multiple\",  # Covers PJM and MISO areas\n",
    "        \"West North Central\": \"Multiple\",  # Covers MISO and SPP areas\n",
    "        \"South Atlantic\": \"Multiple\",  # Covers PJM areas\n",
    "        \"East South Central\": \"Multiple\",  # Covers PJM and MISO areas\n",
    "        \"West South Central\": \"Multiple\",  # Covers ERCOT, SPP, and MISO areas\n",
    "        \"Mountain\": \"Multiple\",  # Covers WEIM areas\n",
    "        \"Pacific Contiguous\": \"Multiple\",  # Covers CAISO and WEIM areas\n",
    "        \"Pacific Noncontiguous\": \"Other\",  # Alaska and Hawaii are not part of the major RTOs/ISOs\n",
    "        \"U.S. Total\": \"Multiple\"\n",
    "    }\n",
    "    \n",
    "    # Update regional entries\n",
    "    for region, rto in region_to_rto.items():\n",
    "        df.loc[df['stateDescription'] == region, 'RTO_ISO'] = rto\n",
    "    \n",
    "    # Count entries by RTO/ISO\n",
    "    rto_counts = df['RTO_ISO'].value_counts()\n",
    "    print(\"\\nNumber of entries by RTO/ISO:\")\n",
    "    print(rto_counts)\n",
    "    \n",
    "    # Save modified data\n",
    "    print(f\"\\nSaving modified data to {output_file}...\")\n",
    "    df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return df\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"monthly_all_states_2020_2024.xlsx\"\n",
    "    output_file = \"monthly_all_states_with_rto_2020_2024.xlsx\"\n",
    "    \n",
    "    processed_data = add_rto_to_electricity_data(input_file, output_file)\n",
    "    \n",
    "    # Display sample of the modified data\n",
    "    print(\"\\nSample of modified data:\")\n",
    "    print(processed_data[['year', 'month', 'stateDescription', 'RTO_ISO', 'price']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Adding RTO/ISO mapping to the data...\n",
      "Reading electricity data from monthly_all_states_2020_2024.xlsx...\n",
      "Loaded 3038 rows of data\n",
      "Adding RTO/ISO information...\n",
      "\n",
      "Number of entries by RTO/ISO:\n",
      "RTO_ISO\n",
      "PJM         588\n",
      "MISO        490\n",
      "WEIM        441\n",
      "Multiple    441\n",
      "Other       343\n",
      "ISO-NE      343\n",
      "SPP         245\n",
      "CAISO        49\n",
      "NYISO        49\n",
      "ERCOT        49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saving modified data to rto_analysis/data/monthly_all_states_with_rto.xlsx...\n",
      "\n",
      "Step 2: Generating visualizations...\n",
      "Reading data from rto_analysis/data/monthly_all_states_with_rto.xlsx...\n",
      "Creating average price by RTO/ISO visualization...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 877\u001b[0m\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDashboard data prepared and saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;66;03m# Run the entire analysis pipeline\u001b[39;00m\n\u001b[0;32m--> 877\u001b[0m     \u001b[43mrun_rto_analysis_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonthly_all_states_2020_2024.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrto_analysis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 47\u001b[0m, in \u001b[0;36mrun_rto_analysis_pipeline\u001b[0;34m(input_file, output_dir)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Step 2: Generate visualizations\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep 2: Generating visualizations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m visualizations \u001b[38;5;241m=\u001b[39m \u001b[43mvisualize_rto_analysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonthly_all_states_with_rto.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfigures_dir\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Step 3: Generate analysis report\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep 3: Generating analysis report...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 277\u001b[0m, in \u001b[0;36mvisualize_rto_analysis\u001b[0;34m(input_file, output_dir)\u001b[0m\n\u001b[1;32m    275\u001b[0m fig1\u001b[38;5;241m.\u001b[39mupdate_layout(xaxis_tickangle\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m45\u001b[39m)\n\u001b[1;32m    276\u001b[0m fig1\u001b[38;5;241m.\u001b[39mwrite_html(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_price_by_rto.html\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 277\u001b[0m \u001b[43mfig1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_price_by_rto.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Get monthly average prices by RTO/ISO for subsequent visualizations\u001b[39;00m\n\u001b[1;32m    280\u001b[0m monthly_rto_prices \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRTO_ISO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/plotly/basedatatypes.py:3827\u001b[0m, in \u001b[0;36mBaseFigure.write_image\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3767\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3768\u001b[0m \u001b[38;5;124;03mConvert a figure to a static image and write it to a file or writeable\u001b[39;00m\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;124;03mobject\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3823\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3825\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/plotly/io/_kaleido.py:266\u001b[0m, in \u001b[0;36mwrite_image\u001b[0;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03mCannot infer image type from output path '{file}'.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 )\n\u001b[1;32m    261\u001b[0m             )\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Request image\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# -------------\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# Do this first so we don't create a file if image conversion fails\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     img_data \u001b[38;5;241m=\u001b[39m \u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Open file\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# ---------\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;66;03m# We previously failed to make sense of `file` as a pathlib object.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;66;03m# Attempt to write to `file` as an open file descriptor.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/plotly/io/_kaleido.py:132\u001b[0m, in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Raise informative error message if Kaleido is not installed\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03mImage export using the \"kaleido\" engine requires the kaleido package,\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mwhich can be installed using pip:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    $ pip install -U kaleido\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Validate figure\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# ---------------\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     fig_dict \u001b[38;5;241m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n",
      "\u001b[0;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "import kaleido\n",
    "\n",
    "def run_rto_analysis_pipeline(input_file='monthly_all_states_2020_2024.xlsx', \n",
    "                             output_dir='rto_analysis'):\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline to process electricity data, add RTO/ISO mapping,\n",
    "    generate visualizations, and create an analysis report.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to the input Excel file containing electricity data\n",
    "    output_dir : str\n",
    "        Directory where all output files will be saved\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    # Create directories for different output types\n",
    "    data_dir = os.path.join(output_dir, 'data')\n",
    "    figures_dir = os.path.join(output_dir, 'figures')\n",
    "    report_dir = os.path.join(output_dir, 'reports')\n",
    "    \n",
    "    for directory in [data_dir, figures_dir, report_dir]:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    \n",
    "    # Step 1: Add RTO/ISO mapping to the data\n",
    "    print(\"Step 1: Adding RTO/ISO mapping to the data...\")\n",
    "    modified_data = add_rto_to_electricity_data(\n",
    "        input_file, \n",
    "        os.path.join(data_dir, 'monthly_all_states_with_rto.xlsx')\n",
    "    )\n",
    "    \n",
    "    # Step 2: Generate visualizations\n",
    "    print(\"\\nStep 2: Generating visualizations...\")\n",
    "    visualizations = visualize_rto_analysis(\n",
    "        os.path.join(data_dir, 'monthly_all_states_with_rto.xlsx'),\n",
    "        figures_dir\n",
    "    )\n",
    "    \n",
    "    # Step 3: Generate analysis report\n",
    "    print(\"\\nStep 3: Generating analysis report...\")\n",
    "    report = generate_rto_analysis_report(\n",
    "        os.path.join(data_dir, 'monthly_all_states_with_rto.xlsx'),\n",
    "        os.path.join(report_dir, 'rto_analysis_report.md')\n",
    "    )\n",
    "    \n",
    "    # Step 4: Generate dashboard data\n",
    "    print(\"\\nStep 4: Preparing data for dashboard...\")\n",
    "    prepare_dashboard_data(\n",
    "        modified_data,\n",
    "        os.path.join(data_dir, 'dashboard_data')\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAnalysis complete! All outputs saved to {output_dir}\")\n",
    "    return {\n",
    "        'modified_data': modified_data,\n",
    "        'visualizations': visualizations,\n",
    "        'report': report\n",
    "    }\n",
    "\n",
    "def add_rto_to_electricity_data(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Add RTO/ISO information to electricity data based on state location.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to the input Excel file containing electricity data\n",
    "    output_file : str\n",
    "        Path where the modified Excel file will be saved\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The modified DataFrame with RTO/ISO information\n",
    "    \"\"\"\n",
    "    print(f\"Reading electricity data from {input_file}...\")\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # Print dataset info\n",
    "    print(f\"Loaded {len(df)} rows of data\")\n",
    "    \n",
    "    # Create a mapping of states to RTOs/ISOs\n",
    "    # This mapping is based on the RTO/ISO information provided\n",
    "    state_to_rto_mapping = {\n",
    "        # CAISO (California ISO)\n",
    "        \"California\": \"CAISO\",\n",
    "        \n",
    "        # ERCOT (Electric Reliability Council of Texas)\n",
    "        \"Texas\": \"ERCOT\",\n",
    "        \n",
    "        # PJM Interconnection\n",
    "        \"Delaware\": \"PJM\",\n",
    "        \"Illinois\": \"PJM\",  # Note: Parts of Illinois are in MISO too\n",
    "        \"Indiana\": \"PJM\",   # Note: Parts of Indiana are in MISO too\n",
    "        \"Kentucky\": \"PJM\",  # Note: Parts of Kentucky are in MISO too\n",
    "        \"Maryland\": \"PJM\",\n",
    "        \"Michigan\": \"PJM\",  # Note: Parts of Michigan are in MISO too\n",
    "        \"New Jersey\": \"PJM\",\n",
    "        \"North Carolina\": \"PJM\",  # Note: Only parts of NC are in PJM\n",
    "        \"Ohio\": \"PJM\",\n",
    "        \"Pennsylvania\": \"PJM\",\n",
    "        \"Tennessee\": \"PJM\",  # Note: Only parts of TN are in PJM\n",
    "        \"Virginia\": \"PJM\",\n",
    "        \"West Virginia\": \"PJM\",\n",
    "        \"District of Columbia\": \"PJM\",\n",
    "        \n",
    "        # MISO (Midcontinent Independent System Operator)\n",
    "        \"Minnesota\": \"MISO\",\n",
    "        \"Iowa\": \"MISO\",\n",
    "        \"Missouri\": \"MISO\",\n",
    "        \"Wisconsin\": \"MISO\",\n",
    "        \"Michigan\": \"MISO\",  # Note: Overlap with PJM\n",
    "        \"Illinois\": \"MISO\",  # Note: Overlap with PJM\n",
    "        \"Indiana\": \"MISO\",   # Note: Overlap with PJM\n",
    "        \"Kentucky\": \"MISO\",  # Note: Overlap with PJM\n",
    "        \"Arkansas\": \"MISO\",  # Note: Parts of Arkansas are in SPP\n",
    "        \"Mississippi\": \"MISO\",\n",
    "        \"Louisiana\": \"MISO\",\n",
    "        \"North Dakota\": \"MISO\",\n",
    "        \"South Dakota\": \"MISO\",\n",
    "        \n",
    "        # SPP (Southwest Power Pool)\n",
    "        \"Oklahoma\": \"SPP\",\n",
    "        \"Kansas\": \"SPP\",\n",
    "        \"Nebraska\": \"SPP\",\n",
    "        \"Arkansas\": \"SPP\",    # Note: Overlap with MISO\n",
    "        \"Louisiana\": \"SPP\",   # Note: Overlap with MISO\n",
    "        \"Missouri\": \"SPP\",    # Note: Overlap with MISO\n",
    "        \"New Mexico\": \"SPP\",\n",
    "        \n",
    "        # NYISO (New York ISO)\n",
    "        \"New York\": \"NYISO\",\n",
    "        \n",
    "        # ISO-NE (ISO New England)\n",
    "        \"Connecticut\": \"ISO-NE\",\n",
    "        \"Maine\": \"ISO-NE\",\n",
    "        \"Massachusetts\": \"ISO-NE\",\n",
    "        \"New Hampshire\": \"ISO-NE\",\n",
    "        \"Rhode Island\": \"ISO-NE\",\n",
    "        \"Vermont\": \"ISO-NE\",\n",
    "        \n",
    "        # WEIM (Western Energy Imbalance Market)\n",
    "        \"Arizona\": \"WEIM\",\n",
    "        \"California\": \"WEIM\",  # Note: Overlap with CAISO\n",
    "        \"Idaho\": \"WEIM\",\n",
    "        \"Nevada\": \"WEIM\",\n",
    "        \"Oregon\": \"WEIM\",\n",
    "        \"Utah\": \"WEIM\",\n",
    "        \"Washington\": \"WEIM\",\n",
    "        \"Wyoming\": \"WEIM\",\n",
    "        \"Colorado\": \"WEIM\",\n",
    "        \"Montana\": \"WEIM\"\n",
    "    }\n",
    "    \n",
    "    # Function to handle overlapping RTOs (taking primary RTO for simplicity)\n",
    "    def get_primary_rto(state):\n",
    "        # For states with overlapping RTOs, prioritize as follows:\n",
    "        # CAISO > ERCOT > PJM > MISO > SPP > NYISO > ISO-NE > WEIM\n",
    "        if state == \"California\":\n",
    "            return \"CAISO\"  # Prioritize CAISO over WEIM for California\n",
    "        elif state == \"Texas\":\n",
    "            return \"ERCOT\"  # Most of Texas is ERCOT\n",
    "        # For states in both PJM and MISO, prioritize based on which covers more of the state\n",
    "        elif state in [\"Illinois\", \"Michigan\"]:\n",
    "            return \"MISO\"   # More of these states are in MISO\n",
    "        elif state in [\"Indiana\", \"Kentucky\"]:\n",
    "            return \"PJM\"    # More of these states are in PJM\n",
    "        elif state == \"Arkansas\":\n",
    "            return \"SPP\"    # More of Arkansas is in SPP\n",
    "        elif state == \"Louisiana\":\n",
    "            return \"MISO\"   # More of Louisiana is in MISO\n",
    "        elif state == \"Missouri\":\n",
    "            return \"MISO\"   # More of Missouri is in MISO\n",
    "        else:\n",
    "            return state_to_rto_mapping.get(state, \"Other\")\n",
    "    \n",
    "    # Add RTO/ISO column\n",
    "    print(\"Adding RTO/ISO information...\")\n",
    "    df['RTO_ISO'] = df['stateDescription'].apply(get_primary_rto)\n",
    "    \n",
    "    # Handle regional entries\n",
    "    # For regional entries, set RTO/ISO based on the region name or mark as \"Multiple\"\n",
    "    region_to_rto = {\n",
    "        \"New England\": \"ISO-NE\",\n",
    "        \"Middle Atlantic\": \"Multiple\",  # Covers PJM and NYISO areas\n",
    "        \"East North Central\": \"Multiple\",  # Covers PJM and MISO areas\n",
    "        \"West North Central\": \"Multiple\",  # Covers MISO and SPP areas\n",
    "        \"South Atlantic\": \"Multiple\",  # Covers PJM areas\n",
    "        \"East South Central\": \"Multiple\",  # Covers PJM and MISO areas\n",
    "        \"West South Central\": \"Multiple\",  # Covers ERCOT, SPP, and MISO areas\n",
    "        \"Mountain\": \"Multiple\",  # Covers WEIM areas\n",
    "        \"Pacific Contiguous\": \"Multiple\",  # Covers CAISO and WEIM areas\n",
    "        \"Pacific Noncontiguous\": \"Other\",  # Alaska and Hawaii are not part of the major RTOs/ISOs\n",
    "        \"U.S. Total\": \"Multiple\"\n",
    "    }\n",
    "    \n",
    "    # Update regional entries\n",
    "    for region, rto in region_to_rto.items():\n",
    "        df.loc[df['stateDescription'] == region, 'RTO_ISO'] = rto\n",
    "    \n",
    "    # Count entries by RTO/ISO\n",
    "    rto_counts = df['RTO_ISO'].value_counts()\n",
    "    print(\"\\nNumber of entries by RTO/ISO:\")\n",
    "    print(rto_counts)\n",
    "    \n",
    "    # Save modified data\n",
    "    print(f\"\\nSaving modified data to {output_file}...\")\n",
    "    df.to_excel(output_file, index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def visualize_rto_analysis(input_file, output_dir='figures'):\n",
    "    \"\"\"\n",
    "    Create visualizations to analyze electricity prices by RTO/ISO.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to the Excel file containing electricity data with RTO/ISO information\n",
    "    output_dir : str\n",
    "        Directory where visualization files will be saved\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the visualization figures\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {input_file}...\")\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Filter out rows not relevant to this analysis (e.g., non-\"all sectors\" data)\n",
    "    if 'sectorName' in df.columns:\n",
    "        df = df[df['sectorName'] == 'all sectors'].copy()\n",
    "    \n",
    "    # Add date column for time series analysis\n",
    "    df['date'] = pd.to_datetime(df['year'].astype(str) + '-' + df['month'].astype(str) + '-01')\n",
    "    \n",
    "    # 1. Average price by RTO/ISO\n",
    "    print(\"Creating average price by RTO/ISO visualization...\")\n",
    "    rto_avg_price = df.groupby('RTO_ISO')['price'].mean().reset_index()\n",
    "    rto_avg_price = rto_avg_price.sort_values('price', ascending=False)\n",
    "    \n",
    "    fig1 = px.bar(\n",
    "        rto_avg_price,\n",
    "        x='RTO_ISO',\n",
    "        y='price',\n",
    "        title='Average Electricity Price by RTO/ISO (2020-2024)',\n",
    "        labels={'price': 'Average Price ($/kWh)', 'RTO_ISO': 'RTO/ISO'},\n",
    "        color='price',\n",
    "        color_continuous_scale='Blues',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig1.update_layout(xaxis_tickangle=-45)\n",
    "    fig1.write_html(os.path.join(output_dir, 'avg_price_by_rto.html'))\n",
    "    fig1.write_image(os.path.join(output_dir, 'avg_price_by_rto.png'))\n",
    "    \n",
    "    # Get monthly average prices by RTO/ISO for subsequent visualizations\n",
    "    monthly_rto_prices = df.groupby(['RTO_ISO', 'date'])['price'].mean().reset_index()\n",
    "    \n",
    "    # Filter to major RTOs only (exclude \"Multiple\" and \"Other\")\n",
    "    major_rtos = ['CAISO', 'ERCOT', 'ISO-NE', 'MISO', 'NYISO', 'PJM', 'SPP', 'WEIM']\n",
    "    monthly_major_rtos = monthly_rto_prices[monthly_rto_prices['RTO_ISO'].isin(major_rtos)]\n",
    "    \n",
    "    # 2. Price trends over time by RTO/ISO\n",
    "    print(\"Creating price trends by RTO/ISO visualization...\")\n",
    "    fig2 = px.line(\n",
    "        monthly_major_rtos,\n",
    "        x='date',\n",
    "        y='price',\n",
    "        color='RTO_ISO',\n",
    "        title='Monthly Electricity Price Trends by RTO/ISO (2020-2024)',\n",
    "        labels={'price': 'Price ($/kWh)', 'date': 'Date', 'RTO_ISO': 'RTO/ISO'},\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig2.write_html(os.path.join(output_dir, 'price_trends_by_rto.html'))\n",
    "    fig2.write_image(os.path.join(output_dir, 'price_trends_by_rto.png'))\n",
    "    \n",
    "    # 3. Price volatility by RTO/ISO\n",
    "    print(\"Creating price volatility by RTO/ISO visualization...\")\n",
    "    rto_price_std = df.groupby('RTO_ISO')['price'].std().reset_index()\n",
    "    rto_price_std = rto_price_std.sort_values('price', ascending=False)\n",
    "    \n",
    "    fig3 = px.bar(\n",
    "        rto_price_std,\n",
    "        x='RTO_ISO',\n",
    "        y='price',\n",
    "        title='Electricity Price Volatility by RTO/ISO (2020-2024)',\n",
    "        labels={'price': 'Price Standard Deviation ($/kWh)', 'RTO_ISO': 'RTO/ISO'},\n",
    "        color='price',\n",
    "        color_continuous_scale='Reds',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig3.update_layout(xaxis_tickangle=-45)\n",
    "    fig3.write_html(os.path.join(output_dir, 'price_volatility_by_rto.html'))\n",
    "    fig3.write_image(os.path.join(output_dir, 'price_volatility_by_rto.png'))\n",
    "    \n",
    "    # 4. Seasonal patterns by RTO/ISO\n",
    "    print(\"Creating seasonal patterns by RTO/ISO visualization...\")\n",
    "    seasonal_patterns = df.groupby(['RTO_ISO', 'month'])['price'].mean().reset_index()\n",
    "    \n",
    "    fig4 = make_subplots(\n",
    "        rows=2, \n",
    "        cols=4,\n",
    "        subplot_titles=major_rtos,\n",
    "        shared_yaxes=True\n",
    "    )\n",
    "    \n",
    "    for i, rto in enumerate(major_rtos):\n",
    "        row = i // 4 + 1\n",
    "        col = i % 4 + 1\n",
    "        \n",
    "        rto_data = seasonal_patterns[seasonal_patterns['RTO_ISO'] == rto]\n",
    "        \n",
    "        fig4.add_trace(\n",
    "            go.Scatter(\n",
    "                x=rto_data['month'],\n",
    "                y=rto_data['price'],\n",
    "                mode='lines+markers',\n",
    "                name=rto\n",
    "            ),\n",
    "            row=row, \n",
    "            col=col\n",
    "        )\n",
    "        \n",
    "        # Update x-axis for each subplot\n",
    "        fig4.update_xaxes(\n",
    "            title_text=\"Month\",\n",
    "            tickmode='array',\n",
    "            tickvals=list(range(1, 13)),\n",
    "            ticktext=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "            row=row,\n",
    "            col=col\n",
    "        )\n",
    "        \n",
    "        if col == 1:\n",
    "            fig4.update_yaxes(title_text=\"Price ($/kWh)\", row=row, col=col)\n",
    "    \n",
    "    fig4.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Seasonal Price Patterns by RTO/ISO (2020-2024)\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig4.write_html(os.path.join(output_dir, 'seasonal_patterns_by_rto.html'))\n",
    "    fig4.write_image(os.path.join(output_dir, 'seasonal_patterns_by_rto.png'))\n",
    "    \n",
    "    # 5. Box plots to show price distributions by RTO/ISO\n",
    "    print(\"Creating price distribution by RTO/ISO visualization...\")\n",
    "    \n",
    "    fig5 = px.box(\n",
    "        df[df['RTO_ISO'].isin(major_rtos)],\n",
    "        x='RTO_ISO',\n",
    "        y='price',\n",
    "        color='RTO_ISO',\n",
    "        title='Electricity Price Distribution by RTO/ISO (2020-2024)',\n",
    "        labels={'price': 'Price ($/kWh)', 'RTO_ISO': 'RTO/ISO'},\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig5.update_layout(showlegend=False)\n",
    "    fig5.write_html(os.path.join(output_dir, 'price_distribution_by_rto.html'))\n",
    "    fig5.write_image(os.path.join(output_dir, 'price_distribution_by_rto.png'))\n",
    "    \n",
    "    # 6. Year-over-year price changes by RTO/ISO\n",
    "    print(\"Creating year-over-year price changes by RTO/ISO visualization...\")\n",
    "    yearly_rto_prices = df.groupby(['RTO_ISO', 'year'])['price'].mean().reset_index()\n",
    "    \n",
    "    # Calculate year-over-year percentage changes\n",
    "    yearly_changes = []\n",
    "    \n",
    "    for rto in major_rtos:\n",
    "        rto_data = yearly_rto_prices[yearly_rto_prices['RTO_ISO'] == rto].sort_values('year')\n",
    "        \n",
    "        for i in range(1, len(rto_data)):\n",
    "            prev_year = rto_data.iloc[i-1]['year']\n",
    "            curr_year = rto_data.iloc[i]['year']\n",
    "            prev_price = rto_data.iloc[i-1]['price']\n",
    "            curr_price = rto_data.iloc[i]['price']\n",
    "            \n",
    "            pct_change = (curr_price - prev_price) / prev_price * 100\n",
    "            \n",
    "            yearly_changes.append({\n",
    "                'RTO_ISO': rto,\n",
    "                'year': curr_year,\n",
    "                'from_year': prev_year,\n",
    "                'to_year': curr_year,\n",
    "                'pct_change': pct_change\n",
    "            })\n",
    "    \n",
    "    yearly_changes_df = pd.DataFrame(yearly_changes)\n",
    "    \n",
    "    fig6 = px.bar(\n",
    "        yearly_changes_df,\n",
    "        x='RTO_ISO',\n",
    "        y='pct_change',\n",
    "        color='year',\n",
    "        barmode='group',\n",
    "        title='Year-over-Year Electricity Price Changes by RTO/ISO',\n",
    "        labels={'pct_change': 'Percentage Change (%)', 'RTO_ISO': 'RTO/ISO', 'year': 'Year'},\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig6.update_layout(xaxis_tickangle=-45)\n",
    "    fig6.write_html(os.path.join(output_dir, 'yearly_changes_by_rto.html'))\n",
    "    fig6.write_image(os.path.join(output_dir, 'yearly_changes_by_rto.png'))\n",
    "    \n",
    "    # 7. Heatmap of prices by RTO/ISO and year\n",
    "    print(\"Creating price heatmap by RTO/ISO and year...\")\n",
    "    pivot_data = yearly_rto_prices[yearly_rto_prices['RTO_ISO'].isin(major_rtos)]\n",
    "    pivot = pivot_data.pivot(index='RTO_ISO', columns='year', values='price')\n",
    "    \n",
    "    fig7 = px.imshow(\n",
    "        pivot,\n",
    "        labels=dict(x=\"Year\", y=\"RTO/ISO\", color=\"Price ($/kWh)\"),\n",
    "        x=pivot.columns,\n",
    "        y=pivot.index,\n",
    "        color_continuous_scale='Blues',\n",
    "        aspect=\"auto\",\n",
    "        title=\"Electricity Price Heatmap by RTO/ISO and Year\"\n",
    "    )\n",
    "    \n",
    "    fig7.update_layout(height=600)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for y in range(len(pivot.index)):\n",
    "        for x in range(len(pivot.columns)):\n",
    "            value = pivot.iloc[y, x]\n",
    "            fig7.add_annotation(\n",
    "                x=pivot.columns[x],\n",
    "                y=pivot.index[y],\n",
    "                text=f\"${value:.2f}\",\n",
    "                showarrow=False,\n",
    "                font=dict(color=\"white\" if value > 15 else \"black\")\n",
    "            )\n",
    "    \n",
    "    fig7.write_html(os.path.join(output_dir, 'price_heatmap_by_rto_year.html'))\n",
    "    fig7.write_image(os.path.join(output_dir, 'price_heatmap_by_rto_year.png'))\n",
    "    \n",
    "    print(\"Visualizations created and saved to the figures directory.\")\n",
    "    return {\n",
    "        'avg_price_by_rto': fig1,\n",
    "        'price_trends_by_rto': fig2,\n",
    "        'price_volatility_by_rto': fig3,\n",
    "        'seasonal_patterns_by_rto': fig4,\n",
    "        'price_distribution_by_rto': fig5,\n",
    "        'yearly_changes_by_rto': fig6,\n",
    "        'price_heatmap_by_rto_year': fig7\n",
    "    }\n",
    "\n",
    "def generate_rto_analysis_report(input_file, output_file=\"rto_analysis_report.md\"):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive analysis report comparing electricity prices across different RTOs/ISOs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to the Excel file containing electricity data with RTO/ISO information\n",
    "    output_file : str\n",
    "        Path where the report will be saved (markdown format)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        The report content as a list of strings\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from {input_file}...\")\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    # Filter out rows not relevant to this analysis\n",
    "    if 'sectorName' in df.columns:\n",
    "        df = df[df['sectorName'] == 'all sectors'].copy()\n",
    "    \n",
    "    # Add date column\n",
    "    df['date'] = pd.to_datetime(df['year'].astype(str) + '-' + df['month'].astype(str) + '-01')\n",
    "    \n",
    "    # Define major RTOs for focused analysis\n",
    "    major_rtos = ['CAISO', 'ERCOT', 'ISO-NE', 'MISO', 'NYISO', 'PJM', 'SPP', 'WEIM']\n",
    "    \n",
    "    # Filter data for major RTOs only\n",
    "    major_rto_data = df[df['RTO_ISO'].isin(major_rtos)].copy()\n",
    "    \n",
    "    # Calculate key statistics by RTO/ISO\n",
    "    rto_stats = major_rto_data.groupby('RTO_ISO').agg({\n",
    "        'price': ['mean', 'std', 'min', 'max'],\n",
    "        'stateDescription': lambda x: ', '.join(sorted(set(x))),\n",
    "        'revenue': 'sum',\n",
    "        'sales': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    rto_stats.columns = ['RTO_ISO', 'avg_price', 'price_std', 'min_price', 'max_price', \n",
    "                         'states', 'total_revenue', 'total_sales']\n",
    "    \n",
    "    # Calculate average price by year and RTO/ISO\n",
    "    yearly_rto_prices = major_rto_data.groupby(['RTO_ISO', 'year'])['price'].mean().reset_index()\n",
    "    \n",
    "    # Calculate year-over-year price changes\n",
    "    yearly_changes = []\n",
    "    \n",
    "    for rto in major_rtos:\n",
    "        rto_data = yearly_rto_prices[yearly_rto_prices['RTO_ISO'] == rto].sort_values('year')\n",
    "        \n",
    "        for i in range(1, len(rto_data)):\n",
    "            prev_year = rto_data.iloc[i-1]['year']\n",
    "            curr_year = rto_data.iloc[i]['year']\n",
    "            prev_price = rto_data.iloc[i-1]['price']\n",
    "            curr_price = rto_data.iloc[i]['price']\n",
    "            \n",
    "            pct_change = (curr_price - prev_price) / prev_price * 100\n",
    "            \n",
    "            yearly_changes.append({\n",
    "                'RTO_ISO': rto,\n",
    "                'year': curr_year,\n",
    "                'from_year': prev_year,\n",
    "                'to_year': curr_year,\n",
    "                'pct_change': pct_change\n",
    "            })\n",
    "    \n",
    "    yearly_changes_df = pd.DataFrame(yearly_changes)\n",
    "    \n",
    "    # Calculate seasonal patterns\n",
    "    monthly_patterns = major_rto_data.groupby(['RTO_ISO', 'month'])['price'].mean().reset_index()\n",
    "    \n",
    "    # Calculate average monthly fluctuation (max month avg - min month avg)\n",
    "    seasonal_fluctuation = []\n",
    "    for rto in major_rtos:\n",
    "        rto_monthly = monthly_patterns[monthly_patterns['RTO_ISO'] == rto]\n",
    "        max_month_price = rto_monthly['price'].max()\n",
    "        min_month_price = rto_monthly['price'].min()\n",
    "        fluctuation = max_month_price - min_month_price\n",
    "        seasonal_fluctuation.append({\n",
    "            'RTO_ISO': rto,\n",
    "            'max_month_price': max_month_price,\n",
    "            'min_month_price': min_month_price,\n",
    "            'seasonal_fluctuation': fluctuation,\n",
    "            'fluctuation_pct': (fluctuation / min_month_price) * 100\n",
    "        })\n",
    "    \n",
    "    seasonal_fluctuation_df = pd.DataFrame(seasonal_fluctuation)\n",
    "    \n",
    "    # Generate the report\n",
    "    print(\"Generating RTO/ISO analysis report...\")\n",
    "    \n",
    "    # Prepare the report content\n",
    "    report = []\n",
    "    \n",
    "    # Title\n",
    "    report.append(\"# Electricity Price Analysis by RTO/ISO (2020-2024)\\n\")\n",
    "    \n",
    "    # Introduction\n",
    "    report.append(\"## Introduction\\n\")\n",
    "    report.append(\"This report analyzes electricity price data across different Regional Transmission Organizations (RTOs) and Independent System Operators (ISOs) in the United States from 2020 to 2024. The analysis provides insights into price variations, trends, volatility, and seasonal patterns across these different electricity markets.\\n\")\n",
    "    \n",
    "    # Overview of RTOs/ISOs\n",
    "    report.append(\"## Overview of RTOs/ISOs\\n\")\n",
    "    report.append(\"Regional Transmission Organizations (RTOs) and Independent System Operators (ISOs) are entities that coordinate, control, and monitor multi-state electric grids in the United States. They're responsible for ensuring reliability, managing wholesale electricity markets, and coordinating planning across their regions.\\n\")\n",
    "    \n",
    "    # List of analyzed RTOs/ISOs\n",
    "    report.append(\"Major RTOs/ISOs included in this analysis:\\n\\n\")\n",
    "    \n",
    "    # Prepare RTO descriptions\n",
    "    rto_descriptions = {\n",
    "        'CAISO': \"California Independent System Operator - operates the grid for most of California\",\n",
    "        'ERCOT': \"Electric Reliability Council of Texas - manages the grid for most of Texas (separate from other U.S. grids)\",\n",
    "        'ISO-NE': \"ISO New England - serves six New England states: Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont\",\n",
    "        'MISO': \"Midcontinent Independent System Operator - covers parts of the Midwest, South, and parts of Canada\",\n",
    "        'NYISO': \"New York Independent System Operator - serves New York State\",\n",
    "        'PJM': \"PJM Interconnection - serves parts of 13 states in the Mid-Atlantic and Midwest regions, and the District of Columbia\",\n",
    "        'SPP': \"Southwest Power Pool - covers parts of the central U.S. including Arkansas, Oklahoma, Kansas, and parts of surrounding states\",\n",
    "        'WEIM': \"Western Energy Imbalance Market - a real-time market covering parts of the Western U.S., led by CAISO\"\n",
    "    }\n",
    "    \n",
    "    for rto in major_rtos:\n",
    "        report.append(f\"- **{rto}**: {rto_descriptions.get(rto, '')}\\n\")\n",
    "    \n",
    "    # Key Price Statistics\n",
    "    report.append(\"\\n## Key Price Statistics by RTO/ISO\\n\")\n",
    "    report.append(\"The table below summarizes key electricity price statistics for each RTO/ISO during the analysis period (2020-2024):\\n\\n\")\n",
    "    \n",
    "    # Prepare the statistics table\n",
    "    stats_table = rto_stats.copy()\n",
    "    stats_table = stats_table[['RTO_ISO', 'avg_price', 'price_std', 'min_price', 'max_price']]\n",
    "    stats_table.columns = ['RTO/ISO', 'Average Price ($/kWh)', 'Price Std Dev', 'Minimum Price', 'Maximum Price']\n",
    "    stats_table = stats_table.sort_values('Average Price ($/kWh)', ascending=False)\n",
    "    \n",
    "    # Format the values\n",
    "    for col in stats_table.columns[1:]:\n",
    "        stats_table[col] = stats_table[col].apply(lambda x: f\"${x:.2f}\")\n",
    "    \n",
    "    # Add the table to the report\n",
    "    report.append(tabulate(stats_table, headers='keys', tablefmt='pipe', showindex=False))\n",
    "    \n",
    "    # Average Price Comparison\n",
    "    report.append(\"\\n\\n## Average Price Comparison\\n\")\n",
    "    report.append(\"There are significant variations in average electricity prices across different RTOs/ISOs:\\n\\n\")\n",
    "    \n",
    "    # Get highest and lowest price RTOs\n",
    "    highest_price_rto = rto_stats.loc[rto_stats['avg_price'].idxmax()]\n",
    "    lowest_price_rto = rto_stats.loc[rto_stats['avg_price'].idxmin()]\n",
    "    \n",
    "    # Calculate the difference\n",
    "    price_difference = highest_price_rto['avg_price'] - lowest_price_rto['avg_price']\n",
    "    price_ratio = highest_price_rto['avg_price'] / lowest_price_rto['avg_price']\n",
    "    \n",
    "    report.append(f\"- **Highest Average Price**: {highest_price_rto['RTO_ISO']} at ${highest_price_rto['avg_price']:.2f}/kWh\\n\")\n",
    "    report.append(f\"- **Lowest Average Price**: {lowest_price_rto['RTO_ISO']} at ${lowest_price_rto['avg_price']:.2f}/kWh\\n\")\n",
    "    report.append(f\"- **Price Difference**: ${price_difference:.2f}/kWh\\n\")\n",
    "    report.append(f\"- **Price Ratio**: Electricity in {highest_price_rto['RTO_ISO']} costs {price_ratio:.1f}x more than in {lowest_price_rto['RTO_ISO']}\\n\")\n",
    "    \n",
    "    # Price Trends\n",
    "    report.append(\"\\n## Price Trends (2020-2024)\\n\")\n",
    "    report.append(\"Analysis of year-over-year price changes for each RTO/ISO reveals interesting patterns:\\n\\n\")\n",
    "    \n",
    "    # Calculate average annual price change by RTO\n",
    "    avg_annual_change = yearly_changes_df.groupby('RTO_ISO')['pct_change'].mean().reset_index()\n",
    "    avg_annual_change = avg_annual_change.sort_values('pct_change', ascending=False)\n",
    "    \n",
    "    report.append(\"### Average Annual Price Change by RTO/ISO\\n\\n\")\n",
    "    \n",
    "    for _, row in avg_annual_change.iterrows():\n",
    "        direction = \"increase\" if row['pct_change'] > 0 else \"decrease\"\n",
    "        report.append(f\"- **{row['RTO_ISO']}**: {abs(row['pct_change']):.1f}% average annual {direction}\\n\")\n",
    "    \n",
    "    report.append(\"\\n### Yearly Price Changes by RTO/ISO\\n\\n\")\n",
    "    \n",
    "    # Create a summary of year-by-year changes\n",
    "    for year in sorted(yearly_changes_df['year'].unique()):\n",
    "        report.append(f\"#### {year} Price Changes\\n\")\n",
    "        year_data = yearly_changes_df[yearly_changes_df['year'] == year].sort_values('pct_change', ascending=False)\n",
    "        \n",
    "        for _, row in year_data.iterrows():\n",
    "            direction = \"increase\" if row['pct_change'] > 0 else \"decrease\"\n",
    "            report.append(f\"- **{row['RTO_ISO']}**: {abs(row['pct_change']):.1f}% {direction}\\n\")\n",
    "        \n",
    "        report.append(\"\\n\")\n",
    "    \n",
    "    # Price Volatility\n",
    "    report.append(\"## Price Volatility\\n\")\n",
    "    report.append(\"Price volatility is an important factor for consumers and market participants. The standard deviation of prices can be used as a measure of volatility.\\n\\n\")\n",
    "    \n",
    "    # Prepare volatility data\n",
    "    volatility_data = rto_stats[['RTO_ISO', 'avg_price', 'price_std']].copy()\n",
    "    volatility_data['coefficient_of_variation'] = volatility_data['price_std'] / volatility_data['avg_price'] * 100\n",
    "    volatility_data = volatility_data.sort_values('coefficient_of_variation', ascending=False)\n",
    "    \n",
    "    report.append(\"### Volatility Ranking (Coefficient of Variation)\\n\\n\")\n",
    "    \n",
    "    for _, row in volatility_data.iterrows():\n",
    "        report.append(f\"- **{row['RTO_ISO']}**: {row['coefficient_of_variation']:.1f}% (Std Dev: ${row['price_std']:.2f})\\n\")\n",
    "    \n",
    "    # Seasonal Patterns\n",
    "    report.append(\"\\n## Seasonal Price Patterns\\n\")\n",
    "    report.append(\"Electricity prices often follow seasonal patterns due to changes in demand and supply conditions throughout the year.\\n\\n\")\n",
    "    \n",
    "    # Add seasonal fluctuation data\n",
    "    seasonal_fluctuation_df = seasonal_fluctuation_df.sort_values('fluctuation_pct', ascending=False)\n",
    "    \n",
    "    report.append(\"### Seasonal Price Fluctuation\\n\\n\")\n",
    "    \n",
    "    for _, row in seasonal_fluctuation_df.iterrows():\n",
    "        report.append(f\"- **{row['RTO_ISO']}**: {row['fluctuation_pct']:.1f}% fluctuation (${row['min_month_price']:.2f} to ${row['max_month_price']:.2f})\\n\")\n",
    "    \n",
    "    # Calculate seasonal patterns - when prices peak for each RTO\n",
    "    peak_months = {}\n",
    "    for rto in major_rtos:\n",
    "        rto_monthly = monthly_patterns[monthly_patterns['RTO_ISO'] == rto]\n",
    "        peak_month = rto_monthly.loc[rto_monthly['price'].idxmax()]['month']\n",
    "        peak_months[rto] = peak_month\n",
    "    \n",
    "    # Convert month numbers to names\n",
    "    month_names = {\n",
    "        1: 'January', 2: 'February', 3: 'March', 4: 'April', \n",
    "        5: 'May', 6: 'June', 7: 'July', 8: 'August',\n",
    "        9: 'September', 10: 'October', 11: 'November', 12: 'December'\n",
    "    }\n",
    "    \n",
    "    report.append(\"\\n### Peak Price Months by RTO/ISO\\n\\n\")\n",
    "    \n",
    "    for rto, month in peak_months.items():\n",
    "        report.append(f\"- **{rto}**: {month_names[month]}\\n\")\n",
    "    \n",
    "    # Market Comparisons\n",
    "    report.append(\"\\n## Market Comparisons\\n\")\n",
    "    \n",
    "    # Calculate the correlation matrix between RTO prices\n",
    "    monthly_pivoted = major_rto_data.pivot_table(\n",
    "        index='date',\n",
    "        columns='RTO_ISO',\n",
    "        values='price',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    correlation_matrix = monthly_pivoted.corr()\n",
    "    \n",
    "    # Find the most and least correlated markets\n",
    "    most_correlated = None\n",
    "    least_correlated = None\n",
    "    max_corr = -1\n",
    "    min_corr = 2\n",
    "    \n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            rto1 = correlation_matrix.columns[i]\n",
    "            rto2 = correlation_matrix.columns[j]\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            \n",
    "            if corr > max_corr:\n",
    "                max_corr = corr\n",
    "                most_correlated = (rto1, rto2)\n",
    "            \n",
    "            if corr < min_corr:\n",
    "                min_corr = corr\n",
    "                least_correlated = (rto1, rto2)\n",
    "    \n",
    "    report.append(\"### Price Correlation Between Markets\\n\\n\")\n",
    "    \n",
    "    report.append(f\"- **Most correlated markets**: {most_correlated[0]} and {most_correlated[1]} (correlation: {max_corr:.2f})\\n\")\n",
    "    report.append(f\"- **Least correlated markets**: {least_correlated[0]} and {least_correlated[1]} (correlation: {min_corr:.2f})\\n\")\n",
    "    \n",
    "    # Final Insights\n",
    "    report.append(\"\\n## Key Insights\\n\")\n",
    "    report.append(\"Based on the analysis of electricity prices across different RTOs/ISOs from 2020 to 2024, several key insights emerge:\\n\\n\")\n",
    "    \n",
    "    # Add insights based on the analysis\n",
    "    insights = [\n",
    "        f\"**Price Disparities**: There are significant price disparities across regions, with {highest_price_rto['RTO_ISO']} having {price_ratio:.1f}x higher average prices than {lowest_price_rto['RTO_ISO']}.\",\n",
    "        f\"**Price Trends**: {avg_annual_change.iloc[0]['RTO_ISO']} has experienced the largest average annual price increases at {avg_annual_change.iloc[0]['pct_change']:.1f}%.\",\n",
    "        f\"**Volatility**: {volatility_data.iloc[0]['RTO_ISO']} shows the highest price volatility, while {volatility_data.iloc[-1]['RTO_ISO']} demonstrates the most stable prices.\",\n",
    "        f\"**Seasonal Patterns**: {seasonal_fluctuation_df.iloc[0]['RTO_ISO']} exhibits the most pronounced seasonal price fluctuations, with prices varying by {seasonal_fluctuation_df.iloc[0]['fluctuation_pct']:.1f}% throughout the year.\",\n",
    "        f\"**Market Interconnection**: {most_correlated[0]} and {most_correlated[1]} show highly correlated price movements, suggesting similar market drivers or interconnected grid operations.\"\n",
    "    ]\n",
    "    \n",
    "    for insight in insights:\n",
    "        report.append(f\"- {insight}\\n\")\n",
    "    \n",
    "    # Conclusion\n",
    "    report.append(\"\\n## Conclusion\\n\")\n",
    "    report.append(\"This analysis highlights the complex nature of electricity markets across different regions of the United States. Factors such as regional fuel mix, transmission constraints, market design, regulatory frameworks, and weather patterns all contribute to the observed differences in electricity prices across RTOs/ISOs.\\n\\n\")\n",
    "    report.append(\"Understanding these differences and patterns is crucial for market participants, policymakers, regulators, and consumers to make informed decisions about energy consumption, investment, and policy development.\\n\")\n",
    "    \n",
    "    # Write the report to a file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    print(f\"Report generated and saved to {output_file}\")\n",
    "    return report\n",
    "\n",
    "\n",
    "def prepare_dashboard_data(df, output_dir):\n",
    "    \"\"\"\n",
    "    Prepare data files for a dashboard application by extracting and\n",
    "    transforming relevant data from the electricity dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing electricity data with RTO/ISO information\n",
    "    output_dir : str\n",
    "        Directory where dashboard data files will be saved\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for dashboard...\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Filter for all sectors data\n",
    "    if 'sectorName' in df.columns:\n",
    "        all_sectors = df[df['sectorName'] == 'all sectors'].copy()\n",
    "    else:\n",
    "        all_sectors = df.copy()\n",
    "    \n",
    "    # Add date column\n",
    "    all_sectors['date'] = pd.to_datetime(all_sectors['year'].astype(str) + '-' + \n",
    "                                         all_sectors['month'].astype(str) + '-01')\n",
    "    \n",
    "    # 1. RTO/ISO average prices\n",
    "    rto_avg_prices = all_sectors.groupby('RTO_ISO')['price'].agg(\n",
    "        ['mean', 'std', 'min', 'max', 'count']\n",
    "    ).reset_index()\n",
    "    rto_avg_prices.columns = ['rto_iso', 'avg_price', 'std_price', 'min_price', 'max_price', 'count']\n",
    "    rto_avg_prices.to_csv(os.path.join(output_dir, 'rto_avg_prices.csv'), index=False)\n",
    "    \n",
    "    # 2. Monthly prices by RTO/ISO\n",
    "    monthly_rto_prices = all_sectors.groupby(['RTO_ISO', 'year', 'month']).agg({\n",
    "        'price': 'mean',\n",
    "        'revenue': 'sum',\n",
    "        'sales': 'sum'\n",
    "    }).reset_index()\n",
    "    monthly_rto_prices['date'] = pd.to_datetime(\n",
    "        monthly_rto_prices['year'].astype(str) + '-' + \n",
    "        monthly_rto_prices['month'].astype(str) + '-01'\n",
    "    )\n",
    "    monthly_rto_prices.to_csv(os.path.join(output_dir, 'monthly_rto_prices.csv'), index=False)\n",
    "    \n",
    "    # 3. State to RTO/ISO mapping\n",
    "    state_rto_mapping = all_sectors[['stateDescription', 'RTO_ISO']].drop_duplicates()\n",
    "    state_rto_mapping.columns = ['state', 'rto_iso']\n",
    "    state_rto_mapping.to_csv(os.path.join(output_dir, 'state_rto_mapping.csv'), index=False)\n",
    "    \n",
    "    # 4. Yearly summary by RTO/ISO\n",
    "    yearly_rto_summary = all_sectors.groupby(['RTO_ISO', 'year']).agg({\n",
    "        'price': 'mean',\n",
    "        'revenue': 'sum',\n",
    "        'sales': 'sum',\n",
    "        'customers': 'mean'\n",
    "    }).reset_index()\n",
    "    yearly_rto_summary.to_csv(os.path.join(output_dir, 'yearly_rto_summary.csv'), index=False)\n",
    "    \n",
    "    # 5. Price comparison data (latest available month)\n",
    "    latest_date = all_sectors['date'].max()\n",
    "    latest_prices = all_sectors[all_sectors['date'] == latest_date].copy()\n",
    "    latest_prices = latest_prices[['stateDescription', 'RTO_ISO', 'price']]\n",
    "    latest_prices.columns = ['state', 'rto_iso', 'price']\n",
    "    latest_prices.to_csv(os.path.join(output_dir, 'latest_prices.csv'), index=False)\n",
    "    \n",
    "    # 6. Dashboard metadata\n",
    "    major_rtos = ['CAISO', 'ERCOT', 'ISO-NE', 'MISO', 'NYISO', 'PJM', 'SPP', 'WEIM']\n",
    "    dashboard_meta = {\n",
    "        'data_updated': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'date_range': {\n",
    "            'start': all_sectors['date'].min().strftime('%Y-%m-%d'),\n",
    "            'end': all_sectors['date'].max().strftime('%Y-%m-%d')\n",
    "        },\n",
    "        'major_rtos': major_rtos,\n",
    "        'available_metrics': ['price', 'revenue', 'sales', 'customers'],\n",
    "        'total_records': len(all_sectors)\n",
    "    }\n",
    "    \n",
    "    # Save metadata as JSON\n",
    "    import json\n",
    "    with open(os.path.join(output_dir, 'dashboard_meta.json'), 'w') as f:\n",
    "        json.dump(dashboard_meta, f, indent=2)\n",
    "    \n",
    "    print(f\"Dashboard data prepared and saved to {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the entire analysis pipeline\n",
    "    run_rto_analysis_pipeline(input_file='monthly_all_states_2020_2024.xlsx', output_dir='rto_analysis' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 276\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Run the optimization\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m model, solution \u001b[38;5;241m=\u001b[39m green_llm_optimization(\u001b[43mparams\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "\n",
    "def green_llm_optimization(params):\n",
    "    \"\"\"\n",
    "    Implement the Green-LLM optimization model using Gurobi\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary containing all necessary parameters\n",
    "        \n",
    "    Returns:\n",
    "        model: Solved Gurobi model\n",
    "        solution: Dictionary containing optimal decision variables\n",
    "    \"\"\"\n",
    "    # Extract parameters\n",
    "    I = params['I']  # Set of user areas\n",
    "    J = params['J']  # Set of data centers\n",
    "    K = params['K']  # Set of query types\n",
    "    T = params['T']  # Set of time slots\n",
    "    R = params['R']  # Set of resource types\n",
    "    DeltaT = params['DeltaT']  # Set of time intervals\n",
    "\n",
    "    \n",
    "    # Query parameters\n",
    "    lmbda = params['lambda']      # Number of type k queries from area i at time t\n",
    "    h = params['h']               # Average input token count for type k queries\n",
    "    f = params['f']               # Average output token count for type k queries\n",
    "    \n",
    "    # Energy parameters\n",
    "    tau_in = params['tau_in']     # Energy consumption coefficient for input tokens\n",
    "    tau_out = params['tau_out']   # Energy consumption coefficient for output tokens\n",
    "    c = params['c']               # Local marginal price at DC j at time t\n",
    "    theta = params['theta']       # Carbon intensity at DC j\n",
    "    delta = params['delta']       # Carbon tax for DC j at time t\n",
    "    \n",
    "    # Data center parameters\n",
    "    P_w = params['P_w']          # Renewable energy generated at DC j at time t\n",
    "    P_max = params['P_max']      # Maximum power from grid for DC j at time t\n",
    "    PUE = params['PUE']          # Power Usage Effectiveness of DC j\n",
    "\n",
    "    # Resource parameters\n",
    "    C = params['C']              # Capacity of type r resource at DC j\n",
    "    alpha = params['alpha']      # Type r resource required for processing type k tokens\n",
    "    \n",
    "    # Model placement parameters\n",
    "    z_prev = params['z_prev']    # Known placement status from previous time slot\n",
    "    f_download = params['f_download']  # Submodel download cost at DC j\n",
    "    \n",
    "    # QoS parameters\n",
    "    s = params['s']              # Unmet penalty for type k query in area i at time t\n",
    "    Gamma = params['Gamma']      # Maximum allowed unmet demand ratio for area i\n",
    "    e = params['e']              # Error rate for processing type k queries\n",
    "    E = params['E']              # Minimum required accuracy for type k queries\n",
    "    \n",
    "    # Delay parameters\n",
    "    beta = params['beta']        # Average token size at time t\n",
    "    B = params['B']              # Available bandwidth between i and j\n",
    "    d = params['d']              # Network delay between area i and DC j\n",
    "    v = params['v']              # Processing delay at DC j at time t\n",
    "    Delta = params['Delta']      # Threshold on average round delay\n",
    "    \n",
    "    # Water parameters\n",
    "    WUE = params['WUE']          # Water Usage Effectiveness of DC j at time t\n",
    "    EWIF = params['EWIF']        # Energy-Water Intensity Factor of DC j at time t\n",
    "    Z = params['Z']              # Maximum allowed water consumption\n",
    "    \n",
    "    # Create a new model\n",
    "    model = gp.Model(\"Green-LLM\")\n",
    "    \n",
    "    # Create decision variables using addVars for cleaner implementation\n",
    "    x = model.addVars([(i, j, k, t) for i in I for j in J for k in K for t in T], vtype=GRB.CONTINUOUS, lb=0, ub=1, name=\"x\")\n",
    "    P_g = model.addVars([(j, t) for j in J for t in T], vtype=GRB.CONTINUOUS, lb=0, name=\"P_g\")\n",
    "    z = model.addVars([(j, k, t) for j in J for k in K for t in T], vtype=GRB.BINARY, name=\"z\")\n",
    "    q = model.addVars([(i, k, t) for i in I for k in K for t in T], vtype=GRB.CONTINUOUS, lb=0, ub=1, name=\"q\")\n",
    "    \n",
    "    # Create variables for each type of delay\n",
    "    D_tran = model.addVars([(i, k, t) for i in I for k in K for t in T], \n",
    "                          vtype=GRB.CONTINUOUS, lb=0, name=\"D_tran\")\n",
    "    D_prop = model.addVars([(i, k, t) for i in I for k in K for t in T], \n",
    "                          vtype=GRB.CONTINUOUS, lb=0, name=\"D_prop\")\n",
    "    D_proc = model.addVars([(i, k, t) for i in I for k in K for t in T], \n",
    "                          vtype=GRB.CONTINUOUS, lb=0, name=\"D_proc\")\n",
    "    \n",
    "    # Water consumption variable\n",
    "    woc = model.addVars([(j, t) for j in J for t in T], vtype=GRB.CONTINUOUS, lb=0, name=\"woc\")\n",
    "    \n",
    "    # Helper variables\n",
    "    eta = model.addVars([(j, t) for j in J for t in T], vtype=GRB.CONTINUOUS, lb=0, name=\"eta\")\n",
    "    P_d = model.addVars([(j, t) for j in J for t in T], vtype=GRB.CONTINUOUS, lb=0, name=\"P_d\")\n",
    "    P_c = model.addVars([(j,t) for j in J for t in T], vtype=GRB.CONTINUOUS, lb=0, name=\"P_c\")\n",
    "    # Update model to integrate new variables\n",
    "    model.update()\n",
    "    \n",
    "    # Objective function components\n",
    "    \n",
    "    # C1: Energy consumption cost\n",
    "    # C1 = gp.quicksum((tau_in[k] * h[k] * lmbda[i, k, t] * x[i, j, k, t] + tau_out[k] * f[k] * lmbda[i, k, t] * x[i, j, k, t]) * gamma[j, t] for i in I for j in J for k in K for t in T)\n",
    "    \n",
    "    # C1: Power procurement cost\n",
    "    C1 = gp.quicksum(c[j, t] * P_g[j, t] for j in J for t in T)\n",
    "    \n",
    "    # C3: Carbon tax\n",
    "    C2 = gp.quicksum(delta[j, t] * theta[j] * P_g[j, t] for j in J for t in T)\n",
    "    \n",
    "    # C4: Model placement and storage cost (avoiding redundant downloads)\n",
    "    C3_terms = []\n",
    "    for j in J:\n",
    "        for k in K:\n",
    "            for t in T:\n",
    "                if t == 0:\n",
    "                    # For the first time slot, use the provided z_prev\n",
    "                    C3_terms.append(f_download[j] * (1 - z_prev[j, k, t]) * z[j, k, t])\n",
    "                else:\n",
    "                    # For subsequent time slots, use the decision from previous time\n",
    "                    C3_terms.append(f_download[j] * (1 - z[j, k, t-1]) * z[j, k, t])\n",
    "    \n",
    "    C3 = gp.quicksum(C3_terms)\n",
    "    \n",
    "    # C5: Unmet demand penalty\n",
    "    C4 = gp.quicksum(s[i, k, t] * lmbda[i, k, t] * q[i, k, t] for i in I for k in K for t in T)\n",
    "    \n",
    "    # Set the objective (minimize total cost)\n",
    "    model.setObjective(C1 + C2 + C3 + C4, GRB.MINIMIZE)\n",
    "    \n",
    "    # Computational energy consumption\n",
    "    model.addConstrs((P_c[j,t] == gp.quicksum((tau_in[k] * h[k] + tau_out[k] * f[k]) * lmbda[i, k, t] * x[i, j, k, t] for i in I for k in K) for j in J for t in T), name=\"computation_consumption\")\n",
    "    \n",
    "    # Total Energy consumption (including cooling and other overheads)\n",
    "    model.addConstrs((P_d[j, t] == PUE[j] * P_c[j, t] for j in J for t in T), name=\"tot_power_consumption\")\n",
    "\n",
    "    # Power balance constraints (Equation 7)\n",
    "    model.addConstrs((P_d[j, t] <= P_g[j, t] + P_w[j, t] for j in J for t in T), name=\"power_balance\")\n",
    "    \n",
    "    # Grid capacity condition (Equation 8)\n",
    "    model.addConstrs((P_g[j, t] <= P_max[j, t] for j in J for t in T), name=\"grid_capacity\")\n",
    "    \n",
    "    # Long-term water footprint constraints (Equation 10)\n",
    "    model.addConstrs((woc[j, t] == (WUE[j, t] / PUE[j] + EWIF[j, t]) * P_d[j, t] for j in J for t in T),name=\"water_consumption\")\n",
    "    model.addConstr(gp.quicksum(woc[j, t] for j in J for t in DeltaT) <= Z, name=\"long_term_water_footprint\")\n",
    "    \n",
    "    \n",
    "    # Ensure workload allocation only if model is placed (Equation 11a)\n",
    "    # since z[j,k,t]=1 means the model IS placed at DC j\n",
    "    model.addConstrs((x[i, j, k, t] <= z[j, k, t] for i in I for j in J for k in K for t in T),name=\"model_placement\")\n",
    "    \n",
    "    # Ensure resource capacity is not exceeded (Equation 11b)\n",
    "    model.addConstrs((gp.quicksum(alpha[k, r] * f[k] * lmbda[i, k, t] * x[i, j, k, t] for i in I for k in K) <=  C[r, j] for j in J for r in R for t in T),name=\"resource_capacity\")\n",
    "    \n",
    "    # Workload allocation constraints (Equation 12)\n",
    "    model.addConstrs((q[i, k, t] + gp.quicksum(x[i, j, k, t] for j in J) == 1 for i in I for k in K for t in T), name=\"workload_allocation\")\n",
    "    \n",
    "    # Define transmission delay (Equation 13)\n",
    "    for i in I:\n",
    "        for k in K:\n",
    "            for t in T:\n",
    "                # Add a small epsilon to avoid division by zero in bandwidth\n",
    "                model.addConstr(\n",
    "                    D_tran[i, k, t] == gp.quicksum(\n",
    "                        beta[i, k, t] * lmbda[i, k, t] * x[i, j, k, t] * (h[k] + f[k]) / max(0.001, B[i, j])\n",
    "                        for j in J\n",
    "                    ),\n",
    "                    f\"trans_delay_def_{i}_{k}_{t}\"\n",
    "                )\n",
    "\n",
    "    # Define propagation delay (Equation 14)\n",
    "    model.addConstrs((D_prop[i, k, t] == gp.quicksum(d[i, j] * lmbda[i, k, t] * x[i, j, k, t] * (h[k] + f[k]) for j in J) for i in I for k in K for t in T), name=\"prop_delay_def\")\n",
    "\n",
    "    # Define processing delay (Equation 15)\n",
    "    model.addConstrs((D_proc[i, k, t] == gp.quicksum(v[j, t] * f[k] * lmbda[i, k, t] * x[i, j, k, t] for j in J) for i in I for k in K for t in T), name=\"proc_delay_def\")\n",
    "\n",
    "    # Total delay constraint (Equation 16)\n",
    "    # model.addConstrs((D_tran[i, k, t] + D_prop[i, k, t] + D_proc[i, k, t] <=  Delta[i, k, t] for i in I for k in K for t in T), name=\"max_delay\")\n",
    "    \n",
    "   #  QoS constraints (Equation 17)\n",
    "    # model.addConstrs(\n",
    "    #     (q[i, k, t] <= Gamma[i] for i in I for k in K for t in T),\n",
    "    #     name=\"qos\"\n",
    "    # )\n",
    "    \n",
    "    # Accuracy constraints (Equation 18)\n",
    "    # Modified to handle potential infeasibility - make sure accuracy requirement is proportional to satisfied demand\n",
    "    for k in K:\n",
    "        for t in T:\n",
    "            total_demand = gp.quicksum(lmbda[i, k, t] for i in I)\n",
    "            satisfied_demand = gp.quicksum((1 - q[i, k, t]) * lmbda[i, k, t] for i in I)\n",
    "            model.addConstr(gp.quicksum((1 - e[k]) * lmbda[i, k, t] * x[i, j, k, t] for i in I for j in J) >= E[k] * satisfied_demand,\n",
    "                f\"accuracy_{k}_{t}\"\n",
    "            )\n",
    "    \n",
    "    # Set optimization parameters\n",
    "    model.setParam('OutputFlag', 0)        # Display optimization details\n",
    "    model.setParam('TimeLimit', 600)       # 10-minute time limit\n",
    "    model.setParam('MIPGap', 0.05)         # Accept solutions within 5% of optimal\n",
    "    model.setParam('FeasibilityTol', 1e-5) # Slightly relaxed feasibility tolerance\n",
    "    model.setParam('NumericFocus', 3)      # Maximum numeric stability\n",
    "    \n",
    "    # Optional: Add warm start if available from previous solution\n",
    "    if 'previous_solution' in params:\n",
    "        prev_sol = params['previous_solution']\n",
    "        for i in I:\n",
    "            for j in J:\n",
    "                for k in K:\n",
    "                    for t in T:\n",
    "                        if (i, j, k, t) in prev_sol['x']:\n",
    "                            x[i, j, k, t].start = prev_sol['x'][i, j, k, t]\n",
    "    \n",
    "    # Optimize the model\n",
    "    try:\n",
    "        model.optimize()\n",
    "    except gp.GurobiError as e:\n",
    "        print(f\"Optimization error: {e}\")\n",
    "        return model, None\n",
    "    \n",
    "    # Check if optimal solution found\n",
    "    if model.status == GRB.OPTIMAL or model.status == GRB.TIME_LIMIT or model.status == GRB.SUBOPTIMAL:\n",
    "        # Extract solution\n",
    "        solution = {\n",
    "            'x': {(i, j, k, t): x[i, j, k, t].X for i in I for j in J for k in K for t in T},\n",
    "            'P_g': {(j, t): P_g[j, t].X for j in J for t in T},\n",
    "            'P_c': {(j, t): P_c[j, t].X for j in J for t in T},\n",
    "            'P_d': {(j, t): P_d[j, t].X for j in J for t in T},\n",
    "            'z': {(j, k, t): z[j, k, t].X for j in J for k in K for t in T},\n",
    "            'q': {(i, k, t): q[i, k, t].X for i in I for k in K for t in T},\n",
    "            'objective_value': model.objVal,\n",
    "            'objective_components': {\n",
    "                'C1': C1.getValue(),\n",
    "                'C2': C2.getValue(),\n",
    "                'C3': C3.getValue(),\n",
    "                'C4': C4.getValue(),\n",
    "            },\n",
    "            'status': model.status\n",
    "        }\n",
    "        \n",
    "        # Add solution quality metrics\n",
    "        if model.status == GRB.OPTIMAL:\n",
    "            solution['quality'] = \"Optimal\"\n",
    "            print(f\"Optimization status: {solution['quality']}\")\n",
    "            print(f\"Objective value: {solution['objective_value']:.2f}\")\n",
    "            print(f\"Energy cost: {solution['objective_components']['C1']:.2f}\")\n",
    "        elif model.status == GRB.TIME_LIMIT:\n",
    "            solution['quality'] = f\"Time limit reached. Gap: {model.MIPGap*100:.2f}%\"\n",
    "        elif model.status == GRB.SUBOPTIMAL:\n",
    "            solution['quality'] = \"Suboptimal solution found\"\n",
    "            \n",
    "        return model, solution\n",
    "    else:\n",
    "        status_codes = {\n",
    "            GRB.LOADED: \"Model is loaded, but not solved\",\n",
    "            GRB.INFEASIBLE: \"Model is infeasible\",\n",
    "            GRB.INF_OR_UNBD: \"Model is infeasible or unbounded\",\n",
    "            GRB.UNBOUNDED: \"Model is unbounded\",\n",
    "            GRB.ITERATION_LIMIT: \"Iteration limit reached\",\n",
    "            GRB.NODE_LIMIT: \"Node limit reached\",\n",
    "            GRB.SOLUTION_LIMIT: \"Solution limit reached\",\n",
    "            GRB.INTERRUPTED: \"Optimization was interrupted\",\n",
    "            GRB.INPROGRESS: \"Optimization in progress\",\n",
    "            GRB.USER_OBJ_LIMIT: \"User objective limit reached\"\n",
    "        }\n",
    "        \n",
    "        status_msg = status_codes.get(model.status, f\"Unknown status: {model.status}\")\n",
    "        print(f\"Optimization failed: {status_msg}\")\n",
    "        \n",
    "        # If infeasible, try to find the source of infeasibility\n",
    "        if model.status == GRB.INFEASIBLE:\n",
    "            print(\"Computing IIS (Irreducible Inconsistent Subsystem)...\")\n",
    "            model.computeIIS()\n",
    "            model.write(\"model_iis.ilp\")\n",
    "            print(\"IIS written to model_iis.ilp\")\n",
    "            \n",
    "        return model, None\n",
    "    \n",
    "# Run the optimization\n",
    "model, solution = green_llm_optimization(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample demand (lambda) values:\n",
      "Region 0, Query type 0, Hour 0: 44 queries\n",
      "Region 0, Query type 0, Hour 1: 45 queries\n",
      "Region 0, Query type 0, Hour 2: 40 queries\n",
      "Region 0, Query type 0, Hour 3: 35 queries\n",
      "Region 0, Query type 0, Hour 4: 40 queries\n",
      "Region 0, Query type 0, Hour 5: 32 queries\n",
      "Region 0, Query type 0, Hour 6: 44 queries\n",
      "Region 0, Query type 0, Hour 7: 30 queries\n",
      "Region 0, Query type 0, Hour 8: 36 queries\n",
      "Region 0, Query type 0, Hour 9: 30 queries\n",
      "Region 0, Query type 0, Hour 10: 46 queries\n",
      "Region 0, Query type 0, Hour 11: 38 queries\n",
      "Region 0, Query type 0, Hour 12: 42 queries\n",
      "Region 0, Query type 0, Hour 13: 40 queries\n",
      "Region 0, Query type 0, Hour 14: 49 queries\n",
      "Region 0, Query type 0, Hour 15: 35 queries\n",
      "Region 0, Query type 0, Hour 16: 30 queries\n",
      "Region 0, Query type 0, Hour 17: 36 queries\n",
      "Region 0, Query type 0, Hour 18: 41 queries\n",
      "Region 0, Query type 0, Hour 19: 49 queries\n",
      "Region 0, Query type 0, Hour 20: 48 queries\n",
      "Region 0, Query type 0, Hour 21: 46 queries\n",
      "Region 0, Query type 0, Hour 22: 33 queries\n",
      "Region 0, Query type 0, Hour 23: 32 queries\n",
      "Region 0, Query type 1, Hour 0: 30 queries\n",
      "Region 0, Query type 1, Hour 1: 47 queries\n",
      "Region 0, Query type 1, Hour 2: 50 queries\n",
      "Region 0, Query type 1, Hour 3: 44 queries\n",
      "Region 0, Query type 1, Hour 4: 37 queries\n",
      "Region 0, Query type 1, Hour 5: 30 queries\n",
      "Region 0, Query type 1, Hour 6: 49 queries\n",
      "Region 0, Query type 1, Hour 7: 43 queries\n",
      "Region 0, Query type 1, Hour 8: 33 queries\n",
      "Region 0, Query type 1, Hour 9: 34 queries\n",
      "Region 0, Query type 1, Hour 10: 42 queries\n",
      "Region 0, Query type 1, Hour 11: 44 queries\n",
      "Region 0, Query type 1, Hour 12: 35 queries\n",
      "Region 0, Query type 1, Hour 13: 30 queries\n",
      "Region 0, Query type 1, Hour 14: 48 queries\n",
      "Region 0, Query type 1, Hour 15: 42 queries\n",
      "Region 0, Query type 1, Hour 16: 35 queries\n",
      "Region 0, Query type 1, Hour 17: 44 queries\n",
      "Region 0, Query type 1, Hour 18: 45 queries\n",
      "Region 0, Query type 1, Hour 19: 45 queries\n",
      "Region 0, Query type 1, Hour 20: 39 queries\n",
      "Region 0, Query type 1, Hour 21: 38 queries\n",
      "Region 0, Query type 1, Hour 22: 38 queries\n",
      "Region 0, Query type 1, Hour 23: 46 queries\n",
      "Region 0, Query type 2, Hour 0: 46 queries\n",
      "Region 0, Query type 2, Hour 1: 46 queries\n",
      "Region 0, Query type 2, Hour 2: 41 queries\n",
      "Region 0, Query type 2, Hour 3: 42 queries\n",
      "Region 0, Query type 2, Hour 4: 41 queries\n",
      "Region 0, Query type 2, Hour 5: 43 queries\n",
      "Region 0, Query type 2, Hour 6: 39 queries\n",
      "Region 0, Query type 2, Hour 7: 48 queries\n",
      "Region 0, Query type 2, Hour 8: 31 queries\n",
      "Region 0, Query type 2, Hour 9: 33 queries\n",
      "Region 0, Query type 2, Hour 10: 48 queries\n",
      "Region 0, Query type 2, Hour 11: 36 queries\n",
      "Region 0, Query type 2, Hour 12: 44 queries\n",
      "Region 0, Query type 2, Hour 13: 44 queries\n",
      "Region 0, Query type 2, Hour 14: 35 queries\n",
      "Region 0, Query type 2, Hour 15: 32 queries\n",
      "Region 0, Query type 2, Hour 16: 35 queries\n",
      "Region 0, Query type 2, Hour 17: 45 queries\n",
      "Region 0, Query type 2, Hour 18: 31 queries\n",
      "Region 0, Query type 2, Hour 19: 36 queries\n",
      "Region 0, Query type 2, Hour 20: 40 queries\n",
      "Region 0, Query type 2, Hour 21: 36 queries\n",
      "Region 0, Query type 2, Hour 22: 33 queries\n",
      "Region 0, Query type 2, Hour 23: 49 queries\n",
      "Region 0, Query type 3, Hour 0: 36 queries\n",
      "Region 0, Query type 3, Hour 1: 33 queries\n",
      "Region 0, Query type 3, Hour 2: 41 queries\n",
      "Region 0, Query type 3, Hour 3: 34 queries\n",
      "Region 0, Query type 3, Hour 4: 48 queries\n",
      "Region 0, Query type 3, Hour 5: 34 queries\n",
      "Region 0, Query type 3, Hour 6: 48 queries\n",
      "Region 0, Query type 3, Hour 7: 50 queries\n",
      "Region 0, Query type 3, Hour 8: 32 queries\n",
      "Region 0, Query type 3, Hour 9: 40 queries\n",
      "Region 0, Query type 3, Hour 10: 36 queries\n",
      "Region 0, Query type 3, Hour 11: 46 queries\n",
      "Region 0, Query type 3, Hour 12: 38 queries\n",
      "Region 0, Query type 3, Hour 13: 50 queries\n",
      "Region 0, Query type 3, Hour 14: 38 queries\n",
      "Region 0, Query type 3, Hour 15: 39 queries\n",
      "Region 0, Query type 3, Hour 16: 42 queries\n",
      "Region 0, Query type 3, Hour 17: 44 queries\n",
      "Region 0, Query type 3, Hour 18: 34 queries\n",
      "Region 0, Query type 3, Hour 19: 32 queries\n",
      "Region 0, Query type 3, Hour 20: 37 queries\n",
      "Region 0, Query type 3, Hour 21: 49 queries\n",
      "Region 0, Query type 3, Hour 22: 39 queries\n",
      "Region 0, Query type 3, Hour 23: 42 queries\n",
      "Region 0, Query type 4, Hour 0: 32 queries\n",
      "Region 0, Query type 4, Hour 1: 37 queries\n",
      "Region 0, Query type 4, Hour 2: 38 queries\n",
      "Region 0, Query type 4, Hour 3: 50 queries\n",
      "Region 0, Query type 4, Hour 4: 41 queries\n",
      "Region 0, Query type 4, Hour 5: 40 queries\n",
      "Region 0, Query type 4, Hour 6: 31 queries\n",
      "Region 0, Query type 4, Hour 7: 47 queries\n",
      "Region 0, Query type 4, Hour 8: 38 queries\n",
      "Region 0, Query type 4, Hour 9: 31 queries\n",
      "Region 0, Query type 4, Hour 10: 31 queries\n",
      "Region 0, Query type 4, Hour 11: 32 queries\n",
      "Region 0, Query type 4, Hour 12: 45 queries\n",
      "Region 0, Query type 4, Hour 13: 48 queries\n",
      "Region 0, Query type 4, Hour 14: 33 queries\n",
      "Region 0, Query type 4, Hour 15: 47 queries\n",
      "Region 0, Query type 4, Hour 16: 49 queries\n",
      "Region 0, Query type 4, Hour 17: 47 queries\n",
      "Region 0, Query type 4, Hour 18: 39 queries\n",
      "Region 0, Query type 4, Hour 19: 40 queries\n",
      "Region 0, Query type 4, Hour 20: 46 queries\n",
      "Region 0, Query type 4, Hour 21: 49 queries\n",
      "Region 0, Query type 4, Hour 22: 45 queries\n",
      "Region 0, Query type 4, Hour 23: 49 queries\n",
      "Region 1, Query type 0, Hour 0: 42 queries\n",
      "Region 1, Query type 0, Hour 1: 31 queries\n",
      "Region 1, Query type 0, Hour 2: 31 queries\n",
      "Region 1, Query type 0, Hour 3: 43 queries\n",
      "Region 1, Query type 0, Hour 4: 33 queries\n",
      "Region 1, Query type 0, Hour 5: 46 queries\n",
      "Region 1, Query type 0, Hour 6: 42 queries\n",
      "Region 1, Query type 0, Hour 7: 33 queries\n",
      "Region 1, Query type 0, Hour 8: 32 queries\n",
      "Region 1, Query type 0, Hour 9: 41 queries\n",
      "Region 1, Query type 0, Hour 10: 37 queries\n",
      "Region 1, Query type 0, Hour 11: 31 queries\n",
      "Region 1, Query type 0, Hour 12: 35 queries\n",
      "Region 1, Query type 0, Hour 13: 39 queries\n",
      "Region 1, Query type 0, Hour 14: 41 queries\n",
      "Region 1, Query type 0, Hour 15: 50 queries\n",
      "Region 1, Query type 0, Hour 16: 39 queries\n",
      "Region 1, Query type 0, Hour 17: 39 queries\n",
      "Region 1, Query type 0, Hour 18: 32 queries\n",
      "Region 1, Query type 0, Hour 19: 49 queries\n",
      "Region 1, Query type 0, Hour 20: 39 queries\n",
      "Region 1, Query type 0, Hour 21: 38 queries\n",
      "Region 1, Query type 0, Hour 22: 49 queries\n",
      "Region 1, Query type 0, Hour 23: 48 queries\n",
      "Region 1, Query type 1, Hour 0: 43 queries\n",
      "Region 1, Query type 1, Hour 1: 31 queries\n",
      "Region 1, Query type 1, Hour 2: 34 queries\n",
      "Region 1, Query type 1, Hour 3: 49 queries\n",
      "Region 1, Query type 1, Hour 4: 37 queries\n",
      "Region 1, Query type 1, Hour 5: 36 queries\n",
      "Region 1, Query type 1, Hour 6: 35 queries\n",
      "Region 1, Query type 1, Hour 7: 40 queries\n",
      "Region 1, Query type 1, Hour 8: 41 queries\n",
      "Region 1, Query type 1, Hour 9: 47 queries\n",
      "Region 1, Query type 1, Hour 10: 48 queries\n",
      "Region 1, Query type 1, Hour 11: 39 queries\n",
      "Region 1, Query type 1, Hour 12: 50 queries\n",
      "Region 1, Query type 1, Hour 13: 43 queries\n",
      "Region 1, Query type 1, Hour 14: 32 queries\n",
      "Region 1, Query type 1, Hour 15: 47 queries\n",
      "Region 1, Query type 1, Hour 16: 44 queries\n",
      "Region 1, Query type 1, Hour 17: 35 queries\n",
      "Region 1, Query type 1, Hour 18: 39 queries\n",
      "Region 1, Query type 1, Hour 19: 41 queries\n",
      "Region 1, Query type 1, Hour 20: 36 queries\n",
      "Region 1, Query type 1, Hour 21: 46 queries\n",
      "Region 1, Query type 1, Hour 22: 30 queries\n",
      "Region 1, Query type 1, Hour 23: 47 queries\n",
      "Region 1, Query type 2, Hour 0: 44 queries\n",
      "Region 1, Query type 2, Hour 1: 48 queries\n",
      "Region 1, Query type 2, Hour 2: 30 queries\n",
      "Region 1, Query type 2, Hour 3: 34 queries\n",
      "Region 1, Query type 2, Hour 4: 44 queries\n",
      "Region 1, Query type 2, Hour 5: 32 queries\n",
      "Region 1, Query type 2, Hour 6: 30 queries\n",
      "Region 1, Query type 2, Hour 7: 37 queries\n",
      "Region 1, Query type 2, Hour 8: 41 queries\n",
      "Region 1, Query type 2, Hour 9: 48 queries\n",
      "Region 1, Query type 2, Hour 10: 41 queries\n",
      "Region 1, Query type 2, Hour 11: 42 queries\n",
      "Region 1, Query type 2, Hour 12: 30 queries\n",
      "Region 1, Query type 2, Hour 13: 48 queries\n",
      "Region 1, Query type 2, Hour 14: 35 queries\n",
      "Region 1, Query type 2, Hour 15: 37 queries\n",
      "Region 1, Query type 2, Hour 16: 33 queries\n",
      "Region 1, Query type 2, Hour 17: 34 queries\n",
      "Region 1, Query type 2, Hour 18: 36 queries\n",
      "Region 1, Query type 2, Hour 19: 33 queries\n",
      "Region 1, Query type 2, Hour 20: 49 queries\n",
      "Region 1, Query type 2, Hour 21: 33 queries\n",
      "Region 1, Query type 2, Hour 22: 44 queries\n",
      "Region 1, Query type 2, Hour 23: 50 queries\n",
      "Region 1, Query type 3, Hour 0: 34 queries\n",
      "Region 1, Query type 3, Hour 1: 38 queries\n",
      "Region 1, Query type 3, Hour 2: 31 queries\n",
      "Region 1, Query type 3, Hour 3: 36 queries\n",
      "Region 1, Query type 3, Hour 4: 46 queries\n",
      "Region 1, Query type 3, Hour 5: 45 queries\n",
      "Region 1, Query type 3, Hour 6: 46 queries\n",
      "Region 1, Query type 3, Hour 7: 49 queries\n",
      "Region 1, Query type 3, Hour 8: 31 queries\n",
      "Region 1, Query type 3, Hour 9: 40 queries\n",
      "Region 1, Query type 3, Hour 10: 31 queries\n",
      "Region 1, Query type 3, Hour 11: 30 queries\n",
      "Region 1, Query type 3, Hour 12: 31 queries\n",
      "Region 1, Query type 3, Hour 13: 38 queries\n",
      "Region 1, Query type 3, Hour 14: 31 queries\n",
      "Region 1, Query type 3, Hour 15: 44 queries\n",
      "Region 1, Query type 3, Hour 16: 39 queries\n",
      "Region 1, Query type 3, Hour 17: 44 queries\n",
      "Region 1, Query type 3, Hour 18: 39 queries\n",
      "Region 1, Query type 3, Hour 19: 46 queries\n",
      "Region 1, Query type 3, Hour 20: 38 queries\n",
      "Region 1, Query type 3, Hour 21: 47 queries\n",
      "Region 1, Query type 3, Hour 22: 33 queries\n",
      "Region 1, Query type 3, Hour 23: 38 queries\n",
      "Region 1, Query type 4, Hour 0: 45 queries\n",
      "Region 1, Query type 4, Hour 1: 33 queries\n",
      "Region 1, Query type 4, Hour 2: 45 queries\n",
      "Region 1, Query type 4, Hour 3: 48 queries\n",
      "Region 1, Query type 4, Hour 4: 30 queries\n",
      "Region 1, Query type 4, Hour 5: 35 queries\n",
      "Region 1, Query type 4, Hour 6: 39 queries\n",
      "Region 1, Query type 4, Hour 7: 45 queries\n",
      "Region 1, Query type 4, Hour 8: 33 queries\n",
      "Region 1, Query type 4, Hour 9: 42 queries\n",
      "Region 1, Query type 4, Hour 10: 45 queries\n",
      "Region 1, Query type 4, Hour 11: 49 queries\n",
      "Region 1, Query type 4, Hour 12: 35 queries\n",
      "Region 1, Query type 4, Hour 13: 40 queries\n",
      "Region 1, Query type 4, Hour 14: 32 queries\n",
      "Region 1, Query type 4, Hour 15: 49 queries\n",
      "Region 1, Query type 4, Hour 16: 39 queries\n",
      "Region 1, Query type 4, Hour 17: 30 queries\n",
      "Region 1, Query type 4, Hour 18: 49 queries\n",
      "Region 1, Query type 4, Hour 19: 40 queries\n",
      "Region 1, Query type 4, Hour 20: 41 queries\n",
      "Region 1, Query type 4, Hour 21: 43 queries\n",
      "Region 1, Query type 4, Hour 22: 37 queries\n",
      "Region 1, Query type 4, Hour 23: 40 queries\n",
      "Region 2, Query type 0, Hour 0: 33 queries\n",
      "Region 2, Query type 0, Hour 1: 36 queries\n",
      "Region 2, Query type 0, Hour 2: 39 queries\n",
      "Region 2, Query type 0, Hour 3: 48 queries\n",
      "Region 2, Query type 0, Hour 4: 33 queries\n",
      "Region 2, Query type 0, Hour 5: 31 queries\n",
      "Region 2, Query type 0, Hour 6: 50 queries\n",
      "Region 2, Query type 0, Hour 7: 47 queries\n",
      "Region 2, Query type 0, Hour 8: 43 queries\n",
      "Region 2, Query type 0, Hour 9: 45 queries\n",
      "Region 2, Query type 0, Hour 10: 40 queries\n",
      "Region 2, Query type 0, Hour 11: 39 queries\n",
      "Region 2, Query type 0, Hour 12: 46 queries\n",
      "Region 2, Query type 0, Hour 13: 43 queries\n",
      "Region 2, Query type 0, Hour 14: 33 queries\n",
      "Region 2, Query type 0, Hour 15: 48 queries\n",
      "Region 2, Query type 0, Hour 16: 31 queries\n",
      "Region 2, Query type 0, Hour 17: 36 queries\n",
      "Region 2, Query type 0, Hour 18: 38 queries\n",
      "Region 2, Query type 0, Hour 19: 48 queries\n",
      "Region 2, Query type 0, Hour 20: 45 queries\n",
      "Region 2, Query type 0, Hour 21: 45 queries\n",
      "Region 2, Query type 0, Hour 22: 47 queries\n",
      "Region 2, Query type 0, Hour 23: 50 queries\n",
      "Region 2, Query type 1, Hour 0: 50 queries\n",
      "Region 2, Query type 1, Hour 1: 40 queries\n",
      "Region 2, Query type 1, Hour 2: 36 queries\n",
      "Region 2, Query type 1, Hour 3: 38 queries\n",
      "Region 2, Query type 1, Hour 4: 33 queries\n",
      "Region 2, Query type 1, Hour 5: 42 queries\n",
      "Region 2, Query type 1, Hour 6: 33 queries\n",
      "Region 2, Query type 1, Hour 7: 48 queries\n",
      "Region 2, Query type 1, Hour 8: 31 queries\n",
      "Region 2, Query type 1, Hour 9: 38 queries\n",
      "Region 2, Query type 1, Hour 10: 41 queries\n",
      "Region 2, Query type 1, Hour 11: 31 queries\n",
      "Region 2, Query type 1, Hour 12: 35 queries\n",
      "Region 2, Query type 1, Hour 13: 34 queries\n",
      "Region 2, Query type 1, Hour 14: 34 queries\n",
      "Region 2, Query type 1, Hour 15: 49 queries\n",
      "Region 2, Query type 1, Hour 16: 34 queries\n",
      "Region 2, Query type 1, Hour 17: 33 queries\n",
      "Region 2, Query type 1, Hour 18: 38 queries\n",
      "Region 2, Query type 1, Hour 19: 39 queries\n",
      "Region 2, Query type 1, Hour 20: 45 queries\n",
      "Region 2, Query type 1, Hour 21: 39 queries\n",
      "Region 2, Query type 1, Hour 22: 39 queries\n",
      "Region 2, Query type 1, Hour 23: 48 queries\n",
      "Region 2, Query type 2, Hour 0: 39 queries\n",
      "Region 2, Query type 2, Hour 1: 35 queries\n",
      "Region 2, Query type 2, Hour 2: 46 queries\n",
      "Region 2, Query type 2, Hour 3: 34 queries\n",
      "Region 2, Query type 2, Hour 4: 35 queries\n",
      "Region 2, Query type 2, Hour 5: 44 queries\n",
      "Region 2, Query type 2, Hour 6: 48 queries\n",
      "Region 2, Query type 2, Hour 7: 47 queries\n",
      "Region 2, Query type 2, Hour 8: 33 queries\n",
      "Region 2, Query type 2, Hour 9: 37 queries\n",
      "Region 2, Query type 2, Hour 10: 45 queries\n",
      "Region 2, Query type 2, Hour 11: 48 queries\n",
      "Region 2, Query type 2, Hour 12: 49 queries\n",
      "Region 2, Query type 2, Hour 13: 32 queries\n",
      "Region 2, Query type 2, Hour 14: 39 queries\n",
      "Region 2, Query type 2, Hour 15: 31 queries\n",
      "Region 2, Query type 2, Hour 16: 33 queries\n",
      "Region 2, Query type 2, Hour 17: 35 queries\n",
      "Region 2, Query type 2, Hour 18: 40 queries\n",
      "Region 2, Query type 2, Hour 19: 43 queries\n",
      "Region 2, Query type 2, Hour 20: 45 queries\n",
      "Region 2, Query type 2, Hour 21: 50 queries\n",
      "Region 2, Query type 2, Hour 22: 48 queries\n",
      "Region 2, Query type 2, Hour 23: 31 queries\n",
      "Region 2, Query type 3, Hour 0: 46 queries\n",
      "Region 2, Query type 3, Hour 1: 38 queries\n",
      "Region 2, Query type 3, Hour 2: 46 queries\n",
      "Region 2, Query type 3, Hour 3: 44 queries\n",
      "Region 2, Query type 3, Hour 4: 36 queries\n",
      "Region 2, Query type 3, Hour 5: 48 queries\n",
      "Region 2, Query type 3, Hour 6: 32 queries\n",
      "Region 2, Query type 3, Hour 7: 30 queries\n",
      "Region 2, Query type 3, Hour 8: 33 queries\n",
      "Region 2, Query type 3, Hour 9: 47 queries\n",
      "Region 2, Query type 3, Hour 10: 37 queries\n",
      "Region 2, Query type 3, Hour 11: 48 queries\n",
      "Region 2, Query type 3, Hour 12: 35 queries\n",
      "Region 2, Query type 3, Hour 13: 30 queries\n",
      "Region 2, Query type 3, Hour 14: 37 queries\n",
      "Region 2, Query type 3, Hour 15: 30 queries\n",
      "Region 2, Query type 3, Hour 16: 48 queries\n",
      "Region 2, Query type 3, Hour 17: 40 queries\n",
      "Region 2, Query type 3, Hour 18: 50 queries\n",
      "Region 2, Query type 3, Hour 19: 44 queries\n",
      "Region 2, Query type 3, Hour 20: 40 queries\n",
      "Region 2, Query type 3, Hour 21: 48 queries\n",
      "Region 2, Query type 3, Hour 22: 50 queries\n",
      "Region 2, Query type 3, Hour 23: 39 queries\n",
      "Region 2, Query type 4, Hour 0: 41 queries\n",
      "Region 2, Query type 4, Hour 1: 30 queries\n",
      "Region 2, Query type 4, Hour 2: 36 queries\n",
      "Region 2, Query type 4, Hour 3: 46 queries\n",
      "Region 2, Query type 4, Hour 4: 39 queries\n",
      "Region 2, Query type 4, Hour 5: 35 queries\n",
      "Region 2, Query type 4, Hour 6: 46 queries\n",
      "Region 2, Query type 4, Hour 7: 47 queries\n",
      "Region 2, Query type 4, Hour 8: 34 queries\n",
      "Region 2, Query type 4, Hour 9: 47 queries\n",
      "Region 2, Query type 4, Hour 10: 46 queries\n",
      "Region 2, Query type 4, Hour 11: 45 queries\n",
      "Region 2, Query type 4, Hour 12: 37 queries\n",
      "Region 2, Query type 4, Hour 13: 42 queries\n",
      "Region 2, Query type 4, Hour 14: 45 queries\n",
      "Region 2, Query type 4, Hour 15: 40 queries\n",
      "Region 2, Query type 4, Hour 16: 35 queries\n",
      "Region 2, Query type 4, Hour 17: 32 queries\n",
      "Region 2, Query type 4, Hour 18: 30 queries\n",
      "Region 2, Query type 4, Hour 19: 47 queries\n",
      "Region 2, Query type 4, Hour 20: 34 queries\n",
      "Region 2, Query type 4, Hour 21: 42 queries\n",
      "Region 2, Query type 4, Hour 22: 39 queries\n",
      "Region 2, Query type 4, Hour 23: 34 queries\n",
      "Region 3, Query type 0, Hour 0: 43 queries\n",
      "Region 3, Query type 0, Hour 1: 46 queries\n",
      "Region 3, Query type 0, Hour 2: 30 queries\n",
      "Region 3, Query type 0, Hour 3: 31 queries\n",
      "Region 3, Query type 0, Hour 4: 36 queries\n",
      "Region 3, Query type 0, Hour 5: 39 queries\n",
      "Region 3, Query type 0, Hour 6: 49 queries\n",
      "Region 3, Query type 0, Hour 7: 36 queries\n",
      "Region 3, Query type 0, Hour 8: 45 queries\n",
      "Region 3, Query type 0, Hour 9: 34 queries\n",
      "Region 3, Query type 0, Hour 10: 41 queries\n",
      "Region 3, Query type 0, Hour 11: 46 queries\n",
      "Region 3, Query type 0, Hour 12: 45 queries\n",
      "Region 3, Query type 0, Hour 13: 49 queries\n",
      "Region 3, Query type 0, Hour 14: 45 queries\n",
      "Region 3, Query type 0, Hour 15: 48 queries\n",
      "Region 3, Query type 0, Hour 16: 44 queries\n",
      "Region 3, Query type 0, Hour 17: 41 queries\n",
      "Region 3, Query type 0, Hour 18: 50 queries\n",
      "Region 3, Query type 0, Hour 19: 39 queries\n",
      "Region 3, Query type 0, Hour 20: 31 queries\n",
      "Region 3, Query type 0, Hour 21: 36 queries\n",
      "Region 3, Query type 0, Hour 22: 42 queries\n",
      "Region 3, Query type 0, Hour 23: 32 queries\n",
      "Region 3, Query type 1, Hour 0: 44 queries\n",
      "Region 3, Query type 1, Hour 1: 30 queries\n",
      "Region 3, Query type 1, Hour 2: 32 queries\n",
      "Region 3, Query type 1, Hour 3: 39 queries\n",
      "Region 3, Query type 1, Hour 4: 44 queries\n",
      "Region 3, Query type 1, Hour 5: 35 queries\n",
      "Region 3, Query type 1, Hour 6: 49 queries\n",
      "Region 3, Query type 1, Hour 7: 44 queries\n",
      "Region 3, Query type 1, Hour 8: 50 queries\n",
      "Region 3, Query type 1, Hour 9: 36 queries\n",
      "Region 3, Query type 1, Hour 10: 50 queries\n",
      "Region 3, Query type 1, Hour 11: 34 queries\n",
      "Region 3, Query type 1, Hour 12: 33 queries\n",
      "Region 3, Query type 1, Hour 13: 47 queries\n",
      "Region 3, Query type 1, Hour 14: 32 queries\n",
      "Region 3, Query type 1, Hour 15: 46 queries\n",
      "Region 3, Query type 1, Hour 16: 47 queries\n",
      "Region 3, Query type 1, Hour 17: 47 queries\n",
      "Region 3, Query type 1, Hour 18: 31 queries\n",
      "Region 3, Query type 1, Hour 19: 42 queries\n",
      "Region 3, Query type 1, Hour 20: 44 queries\n",
      "Region 3, Query type 1, Hour 21: 34 queries\n",
      "Region 3, Query type 1, Hour 22: 32 queries\n",
      "Region 3, Query type 1, Hour 23: 34 queries\n",
      "Region 3, Query type 2, Hour 0: 34 queries\n",
      "Region 3, Query type 2, Hour 1: 34 queries\n",
      "Region 3, Query type 2, Hour 2: 32 queries\n",
      "Region 3, Query type 2, Hour 3: 32 queries\n",
      "Region 3, Query type 2, Hour 4: 49 queries\n",
      "Region 3, Query type 2, Hour 5: 47 queries\n",
      "Region 3, Query type 2, Hour 6: 49 queries\n",
      "Region 3, Query type 2, Hour 7: 33 queries\n",
      "Region 3, Query type 2, Hour 8: 30 queries\n",
      "Region 3, Query type 2, Hour 9: 45 queries\n",
      "Region 3, Query type 2, Hour 10: 37 queries\n",
      "Region 3, Query type 2, Hour 11: 46 queries\n",
      "Region 3, Query type 2, Hour 12: 37 queries\n",
      "Region 3, Query type 2, Hour 13: 36 queries\n",
      "Region 3, Query type 2, Hour 14: 37 queries\n",
      "Region 3, Query type 2, Hour 15: 50 queries\n",
      "Region 3, Query type 2, Hour 16: 41 queries\n",
      "Region 3, Query type 2, Hour 17: 45 queries\n",
      "Region 3, Query type 2, Hour 18: 41 queries\n",
      "Region 3, Query type 2, Hour 19: 48 queries\n",
      "Region 3, Query type 2, Hour 20: 39 queries\n",
      "Region 3, Query type 2, Hour 21: 36 queries\n",
      "Region 3, Query type 2, Hour 22: 41 queries\n",
      "Region 3, Query type 2, Hour 23: 49 queries\n",
      "Region 3, Query type 3, Hour 0: 34 queries\n",
      "Region 3, Query type 3, Hour 1: 45 queries\n",
      "Region 3, Query type 3, Hour 2: 38 queries\n",
      "Region 3, Query type 3, Hour 3: 36 queries\n",
      "Region 3, Query type 3, Hour 4: 49 queries\n",
      "Region 3, Query type 3, Hour 5: 43 queries\n",
      "Region 3, Query type 3, Hour 6: 40 queries\n",
      "Region 3, Query type 3, Hour 7: 36 queries\n",
      "Region 3, Query type 3, Hour 8: 39 queries\n",
      "Region 3, Query type 3, Hour 9: 38 queries\n",
      "Region 3, Query type 3, Hour 10: 33 queries\n",
      "Region 3, Query type 3, Hour 11: 45 queries\n",
      "Region 3, Query type 3, Hour 12: 39 queries\n",
      "Region 3, Query type 3, Hour 13: 48 queries\n",
      "Region 3, Query type 3, Hour 14: 46 queries\n",
      "Region 3, Query type 3, Hour 15: 38 queries\n",
      "Region 3, Query type 3, Hour 16: 42 queries\n",
      "Region 3, Query type 3, Hour 17: 30 queries\n",
      "Region 3, Query type 3, Hour 18: 31 queries\n",
      "Region 3, Query type 3, Hour 19: 42 queries\n",
      "Region 3, Query type 3, Hour 20: 38 queries\n",
      "Region 3, Query type 3, Hour 21: 36 queries\n",
      "Region 3, Query type 3, Hour 22: 45 queries\n",
      "Region 3, Query type 3, Hour 23: 48 queries\n",
      "Region 3, Query type 4, Hour 0: 36 queries\n",
      "Region 3, Query type 4, Hour 1: 35 queries\n",
      "Region 3, Query type 4, Hour 2: 47 queries\n",
      "Region 3, Query type 4, Hour 3: 48 queries\n",
      "Region 3, Query type 4, Hour 4: 40 queries\n",
      "Region 3, Query type 4, Hour 5: 41 queries\n",
      "Region 3, Query type 4, Hour 6: 39 queries\n",
      "Region 3, Query type 4, Hour 7: 42 queries\n",
      "Region 3, Query type 4, Hour 8: 41 queries\n",
      "Region 3, Query type 4, Hour 9: 50 queries\n",
      "Region 3, Query type 4, Hour 10: 39 queries\n",
      "Region 3, Query type 4, Hour 11: 49 queries\n",
      "Region 3, Query type 4, Hour 12: 47 queries\n",
      "Region 3, Query type 4, Hour 13: 33 queries\n",
      "Region 3, Query type 4, Hour 14: 34 queries\n",
      "Region 3, Query type 4, Hour 15: 34 queries\n",
      "Region 3, Query type 4, Hour 16: 41 queries\n",
      "Region 3, Query type 4, Hour 17: 32 queries\n",
      "Region 3, Query type 4, Hour 18: 37 queries\n",
      "Region 3, Query type 4, Hour 19: 31 queries\n",
      "Region 3, Query type 4, Hour 20: 47 queries\n",
      "Region 3, Query type 4, Hour 21: 38 queries\n",
      "Region 3, Query type 4, Hour 22: 42 queries\n",
      "Region 3, Query type 4, Hour 23: 36 queries\n",
      "Region 4, Query type 0, Hour 0: 39 queries\n",
      "Region 4, Query type 0, Hour 1: 38 queries\n",
      "Region 4, Query type 0, Hour 2: 44 queries\n",
      "Region 4, Query type 0, Hour 3: 34 queries\n",
      "Region 4, Query type 0, Hour 4: 41 queries\n",
      "Region 4, Query type 0, Hour 5: 30 queries\n",
      "Region 4, Query type 0, Hour 6: 41 queries\n",
      "Region 4, Query type 0, Hour 7: 35 queries\n",
      "Region 4, Query type 0, Hour 8: 48 queries\n",
      "Region 4, Query type 0, Hour 9: 48 queries\n",
      "Region 4, Query type 0, Hour 10: 45 queries\n",
      "Region 4, Query type 0, Hour 11: 48 queries\n",
      "Region 4, Query type 0, Hour 12: 32 queries\n",
      "Region 4, Query type 0, Hour 13: 39 queries\n",
      "Region 4, Query type 0, Hour 14: 47 queries\n",
      "Region 4, Query type 0, Hour 15: 42 queries\n",
      "Region 4, Query type 0, Hour 16: 47 queries\n",
      "Region 4, Query type 0, Hour 17: 32 queries\n",
      "Region 4, Query type 0, Hour 18: 32 queries\n",
      "Region 4, Query type 0, Hour 19: 41 queries\n",
      "Region 4, Query type 0, Hour 20: 49 queries\n",
      "Region 4, Query type 0, Hour 21: 36 queries\n",
      "Region 4, Query type 0, Hour 22: 37 queries\n",
      "Region 4, Query type 0, Hour 23: 38 queries\n",
      "Region 4, Query type 1, Hour 0: 43 queries\n",
      "Region 4, Query type 1, Hour 1: 34 queries\n",
      "Region 4, Query type 1, Hour 2: 50 queries\n",
      "Region 4, Query type 1, Hour 3: 48 queries\n",
      "Region 4, Query type 1, Hour 4: 50 queries\n",
      "Region 4, Query type 1, Hour 5: 45 queries\n",
      "Region 4, Query type 1, Hour 6: 41 queries\n",
      "Region 4, Query type 1, Hour 7: 50 queries\n",
      "Region 4, Query type 1, Hour 8: 33 queries\n",
      "Region 4, Query type 1, Hour 9: 37 queries\n",
      "Region 4, Query type 1, Hour 10: 36 queries\n",
      "Region 4, Query type 1, Hour 11: 44 queries\n",
      "Region 4, Query type 1, Hour 12: 35 queries\n",
      "Region 4, Query type 1, Hour 13: 34 queries\n",
      "Region 4, Query type 1, Hour 14: 43 queries\n",
      "Region 4, Query type 1, Hour 15: 50 queries\n",
      "Region 4, Query type 1, Hour 16: 42 queries\n",
      "Region 4, Query type 1, Hour 17: 39 queries\n",
      "Region 4, Query type 1, Hour 18: 47 queries\n",
      "Region 4, Query type 1, Hour 19: 33 queries\n",
      "Region 4, Query type 1, Hour 20: 37 queries\n",
      "Region 4, Query type 1, Hour 21: 40 queries\n",
      "Region 4, Query type 1, Hour 22: 46 queries\n",
      "Region 4, Query type 1, Hour 23: 47 queries\n",
      "Region 4, Query type 2, Hour 0: 50 queries\n",
      "Region 4, Query type 2, Hour 1: 40 queries\n",
      "Region 4, Query type 2, Hour 2: 45 queries\n",
      "Region 4, Query type 2, Hour 3: 41 queries\n",
      "Region 4, Query type 2, Hour 4: 42 queries\n",
      "Region 4, Query type 2, Hour 5: 45 queries\n",
      "Region 4, Query type 2, Hour 6: 50 queries\n",
      "Region 4, Query type 2, Hour 7: 30 queries\n",
      "Region 4, Query type 2, Hour 8: 45 queries\n",
      "Region 4, Query type 2, Hour 9: 44 queries\n",
      "Region 4, Query type 2, Hour 10: 35 queries\n",
      "Region 4, Query type 2, Hour 11: 41 queries\n",
      "Region 4, Query type 2, Hour 12: 37 queries\n",
      "Region 4, Query type 2, Hour 13: 36 queries\n",
      "Region 4, Query type 2, Hour 14: 31 queries\n",
      "Region 4, Query type 2, Hour 15: 41 queries\n",
      "Region 4, Query type 2, Hour 16: 50 queries\n",
      "Region 4, Query type 2, Hour 17: 43 queries\n",
      "Region 4, Query type 2, Hour 18: 45 queries\n",
      "Region 4, Query type 2, Hour 19: 40 queries\n",
      "Region 4, Query type 2, Hour 20: 49 queries\n",
      "Region 4, Query type 2, Hour 21: 38 queries\n",
      "Region 4, Query type 2, Hour 22: 39 queries\n",
      "Region 4, Query type 2, Hour 23: 46 queries\n",
      "Region 4, Query type 3, Hour 0: 41 queries\n",
      "Region 4, Query type 3, Hour 1: 35 queries\n",
      "Region 4, Query type 3, Hour 2: 48 queries\n",
      "Region 4, Query type 3, Hour 3: 40 queries\n",
      "Region 4, Query type 3, Hour 4: 38 queries\n",
      "Region 4, Query type 3, Hour 5: 50 queries\n",
      "Region 4, Query type 3, Hour 6: 35 queries\n",
      "Region 4, Query type 3, Hour 7: 45 queries\n",
      "Region 4, Query type 3, Hour 8: 49 queries\n",
      "Region 4, Query type 3, Hour 9: 45 queries\n",
      "Region 4, Query type 3, Hour 10: 35 queries\n",
      "Region 4, Query type 3, Hour 11: 34 queries\n",
      "Region 4, Query type 3, Hour 12: 50 queries\n",
      "Region 4, Query type 3, Hour 13: 33 queries\n",
      "Region 4, Query type 3, Hour 14: 31 queries\n",
      "Region 4, Query type 3, Hour 15: 35 queries\n",
      "Region 4, Query type 3, Hour 16: 38 queries\n",
      "Region 4, Query type 3, Hour 17: 46 queries\n",
      "Region 4, Query type 3, Hour 18: 43 queries\n",
      "Region 4, Query type 3, Hour 19: 36 queries\n",
      "Region 4, Query type 3, Hour 20: 48 queries\n",
      "Region 4, Query type 3, Hour 21: 30 queries\n",
      "Region 4, Query type 3, Hour 22: 49 queries\n",
      "Region 4, Query type 3, Hour 23: 44 queries\n",
      "Region 4, Query type 4, Hour 0: 47 queries\n",
      "Region 4, Query type 4, Hour 1: 32 queries\n",
      "Region 4, Query type 4, Hour 2: 47 queries\n",
      "Region 4, Query type 4, Hour 3: 31 queries\n",
      "Region 4, Query type 4, Hour 4: 39 queries\n",
      "Region 4, Query type 4, Hour 5: 40 queries\n",
      "Region 4, Query type 4, Hour 6: 32 queries\n",
      "Region 4, Query type 4, Hour 7: 36 queries\n",
      "Region 4, Query type 4, Hour 8: 37 queries\n",
      "Region 4, Query type 4, Hour 9: 48 queries\n",
      "Region 4, Query type 4, Hour 10: 47 queries\n",
      "Region 4, Query type 4, Hour 11: 42 queries\n",
      "Region 4, Query type 4, Hour 12: 42 queries\n",
      "Region 4, Query type 4, Hour 13: 48 queries\n",
      "Region 4, Query type 4, Hour 14: 41 queries\n",
      "Region 4, Query type 4, Hour 15: 46 queries\n",
      "Region 4, Query type 4, Hour 16: 41 queries\n",
      "Region 4, Query type 4, Hour 17: 42 queries\n",
      "Region 4, Query type 4, Hour 18: 41 queries\n",
      "Region 4, Query type 4, Hour 19: 48 queries\n",
      "Region 4, Query type 4, Hour 20: 43 queries\n",
      "Region 4, Query type 4, Hour 21: 36 queries\n",
      "Region 4, Query type 4, Hour 22: 42 queries\n",
      "Region 4, Query type 4, Hour 23: 35 queries\n",
      "Region 5, Query type 0, Hour 0: 48 queries\n",
      "Region 5, Query type 0, Hour 1: 48 queries\n",
      "Region 5, Query type 0, Hour 2: 33 queries\n",
      "Region 5, Query type 0, Hour 3: 48 queries\n",
      "Region 5, Query type 0, Hour 4: 38 queries\n",
      "Region 5, Query type 0, Hour 5: 45 queries\n",
      "Region 5, Query type 0, Hour 6: 33 queries\n",
      "Region 5, Query type 0, Hour 7: 35 queries\n",
      "Region 5, Query type 0, Hour 8: 37 queries\n",
      "Region 5, Query type 0, Hour 9: 45 queries\n",
      "Region 5, Query type 0, Hour 10: 34 queries\n",
      "Region 5, Query type 0, Hour 11: 50 queries\n",
      "Region 5, Query type 0, Hour 12: 39 queries\n",
      "Region 5, Query type 0, Hour 13: 42 queries\n",
      "Region 5, Query type 0, Hour 14: 43 queries\n",
      "Region 5, Query type 0, Hour 15: 36 queries\n",
      "Region 5, Query type 0, Hour 16: 49 queries\n",
      "Region 5, Query type 0, Hour 17: 35 queries\n",
      "Region 5, Query type 0, Hour 18: 36 queries\n",
      "Region 5, Query type 0, Hour 19: 34 queries\n",
      "Region 5, Query type 0, Hour 20: 50 queries\n",
      "Region 5, Query type 0, Hour 21: 47 queries\n",
      "Region 5, Query type 0, Hour 22: 41 queries\n",
      "Region 5, Query type 0, Hour 23: 35 queries\n",
      "Region 5, Query type 1, Hour 0: 30 queries\n",
      "Region 5, Query type 1, Hour 1: 33 queries\n",
      "Region 5, Query type 1, Hour 2: 48 queries\n",
      "Region 5, Query type 1, Hour 3: 41 queries\n",
      "Region 5, Query type 1, Hour 4: 44 queries\n",
      "Region 5, Query type 1, Hour 5: 32 queries\n",
      "Region 5, Query type 1, Hour 6: 49 queries\n",
      "Region 5, Query type 1, Hour 7: 37 queries\n",
      "Region 5, Query type 1, Hour 8: 40 queries\n",
      "Region 5, Query type 1, Hour 9: 45 queries\n",
      "Region 5, Query type 1, Hour 10: 38 queries\n",
      "Region 5, Query type 1, Hour 11: 32 queries\n",
      "Region 5, Query type 1, Hour 12: 34 queries\n",
      "Region 5, Query type 1, Hour 13: 49 queries\n",
      "Region 5, Query type 1, Hour 14: 43 queries\n",
      "Region 5, Query type 1, Hour 15: 44 queries\n",
      "Region 5, Query type 1, Hour 16: 35 queries\n",
      "Region 5, Query type 1, Hour 17: 33 queries\n",
      "Region 5, Query type 1, Hour 18: 37 queries\n",
      "Region 5, Query type 1, Hour 19: 48 queries\n",
      "Region 5, Query type 1, Hour 20: 34 queries\n",
      "Region 5, Query type 1, Hour 21: 42 queries\n",
      "Region 5, Query type 1, Hour 22: 45 queries\n",
      "Region 5, Query type 1, Hour 23: 30 queries\n",
      "Region 5, Query type 2, Hour 0: 42 queries\n",
      "Region 5, Query type 2, Hour 1: 40 queries\n",
      "Region 5, Query type 2, Hour 2: 50 queries\n",
      "Region 5, Query type 2, Hour 3: 37 queries\n",
      "Region 5, Query type 2, Hour 4: 37 queries\n",
      "Region 5, Query type 2, Hour 5: 43 queries\n",
      "Region 5, Query type 2, Hour 6: 42 queries\n",
      "Region 5, Query type 2, Hour 7: 32 queries\n",
      "Region 5, Query type 2, Hour 8: 33 queries\n",
      "Region 5, Query type 2, Hour 9: 49 queries\n",
      "Region 5, Query type 2, Hour 10: 44 queries\n",
      "Region 5, Query type 2, Hour 11: 44 queries\n",
      "Region 5, Query type 2, Hour 12: 50 queries\n",
      "Region 5, Query type 2, Hour 13: 42 queries\n",
      "Region 5, Query type 2, Hour 14: 38 queries\n",
      "Region 5, Query type 2, Hour 15: 39 queries\n",
      "Region 5, Query type 2, Hour 16: 41 queries\n",
      "Region 5, Query type 2, Hour 17: 43 queries\n",
      "Region 5, Query type 2, Hour 18: 30 queries\n",
      "Region 5, Query type 2, Hour 19: 43 queries\n",
      "Region 5, Query type 2, Hour 20: 43 queries\n",
      "Region 5, Query type 2, Hour 21: 37 queries\n",
      "Region 5, Query type 2, Hour 22: 45 queries\n",
      "Region 5, Query type 2, Hour 23: 43 queries\n",
      "Region 5, Query type 3, Hour 0: 30 queries\n",
      "Region 5, Query type 3, Hour 1: 45 queries\n",
      "Region 5, Query type 3, Hour 2: 47 queries\n",
      "Region 5, Query type 3, Hour 3: 36 queries\n",
      "Region 5, Query type 3, Hour 4: 44 queries\n",
      "Region 5, Query type 3, Hour 5: 46 queries\n",
      "Region 5, Query type 3, Hour 6: 48 queries\n",
      "Region 5, Query type 3, Hour 7: 42 queries\n",
      "Region 5, Query type 3, Hour 8: 38 queries\n",
      "Region 5, Query type 3, Hour 9: 44 queries\n",
      "Region 5, Query type 3, Hour 10: 32 queries\n",
      "Region 5, Query type 3, Hour 11: 30 queries\n",
      "Region 5, Query type 3, Hour 12: 45 queries\n",
      "Region 5, Query type 3, Hour 13: 37 queries\n",
      "Region 5, Query type 3, Hour 14: 42 queries\n",
      "Region 5, Query type 3, Hour 15: 37 queries\n",
      "Region 5, Query type 3, Hour 16: 46 queries\n",
      "Region 5, Query type 3, Hour 17: 43 queries\n",
      "Region 5, Query type 3, Hour 18: 50 queries\n",
      "Region 5, Query type 3, Hour 19: 50 queries\n",
      "Region 5, Query type 3, Hour 20: 36 queries\n",
      "Region 5, Query type 3, Hour 21: 44 queries\n",
      "Region 5, Query type 3, Hour 22: 45 queries\n",
      "Region 5, Query type 3, Hour 23: 48 queries\n",
      "Region 5, Query type 4, Hour 0: 35 queries\n",
      "Region 5, Query type 4, Hour 1: 30 queries\n",
      "Region 5, Query type 4, Hour 2: 41 queries\n",
      "Region 5, Query type 4, Hour 3: 47 queries\n",
      "Region 5, Query type 4, Hour 4: 45 queries\n",
      "Region 5, Query type 4, Hour 5: 41 queries\n",
      "Region 5, Query type 4, Hour 6: 49 queries\n",
      "Region 5, Query type 4, Hour 7: 42 queries\n",
      "Region 5, Query type 4, Hour 8: 37 queries\n",
      "Region 5, Query type 4, Hour 9: 30 queries\n",
      "Region 5, Query type 4, Hour 10: 50 queries\n",
      "Region 5, Query type 4, Hour 11: 36 queries\n",
      "Region 5, Query type 4, Hour 12: 44 queries\n",
      "Region 5, Query type 4, Hour 13: 42 queries\n",
      "Region 5, Query type 4, Hour 14: 42 queries\n",
      "Region 5, Query type 4, Hour 15: 38 queries\n",
      "Region 5, Query type 4, Hour 16: 44 queries\n",
      "Region 5, Query type 4, Hour 17: 33 queries\n",
      "Region 5, Query type 4, Hour 18: 42 queries\n",
      "Region 5, Query type 4, Hour 19: 41 queries\n",
      "Region 5, Query type 4, Hour 20: 34 queries\n",
      "Region 5, Query type 4, Hour 21: 36 queries\n",
      "Region 5, Query type 4, Hour 22: 37 queries\n",
      "Region 5, Query type 4, Hour 23: 34 queries\n",
      "Region 6, Query type 0, Hour 0: 38 queries\n",
      "Region 6, Query type 0, Hour 1: 43 queries\n",
      "Region 6, Query type 0, Hour 2: 41 queries\n",
      "Region 6, Query type 0, Hour 3: 33 queries\n",
      "Region 6, Query type 0, Hour 4: 49 queries\n",
      "Region 6, Query type 0, Hour 5: 37 queries\n",
      "Region 6, Query type 0, Hour 6: 46 queries\n",
      "Region 6, Query type 0, Hour 7: 37 queries\n",
      "Region 6, Query type 0, Hour 8: 50 queries\n",
      "Region 6, Query type 0, Hour 9: 30 queries\n",
      "Region 6, Query type 0, Hour 10: 49 queries\n",
      "Region 6, Query type 0, Hour 11: 31 queries\n",
      "Region 6, Query type 0, Hour 12: 33 queries\n",
      "Region 6, Query type 0, Hour 13: 50 queries\n",
      "Region 6, Query type 0, Hour 14: 30 queries\n",
      "Region 6, Query type 0, Hour 15: 36 queries\n",
      "Region 6, Query type 0, Hour 16: 30 queries\n",
      "Region 6, Query type 0, Hour 17: 38 queries\n",
      "Region 6, Query type 0, Hour 18: 32 queries\n",
      "Region 6, Query type 0, Hour 19: 41 queries\n",
      "Region 6, Query type 0, Hour 20: 48 queries\n",
      "Region 6, Query type 0, Hour 21: 44 queries\n",
      "Region 6, Query type 0, Hour 22: 45 queries\n",
      "Region 6, Query type 0, Hour 23: 39 queries\n",
      "Region 6, Query type 1, Hour 0: 35 queries\n",
      "Region 6, Query type 1, Hour 1: 46 queries\n",
      "Region 6, Query type 1, Hour 2: 50 queries\n",
      "Region 6, Query type 1, Hour 3: 46 queries\n",
      "Region 6, Query type 1, Hour 4: 50 queries\n",
      "Region 6, Query type 1, Hour 5: 48 queries\n",
      "Region 6, Query type 1, Hour 6: 47 queries\n",
      "Region 6, Query type 1, Hour 7: 40 queries\n",
      "Region 6, Query type 1, Hour 8: 36 queries\n",
      "Region 6, Query type 1, Hour 9: 39 queries\n",
      "Region 6, Query type 1, Hour 10: 36 queries\n",
      "Region 6, Query type 1, Hour 11: 34 queries\n",
      "Region 6, Query type 1, Hour 12: 41 queries\n",
      "Region 6, Query type 1, Hour 13: 45 queries\n",
      "Region 6, Query type 1, Hour 14: 50 queries\n",
      "Region 6, Query type 1, Hour 15: 47 queries\n",
      "Region 6, Query type 1, Hour 16: 32 queries\n",
      "Region 6, Query type 1, Hour 17: 49 queries\n",
      "Region 6, Query type 1, Hour 18: 44 queries\n",
      "Region 6, Query type 1, Hour 19: 34 queries\n",
      "Region 6, Query type 1, Hour 20: 42 queries\n",
      "Region 6, Query type 1, Hour 21: 48 queries\n",
      "Region 6, Query type 1, Hour 22: 41 queries\n",
      "Region 6, Query type 1, Hour 23: 50 queries\n",
      "Region 6, Query type 2, Hour 0: 34 queries\n",
      "Region 6, Query type 2, Hour 1: 35 queries\n",
      "Region 6, Query type 2, Hour 2: 36 queries\n",
      "Region 6, Query type 2, Hour 3: 39 queries\n",
      "Region 6, Query type 2, Hour 4: 45 queries\n",
      "Region 6, Query type 2, Hour 5: 37 queries\n",
      "Region 6, Query type 2, Hour 6: 38 queries\n",
      "Region 6, Query type 2, Hour 7: 33 queries\n",
      "Region 6, Query type 2, Hour 8: 50 queries\n",
      "Region 6, Query type 2, Hour 9: 50 queries\n",
      "Region 6, Query type 2, Hour 10: 37 queries\n",
      "Region 6, Query type 2, Hour 11: 36 queries\n",
      "Region 6, Query type 2, Hour 12: 44 queries\n",
      "Region 6, Query type 2, Hour 13: 50 queries\n",
      "Region 6, Query type 2, Hour 14: 32 queries\n",
      "Region 6, Query type 2, Hour 15: 46 queries\n",
      "Region 6, Query type 2, Hour 16: 40 queries\n",
      "Region 6, Query type 2, Hour 17: 38 queries\n",
      "Region 6, Query type 2, Hour 18: 34 queries\n",
      "Region 6, Query type 2, Hour 19: 39 queries\n",
      "Region 6, Query type 2, Hour 20: 44 queries\n",
      "Region 6, Query type 2, Hour 21: 43 queries\n",
      "Region 6, Query type 2, Hour 22: 33 queries\n",
      "Region 6, Query type 2, Hour 23: 50 queries\n",
      "Region 6, Query type 3, Hour 0: 49 queries\n",
      "Region 6, Query type 3, Hour 1: 31 queries\n",
      "Region 6, Query type 3, Hour 2: 41 queries\n",
      "Region 6, Query type 3, Hour 3: 30 queries\n",
      "Region 6, Query type 3, Hour 4: 49 queries\n",
      "Region 6, Query type 3, Hour 5: 34 queries\n",
      "Region 6, Query type 3, Hour 6: 35 queries\n",
      "Region 6, Query type 3, Hour 7: 40 queries\n",
      "Region 6, Query type 3, Hour 8: 37 queries\n",
      "Region 6, Query type 3, Hour 9: 36 queries\n",
      "Region 6, Query type 3, Hour 10: 33 queries\n",
      "Region 6, Query type 3, Hour 11: 35 queries\n",
      "Region 6, Query type 3, Hour 12: 45 queries\n",
      "Region 6, Query type 3, Hour 13: 49 queries\n",
      "Region 6, Query type 3, Hour 14: 44 queries\n",
      "Region 6, Query type 3, Hour 15: 43 queries\n",
      "Region 6, Query type 3, Hour 16: 41 queries\n",
      "Region 6, Query type 3, Hour 17: 41 queries\n",
      "Region 6, Query type 3, Hour 18: 45 queries\n",
      "Region 6, Query type 3, Hour 19: 44 queries\n",
      "Region 6, Query type 3, Hour 20: 42 queries\n",
      "Region 6, Query type 3, Hour 21: 40 queries\n",
      "Region 6, Query type 3, Hour 22: 46 queries\n",
      "Region 6, Query type 3, Hour 23: 50 queries\n",
      "Region 6, Query type 4, Hour 0: 44 queries\n",
      "Region 6, Query type 4, Hour 1: 43 queries\n",
      "Region 6, Query type 4, Hour 2: 39 queries\n",
      "Region 6, Query type 4, Hour 3: 41 queries\n",
      "Region 6, Query type 4, Hour 4: 45 queries\n",
      "Region 6, Query type 4, Hour 5: 44 queries\n",
      "Region 6, Query type 4, Hour 6: 39 queries\n",
      "Region 6, Query type 4, Hour 7: 33 queries\n",
      "Region 6, Query type 4, Hour 8: 38 queries\n",
      "Region 6, Query type 4, Hour 9: 50 queries\n",
      "Region 6, Query type 4, Hour 10: 49 queries\n",
      "Region 6, Query type 4, Hour 11: 38 queries\n",
      "Region 6, Query type 4, Hour 12: 50 queries\n",
      "Region 6, Query type 4, Hour 13: 49 queries\n",
      "Region 6, Query type 4, Hour 14: 40 queries\n",
      "Region 6, Query type 4, Hour 15: 40 queries\n",
      "Region 6, Query type 4, Hour 16: 36 queries\n",
      "Region 6, Query type 4, Hour 17: 50 queries\n",
      "Region 6, Query type 4, Hour 18: 39 queries\n",
      "Region 6, Query type 4, Hour 19: 33 queries\n",
      "Region 6, Query type 4, Hour 20: 34 queries\n",
      "Region 6, Query type 4, Hour 21: 34 queries\n",
      "Region 6, Query type 4, Hour 22: 40 queries\n",
      "Region 6, Query type 4, Hour 23: 38 queries\n",
      "Region 7, Query type 0, Hour 0: 45 queries\n",
      "Region 7, Query type 0, Hour 1: 38 queries\n",
      "Region 7, Query type 0, Hour 2: 33 queries\n",
      "Region 7, Query type 0, Hour 3: 31 queries\n",
      "Region 7, Query type 0, Hour 4: 43 queries\n",
      "Region 7, Query type 0, Hour 5: 32 queries\n",
      "Region 7, Query type 0, Hour 6: 47 queries\n",
      "Region 7, Query type 0, Hour 7: 50 queries\n",
      "Region 7, Query type 0, Hour 8: 35 queries\n",
      "Region 7, Query type 0, Hour 9: 40 queries\n",
      "Region 7, Query type 0, Hour 10: 50 queries\n",
      "Region 7, Query type 0, Hour 11: 47 queries\n",
      "Region 7, Query type 0, Hour 12: 41 queries\n",
      "Region 7, Query type 0, Hour 13: 34 queries\n",
      "Region 7, Query type 0, Hour 14: 38 queries\n",
      "Region 7, Query type 0, Hour 15: 49 queries\n",
      "Region 7, Query type 0, Hour 16: 47 queries\n",
      "Region 7, Query type 0, Hour 17: 48 queries\n",
      "Region 7, Query type 0, Hour 18: 32 queries\n",
      "Region 7, Query type 0, Hour 19: 38 queries\n",
      "Region 7, Query type 0, Hour 20: 33 queries\n",
      "Region 7, Query type 0, Hour 21: 46 queries\n",
      "Region 7, Query type 0, Hour 22: 46 queries\n",
      "Region 7, Query type 0, Hour 23: 31 queries\n",
      "Region 7, Query type 1, Hour 0: 38 queries\n",
      "Region 7, Query type 1, Hour 1: 38 queries\n",
      "Region 7, Query type 1, Hour 2: 34 queries\n",
      "Region 7, Query type 1, Hour 3: 44 queries\n",
      "Region 7, Query type 1, Hour 4: 44 queries\n",
      "Region 7, Query type 1, Hour 5: 32 queries\n",
      "Region 7, Query type 1, Hour 6: 48 queries\n",
      "Region 7, Query type 1, Hour 7: 32 queries\n",
      "Region 7, Query type 1, Hour 8: 35 queries\n",
      "Region 7, Query type 1, Hour 9: 33 queries\n",
      "Region 7, Query type 1, Hour 10: 42 queries\n",
      "Region 7, Query type 1, Hour 11: 42 queries\n",
      "Region 7, Query type 1, Hour 12: 40 queries\n",
      "Region 7, Query type 1, Hour 13: 34 queries\n",
      "Region 7, Query type 1, Hour 14: 30 queries\n",
      "Region 7, Query type 1, Hour 15: 38 queries\n",
      "Region 7, Query type 1, Hour 16: 46 queries\n",
      "Region 7, Query type 1, Hour 17: 30 queries\n",
      "Region 7, Query type 1, Hour 18: 32 queries\n",
      "Region 7, Query type 1, Hour 19: 44 queries\n",
      "Region 7, Query type 1, Hour 20: 50 queries\n",
      "Region 7, Query type 1, Hour 21: 39 queries\n",
      "Region 7, Query type 1, Hour 22: 37 queries\n",
      "Region 7, Query type 1, Hour 23: 41 queries\n",
      "Region 7, Query type 2, Hour 0: 44 queries\n",
      "Region 7, Query type 2, Hour 1: 38 queries\n",
      "Region 7, Query type 2, Hour 2: 39 queries\n",
      "Region 7, Query type 2, Hour 3: 49 queries\n",
      "Region 7, Query type 2, Hour 4: 38 queries\n",
      "Region 7, Query type 2, Hour 5: 44 queries\n",
      "Region 7, Query type 2, Hour 6: 50 queries\n",
      "Region 7, Query type 2, Hour 7: 50 queries\n",
      "Region 7, Query type 2, Hour 8: 30 queries\n",
      "Region 7, Query type 2, Hour 9: 31 queries\n",
      "Region 7, Query type 2, Hour 10: 48 queries\n",
      "Region 7, Query type 2, Hour 11: 43 queries\n",
      "Region 7, Query type 2, Hour 12: 37 queries\n",
      "Region 7, Query type 2, Hour 13: 35 queries\n",
      "Region 7, Query type 2, Hour 14: 41 queries\n",
      "Region 7, Query type 2, Hour 15: 44 queries\n",
      "Region 7, Query type 2, Hour 16: 43 queries\n",
      "Region 7, Query type 2, Hour 17: 50 queries\n",
      "Region 7, Query type 2, Hour 18: 30 queries\n",
      "Region 7, Query type 2, Hour 19: 40 queries\n",
      "Region 7, Query type 2, Hour 20: 32 queries\n",
      "Region 7, Query type 2, Hour 21: 48 queries\n",
      "Region 7, Query type 2, Hour 22: 47 queries\n",
      "Region 7, Query type 2, Hour 23: 49 queries\n",
      "Region 7, Query type 3, Hour 0: 31 queries\n",
      "Region 7, Query type 3, Hour 1: 36 queries\n",
      "Region 7, Query type 3, Hour 2: 37 queries\n",
      "Region 7, Query type 3, Hour 3: 48 queries\n",
      "Region 7, Query type 3, Hour 4: 44 queries\n",
      "Region 7, Query type 3, Hour 5: 39 queries\n",
      "Region 7, Query type 3, Hour 6: 30 queries\n",
      "Region 7, Query type 3, Hour 7: 35 queries\n",
      "Region 7, Query type 3, Hour 8: 42 queries\n",
      "Region 7, Query type 3, Hour 9: 43 queries\n",
      "Region 7, Query type 3, Hour 10: 32 queries\n",
      "Region 7, Query type 3, Hour 11: 40 queries\n",
      "Region 7, Query type 3, Hour 12: 32 queries\n",
      "Region 7, Query type 3, Hour 13: 48 queries\n",
      "Region 7, Query type 3, Hour 14: 39 queries\n",
      "Region 7, Query type 3, Hour 15: 44 queries\n",
      "Region 7, Query type 3, Hour 16: 40 queries\n",
      "Region 7, Query type 3, Hour 17: 44 queries\n",
      "Region 7, Query type 3, Hour 18: 42 queries\n",
      "Region 7, Query type 3, Hour 19: 34 queries\n",
      "Region 7, Query type 3, Hour 20: 33 queries\n",
      "Region 7, Query type 3, Hour 21: 42 queries\n",
      "Region 7, Query type 3, Hour 22: 38 queries\n",
      "Region 7, Query type 3, Hour 23: 46 queries\n",
      "Region 7, Query type 4, Hour 0: 31 queries\n",
      "Region 7, Query type 4, Hour 1: 34 queries\n",
      "Region 7, Query type 4, Hour 2: 45 queries\n",
      "Region 7, Query type 4, Hour 3: 36 queries\n",
      "Region 7, Query type 4, Hour 4: 30 queries\n",
      "Region 7, Query type 4, Hour 5: 41 queries\n",
      "Region 7, Query type 4, Hour 6: 50 queries\n",
      "Region 7, Query type 4, Hour 7: 40 queries\n",
      "Region 7, Query type 4, Hour 8: 41 queries\n",
      "Region 7, Query type 4, Hour 9: 48 queries\n",
      "Region 7, Query type 4, Hour 10: 41 queries\n",
      "Region 7, Query type 4, Hour 11: 41 queries\n",
      "Region 7, Query type 4, Hour 12: 43 queries\n",
      "Region 7, Query type 4, Hour 13: 31 queries\n",
      "Region 7, Query type 4, Hour 14: 31 queries\n",
      "Region 7, Query type 4, Hour 15: 33 queries\n",
      "Region 7, Query type 4, Hour 16: 49 queries\n",
      "Region 7, Query type 4, Hour 17: 45 queries\n",
      "Region 7, Query type 4, Hour 18: 39 queries\n",
      "Region 7, Query type 4, Hour 19: 42 queries\n",
      "Region 7, Query type 4, Hour 20: 31 queries\n",
      "Region 7, Query type 4, Hour 21: 32 queries\n",
      "Region 7, Query type 4, Hour 22: 48 queries\n",
      "Region 7, Query type 4, Hour 23: 35 queries\n",
      "Region 8, Query type 0, Hour 0: 48 queries\n",
      "Region 8, Query type 0, Hour 1: 50 queries\n",
      "Region 8, Query type 0, Hour 2: 49 queries\n",
      "Region 8, Query type 0, Hour 3: 48 queries\n",
      "Region 8, Query type 0, Hour 4: 37 queries\n",
      "Region 8, Query type 0, Hour 5: 43 queries\n",
      "Region 8, Query type 0, Hour 6: 39 queries\n",
      "Region 8, Query type 0, Hour 7: 32 queries\n",
      "Region 8, Query type 0, Hour 8: 35 queries\n",
      "Region 8, Query type 0, Hour 9: 44 queries\n",
      "Region 8, Query type 0, Hour 10: 47 queries\n",
      "Region 8, Query type 0, Hour 11: 43 queries\n",
      "Region 8, Query type 0, Hour 12: 48 queries\n",
      "Region 8, Query type 0, Hour 13: 39 queries\n",
      "Region 8, Query type 0, Hour 14: 44 queries\n",
      "Region 8, Query type 0, Hour 15: 33 queries\n",
      "Region 8, Query type 0, Hour 16: 31 queries\n",
      "Region 8, Query type 0, Hour 17: 32 queries\n",
      "Region 8, Query type 0, Hour 18: 36 queries\n",
      "Region 8, Query type 0, Hour 19: 45 queries\n",
      "Region 8, Query type 0, Hour 20: 39 queries\n",
      "Region 8, Query type 0, Hour 21: 48 queries\n",
      "Region 8, Query type 0, Hour 22: 46 queries\n",
      "Region 8, Query type 0, Hour 23: 48 queries\n",
      "Region 8, Query type 1, Hour 0: 34 queries\n",
      "Region 8, Query type 1, Hour 1: 49 queries\n",
      "Region 8, Query type 1, Hour 2: 42 queries\n",
      "Region 8, Query type 1, Hour 3: 42 queries\n",
      "Region 8, Query type 1, Hour 4: 40 queries\n",
      "Region 8, Query type 1, Hour 5: 40 queries\n",
      "Region 8, Query type 1, Hour 6: 45 queries\n",
      "Region 8, Query type 1, Hour 7: 44 queries\n",
      "Region 8, Query type 1, Hour 8: 46 queries\n",
      "Region 8, Query type 1, Hour 9: 47 queries\n",
      "Region 8, Query type 1, Hour 10: 48 queries\n",
      "Region 8, Query type 1, Hour 11: 33 queries\n",
      "Region 8, Query type 1, Hour 12: 34 queries\n",
      "Region 8, Query type 1, Hour 13: 36 queries\n",
      "Region 8, Query type 1, Hour 14: 31 queries\n",
      "Region 8, Query type 1, Hour 15: 30 queries\n",
      "Region 8, Query type 1, Hour 16: 41 queries\n",
      "Region 8, Query type 1, Hour 17: 39 queries\n",
      "Region 8, Query type 1, Hour 18: 37 queries\n",
      "Region 8, Query type 1, Hour 19: 30 queries\n",
      "Region 8, Query type 1, Hour 20: 41 queries\n",
      "Region 8, Query type 1, Hour 21: 31 queries\n",
      "Region 8, Query type 1, Hour 22: 39 queries\n",
      "Region 8, Query type 1, Hour 23: 46 queries\n",
      "Region 8, Query type 2, Hour 0: 32 queries\n",
      "Region 8, Query type 2, Hour 1: 36 queries\n",
      "Region 8, Query type 2, Hour 2: 41 queries\n",
      "Region 8, Query type 2, Hour 3: 37 queries\n",
      "Region 8, Query type 2, Hour 4: 49 queries\n",
      "Region 8, Query type 2, Hour 5: 41 queries\n",
      "Region 8, Query type 2, Hour 6: 32 queries\n",
      "Region 8, Query type 2, Hour 7: 46 queries\n",
      "Region 8, Query type 2, Hour 8: 38 queries\n",
      "Region 8, Query type 2, Hour 9: 39 queries\n",
      "Region 8, Query type 2, Hour 10: 35 queries\n",
      "Region 8, Query type 2, Hour 11: 30 queries\n",
      "Region 8, Query type 2, Hour 12: 41 queries\n",
      "Region 8, Query type 2, Hour 13: 34 queries\n",
      "Region 8, Query type 2, Hour 14: 42 queries\n",
      "Region 8, Query type 2, Hour 15: 44 queries\n",
      "Region 8, Query type 2, Hour 16: 40 queries\n",
      "Region 8, Query type 2, Hour 17: 37 queries\n",
      "Region 8, Query type 2, Hour 18: 34 queries\n",
      "Region 8, Query type 2, Hour 19: 31 queries\n",
      "Region 8, Query type 2, Hour 20: 41 queries\n",
      "Region 8, Query type 2, Hour 21: 31 queries\n",
      "Region 8, Query type 2, Hour 22: 45 queries\n",
      "Region 8, Query type 2, Hour 23: 50 queries\n",
      "Region 8, Query type 3, Hour 0: 32 queries\n",
      "Region 8, Query type 3, Hour 1: 39 queries\n",
      "Region 8, Query type 3, Hour 2: 48 queries\n",
      "Region 8, Query type 3, Hour 3: 37 queries\n",
      "Region 8, Query type 3, Hour 4: 36 queries\n",
      "Region 8, Query type 3, Hour 5: 30 queries\n",
      "Region 8, Query type 3, Hour 6: 41 queries\n",
      "Region 8, Query type 3, Hour 7: 34 queries\n",
      "Region 8, Query type 3, Hour 8: 45 queries\n",
      "Region 8, Query type 3, Hour 9: 33 queries\n",
      "Region 8, Query type 3, Hour 10: 46 queries\n",
      "Region 8, Query type 3, Hour 11: 50 queries\n",
      "Region 8, Query type 3, Hour 12: 31 queries\n",
      "Region 8, Query type 3, Hour 13: 41 queries\n",
      "Region 8, Query type 3, Hour 14: 30 queries\n",
      "Region 8, Query type 3, Hour 15: 41 queries\n",
      "Region 8, Query type 3, Hour 16: 41 queries\n",
      "Region 8, Query type 3, Hour 17: 41 queries\n",
      "Region 8, Query type 3, Hour 18: 48 queries\n",
      "Region 8, Query type 3, Hour 19: 46 queries\n",
      "Region 8, Query type 3, Hour 20: 49 queries\n",
      "Region 8, Query type 3, Hour 21: 40 queries\n",
      "Region 8, Query type 3, Hour 22: 42 queries\n",
      "Region 8, Query type 3, Hour 23: 37 queries\n",
      "Region 8, Query type 4, Hour 0: 39 queries\n",
      "Region 8, Query type 4, Hour 1: 37 queries\n",
      "Region 8, Query type 4, Hour 2: 49 queries\n",
      "Region 8, Query type 4, Hour 3: 37 queries\n",
      "Region 8, Query type 4, Hour 4: 38 queries\n",
      "Region 8, Query type 4, Hour 5: 36 queries\n",
      "Region 8, Query type 4, Hour 6: 43 queries\n",
      "Region 8, Query type 4, Hour 7: 31 queries\n",
      "Region 8, Query type 4, Hour 8: 40 queries\n",
      "Region 8, Query type 4, Hour 9: 37 queries\n",
      "Region 8, Query type 4, Hour 10: 42 queries\n",
      "Region 8, Query type 4, Hour 11: 50 queries\n",
      "Region 8, Query type 4, Hour 12: 44 queries\n",
      "Region 8, Query type 4, Hour 13: 32 queries\n",
      "Region 8, Query type 4, Hour 14: 43 queries\n",
      "Region 8, Query type 4, Hour 15: 44 queries\n",
      "Region 8, Query type 4, Hour 16: 31 queries\n",
      "Region 8, Query type 4, Hour 17: 48 queries\n",
      "Region 8, Query type 4, Hour 18: 41 queries\n",
      "Region 8, Query type 4, Hour 19: 49 queries\n",
      "Region 8, Query type 4, Hour 20: 33 queries\n",
      "Region 8, Query type 4, Hour 21: 32 queries\n",
      "Region 8, Query type 4, Hour 22: 34 queries\n",
      "Region 8, Query type 4, Hour 23: 32 queries\n",
      "Region 9, Query type 0, Hour 0: 44 queries\n",
      "Region 9, Query type 0, Hour 1: 48 queries\n",
      "Region 9, Query type 0, Hour 2: 31 queries\n",
      "Region 9, Query type 0, Hour 3: 41 queries\n",
      "Region 9, Query type 0, Hour 4: 34 queries\n",
      "Region 9, Query type 0, Hour 5: 46 queries\n",
      "Region 9, Query type 0, Hour 6: 43 queries\n",
      "Region 9, Query type 0, Hour 7: 39 queries\n",
      "Region 9, Query type 0, Hour 8: 32 queries\n",
      "Region 9, Query type 0, Hour 9: 33 queries\n",
      "Region 9, Query type 0, Hour 10: 35 queries\n",
      "Region 9, Query type 0, Hour 11: 45 queries\n",
      "Region 9, Query type 0, Hour 12: 36 queries\n",
      "Region 9, Query type 0, Hour 13: 44 queries\n",
      "Region 9, Query type 0, Hour 14: 50 queries\n",
      "Region 9, Query type 0, Hour 15: 49 queries\n",
      "Region 9, Query type 0, Hour 16: 40 queries\n",
      "Region 9, Query type 0, Hour 17: 50 queries\n",
      "Region 9, Query type 0, Hour 18: 30 queries\n",
      "Region 9, Query type 0, Hour 19: 34 queries\n",
      "Region 9, Query type 0, Hour 20: 42 queries\n",
      "Region 9, Query type 0, Hour 21: 50 queries\n",
      "Region 9, Query type 0, Hour 22: 43 queries\n",
      "Region 9, Query type 0, Hour 23: 38 queries\n",
      "Region 9, Query type 1, Hour 0: 46 queries\n",
      "Region 9, Query type 1, Hour 1: 44 queries\n",
      "Region 9, Query type 1, Hour 2: 50 queries\n",
      "Region 9, Query type 1, Hour 3: 36 queries\n",
      "Region 9, Query type 1, Hour 4: 41 queries\n",
      "Region 9, Query type 1, Hour 5: 35 queries\n",
      "Region 9, Query type 1, Hour 6: 50 queries\n",
      "Region 9, Query type 1, Hour 7: 45 queries\n",
      "Region 9, Query type 1, Hour 8: 34 queries\n",
      "Region 9, Query type 1, Hour 9: 38 queries\n",
      "Region 9, Query type 1, Hour 10: 43 queries\n",
      "Region 9, Query type 1, Hour 11: 37 queries\n",
      "Region 9, Query type 1, Hour 12: 43 queries\n",
      "Region 9, Query type 1, Hour 13: 41 queries\n",
      "Region 9, Query type 1, Hour 14: 39 queries\n",
      "Region 9, Query type 1, Hour 15: 49 queries\n",
      "Region 9, Query type 1, Hour 16: 49 queries\n",
      "Region 9, Query type 1, Hour 17: 50 queries\n",
      "Region 9, Query type 1, Hour 18: 42 queries\n",
      "Region 9, Query type 1, Hour 19: 48 queries\n",
      "Region 9, Query type 1, Hour 20: 44 queries\n",
      "Region 9, Query type 1, Hour 21: 37 queries\n",
      "Region 9, Query type 1, Hour 22: 50 queries\n",
      "Region 9, Query type 1, Hour 23: 39 queries\n",
      "Region 9, Query type 2, Hour 0: 32 queries\n",
      "Region 9, Query type 2, Hour 1: 50 queries\n",
      "Region 9, Query type 2, Hour 2: 32 queries\n",
      "Region 9, Query type 2, Hour 3: 34 queries\n",
      "Region 9, Query type 2, Hour 4: 45 queries\n",
      "Region 9, Query type 2, Hour 5: 46 queries\n",
      "Region 9, Query type 2, Hour 6: 35 queries\n",
      "Region 9, Query type 2, Hour 7: 49 queries\n",
      "Region 9, Query type 2, Hour 8: 49 queries\n",
      "Region 9, Query type 2, Hour 9: 45 queries\n",
      "Region 9, Query type 2, Hour 10: 31 queries\n",
      "Region 9, Query type 2, Hour 11: 49 queries\n",
      "Region 9, Query type 2, Hour 12: 44 queries\n",
      "Region 9, Query type 2, Hour 13: 47 queries\n",
      "Region 9, Query type 2, Hour 14: 30 queries\n",
      "Region 9, Query type 2, Hour 15: 48 queries\n",
      "Region 9, Query type 2, Hour 16: 41 queries\n",
      "Region 9, Query type 2, Hour 17: 37 queries\n",
      "Region 9, Query type 2, Hour 18: 35 queries\n",
      "Region 9, Query type 2, Hour 19: 41 queries\n",
      "Region 9, Query type 2, Hour 20: 36 queries\n",
      "Region 9, Query type 2, Hour 21: 47 queries\n",
      "Region 9, Query type 2, Hour 22: 43 queries\n",
      "Region 9, Query type 2, Hour 23: 37 queries\n",
      "Region 9, Query type 3, Hour 0: 49 queries\n",
      "Region 9, Query type 3, Hour 1: 43 queries\n",
      "Region 9, Query type 3, Hour 2: 34 queries\n",
      "Region 9, Query type 3, Hour 3: 38 queries\n",
      "Region 9, Query type 3, Hour 4: 37 queries\n",
      "Region 9, Query type 3, Hour 5: 31 queries\n",
      "Region 9, Query type 3, Hour 6: 45 queries\n",
      "Region 9, Query type 3, Hour 7: 42 queries\n",
      "Region 9, Query type 3, Hour 8: 36 queries\n",
      "Region 9, Query type 3, Hour 9: 45 queries\n",
      "Region 9, Query type 3, Hour 10: 36 queries\n",
      "Region 9, Query type 3, Hour 11: 43 queries\n",
      "Region 9, Query type 3, Hour 12: 43 queries\n",
      "Region 9, Query type 3, Hour 13: 41 queries\n",
      "Region 9, Query type 3, Hour 14: 30 queries\n",
      "Region 9, Query type 3, Hour 15: 30 queries\n",
      "Region 9, Query type 3, Hour 16: 41 queries\n",
      "Region 9, Query type 3, Hour 17: 49 queries\n",
      "Region 9, Query type 3, Hour 18: 50 queries\n",
      "Region 9, Query type 3, Hour 19: 44 queries\n",
      "Region 9, Query type 3, Hour 20: 33 queries\n",
      "Region 9, Query type 3, Hour 21: 36 queries\n",
      "Region 9, Query type 3, Hour 22: 45 queries\n",
      "Region 9, Query type 3, Hour 23: 32 queries\n",
      "Region 9, Query type 4, Hour 0: 36 queries\n",
      "Region 9, Query type 4, Hour 1: 35 queries\n",
      "Region 9, Query type 4, Hour 2: 45 queries\n",
      "Region 9, Query type 4, Hour 3: 37 queries\n",
      "Region 9, Query type 4, Hour 4: 35 queries\n",
      "Region 9, Query type 4, Hour 5: 39 queries\n",
      "Region 9, Query type 4, Hour 6: 45 queries\n",
      "Region 9, Query type 4, Hour 7: 40 queries\n",
      "Region 9, Query type 4, Hour 8: 42 queries\n",
      "Region 9, Query type 4, Hour 9: 35 queries\n",
      "Region 9, Query type 4, Hour 10: 30 queries\n",
      "Region 9, Query type 4, Hour 11: 50 queries\n",
      "Region 9, Query type 4, Hour 12: 48 queries\n",
      "Region 9, Query type 4, Hour 13: 41 queries\n",
      "Region 9, Query type 4, Hour 14: 35 queries\n",
      "Region 9, Query type 4, Hour 15: 47 queries\n",
      "Region 9, Query type 4, Hour 16: 42 queries\n",
      "Region 9, Query type 4, Hour 17: 36 queries\n",
      "Region 9, Query type 4, Hour 18: 42 queries\n",
      "Region 9, Query type 4, Hour 19: 50 queries\n",
      "Region 9, Query type 4, Hour 20: 37 queries\n",
      "Region 9, Query type 4, Hour 21: 50 queries\n",
      "Region 9, Query type 4, Hour 22: 46 queries\n",
      "Region 9, Query type 4, Hour 23: 38 queries\n"
     ]
    }
   ],
   "source": [
    "# Generate temporal-spatial demand for queries\n",
    "I = list(range(10))\n",
    "J = list(range(10))   \n",
    "K = list(range(5))\n",
    "T = list(range(24))\n",
    "lmbda = {}\n",
    "for i in I:\n",
    "    for k in K:\n",
    "        for t in T:\n",
    "            # Generate demand between 30-50 queries per time slot\n",
    "            lmbda[(i, k, t)] = np.random.randint(30, 51)\n",
    "    \n",
    "params['lambda'] = lmbda\n",
    "\n",
    "# Print some sample demand values to verify\n",
    "print(\"\\nSample demand (lambda) values:\")\n",
    "for i in I:  \n",
    "    for k in K:  \n",
    "        for t in T:  # Every 8 hours\n",
    "            print(f\"Region {i}, Query type {k}, Hour {t}: {lmbda[(i, k, t)]} queries\")\n",
    "\n",
    "# Calculate total demand metrics\n",
    "total_demand = sum(lmbda.values())\n",
    "avg_demand_per_hour = total_demand / (len(I) * len(K) * len(T))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
