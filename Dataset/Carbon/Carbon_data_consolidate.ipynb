{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c260f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carbon intensity dataset consolidation\n",
    "# This script consolidates carbon intensity data from various sources into a single CSV file.\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53896b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data consolidation process...\n",
      "Processing US-MIDW-LGEE_2024_hourly.csv...\n",
      "Processing US-NE-ISNE_2024_hourly.csv...\n",
      "Processing US-NW-NEVP_2024_hourly.csv...\n",
      "Processing US-NY-NYIS_2024_hourly.csv...\n",
      "Processing US-SE-SEPA_2024_hourly.csv...\n",
      "Processing US-SW-AZPS_2024_hourly.csv...\n",
      "Processing US-NW-PGE_2024_hourly.csv...\n",
      "Processing US-CAL-CISO_2024_hourly.csv...\n",
      "Processing US-CAL-LDWP_2024_hourly.csv...\n",
      "Consolidated data saved to consolidated_carbon_intensity_march24_25.csv\n",
      "\n",
      "Summary statistics:\n",
      "Total number of rows: 432\n",
      "Unique zones: 9\n",
      "Zones included: Arizona Public Service Company, CAISO, ISO New England, Los Angeles Department of Water and Power, Louisville Gas and Electric Company and Kentucky Utilities, Nevada Power Company, New York ISO, Portland General Electric Company, Southeastern Power Administration\n",
      "Date range: 2024-03-24 00:00:00 to 2024-03-25 23:00:00\n",
      "\n",
      "Warning: Missing values detected in the consolidated data:\n",
      "Datetime (UTC)                              0\n",
      "Zone name                                   0\n",
      "Carbon intensity gCO₂eq/kWh (Life cycle)    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def consolidate_carbon_intensity_data(input_files, output_file):\n",
    "    \"\"\"\n",
    "    Consolidate carbon intensity data from multiple CSV files.\n",
    "    \n",
    "    This function:\n",
    "    1. Extracts data only for March 24-25\n",
    "    2. Keeps only life cycle carbon intensity\n",
    "    3. Preserves zone names for region identification\n",
    "    4. Sorts by datetime and zone name\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_files : list\n",
    "        List of input CSV file paths\n",
    "    output_file : str\n",
    "        Path to save the consolidated data\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The consolidated dataframe\n",
    "    \"\"\"\n",
    "    print(\"Starting data consolidation process...\")\n",
    "    \n",
    "    # Process all files and concatenate results\n",
    "    all_data = []\n",
    "    for file_name in input_files:\n",
    "        if os.path.exists(file_name):\n",
    "            df = process_file(file_name)\n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "            else:\n",
    "                print(f\"No March 24-25 data found in {file_name}\")\n",
    "        else:\n",
    "            print(f\"Warning: File {file_name} not found. Skipping...\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if all_data:\n",
    "        consolidated_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Sort by datetime and zone name\n",
    "        consolidated_df = consolidated_df.sort_values(['Datetime (UTC)', 'Zone name'])\n",
    "        \n",
    "        # Save to output file\n",
    "        consolidated_df.to_csv(output_file, index=False)\n",
    "        print(f\"Consolidated data saved to {output_file}\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary statistics:\")\n",
    "        print(f\"Total number of rows: {len(consolidated_df)}\")\n",
    "        print(f\"Unique zones: {consolidated_df['Zone name'].nunique()}\")\n",
    "        zones = consolidated_df['Zone name'].unique()\n",
    "        print(f\"Zones included: {', '.join(zones)}\")\n",
    "        print(f\"Date range: {consolidated_df['Datetime (UTC)'].min()} to {consolidated_df['Datetime (UTC)'].max()}\")\n",
    "        \n",
    "        return consolidated_df\n",
    "    else:\n",
    "        print(\"No data was processed. Check if input files exist and contain March 24-25 data.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a single CSV file to extract March 24-25 data with life cycle carbon intensity.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Processed dataframe with filtered data\n",
    "    \"\"\"\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert date string to datetime\n",
    "        df['Datetime (UTC)'] = pd.to_datetime(df['Datetime (UTC)'])\n",
    "        \n",
    "        # Filter for March 24-25 only\n",
    "        march_24_25 = df[(df['Datetime (UTC)'].dt.month == 3) & \n",
    "                         (df['Datetime (UTC)'].dt.day >= 24) & \n",
    "                         (df['Datetime (UTC)'].dt.day <= 25)]\n",
    "        \n",
    "        # Select only the columns we need\n",
    "        columns_to_keep = ['Datetime (UTC)', 'Zone name', 'Carbon intensity gCO₂eq/kWh (Life cycle)']\n",
    "        filtered_df = march_24_25[columns_to_keep]\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of input CSV files\n",
    "    input_files = [\n",
    "        \"US-MIDW-LGEE_2024_hourly.csv\",\n",
    "        \"US-NE-ISNE_2024_hourly.csv\",\n",
    "        \"US-NW-NEVP_2024_hourly.csv\",\n",
    "        \"US-NY-NYIS_2024_hourly.csv\",\n",
    "        \"US-SE-SEPA_2024_hourly.csv\",\n",
    "        \"US-SW-AZPS_2024_hourly.csv\",\n",
    "        \"US-NW-PGE_2024_hourly.csv\",\n",
    "        \"US-CAL-CISO_2024_hourly.csv\",\n",
    "        \"US-CAL-LDWP_2024_hourly.csv\"\n",
    "    ]\n",
    "    \n",
    "    # Output file name\n",
    "    output_file = \"consolidated_carbon_intensity_march24_25.csv\"\n",
    "    \n",
    "    # Process and consolidate the data\n",
    "    consolidated_df = consolidate_carbon_intensity_data(input_files, output_file)\n",
    "    \n",
    "    # Additional validation\n",
    "    if not consolidated_df.empty:\n",
    "        # Check for missing values\n",
    "        missing_values = consolidated_df.isnull().sum()\n",
    "        if missing_values.sum() > 0:\n",
    "            print(\"\\nWarning: Missing values detected in the consolidated data:\")\n",
    "            print(missing_values)\n",
    "        else:\n",
    "            print(\"\\nNo missing values detected in the consolidated data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187fb99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data consolidation process...\n",
      "Processing US-NE-ISNE_2024_hourly.csv...\n",
      "Processing US-NY-NYIS_2024_hourly.csv...\n",
      "Processing US-SW-AZPS_2024_hourly.csv...\n",
      "Processing US-CAL-CISO_2024_hourly.csv...\n",
      "Processing US-CAL-LDWP_2024_hourly.csv...\n",
      "Processing US-TEX-ERCO_2024_hourly.csv...\n",
      "Processing US-SE-SEPA_2024_hourly.csv...\n",
      "Processing US-NW-PGE_2024_hourly.csv...\n",
      "Processing US-NW-NEVP_2024_hourly.csv...\n",
      "Processing US-NW-PACE_2024_hourly.csv...\n",
      "Processing US-MIDW-MISO_2024_hourly.csv...\n",
      "Processing US-MIDW-LGEE_2024_hourly.csv...\n",
      "Consolidated data saved to consolidated_carbon_intensity_march24.csv\n",
      "\n",
      "Summary statistics:\n",
      "Total number of rows (timestamps): 24\n",
      "Number of zones (columns): 12\n",
      "Zones included: Arizona, CAISO, Electric, ISO, Los, Louisville, Midcontinent, Nevada, New, Pacificorp, Portland, Southeastern\n",
      "Date range: 2024-03-24 00:00:00 to 2024-03-24 23:00:00\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "Filled data (with neighbor averaging) saved to consolidated_carbon_intensity_march24_filled.csv\n",
      "\n",
      "No missing values detected in the consolidated data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def consolidate_carbon_intensity_data(input_files, output_file):\n",
    "    \"\"\"\n",
    "    Consolidate carbon intensity data from multiple CSV files.\n",
    "    \n",
    "    This function:\n",
    "    1. Extracts data only for March 24-25\n",
    "    2. Keeps only life cycle carbon intensity\n",
    "    3. Creates a wide format with each zone having its own column\n",
    "    4. Sorts by datetime\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_files : list\n",
    "        List of input CSV file paths\n",
    "    output_file : str\n",
    "        Path to save the consolidated data\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The consolidated dataframe\n",
    "    \"\"\"\n",
    "    print(\"Starting data consolidation process...\")\n",
    "    \n",
    "    # Process all files and concatenate results\n",
    "    all_data = []\n",
    "    for file_name in input_files:\n",
    "        if os.path.exists(file_name):\n",
    "            df = process_file(file_name)\n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "            else:\n",
    "                print(f\"No March 24-25 data found in {file_name}\")\n",
    "        else:\n",
    "            print(f\"Warning: File {file_name} not found. Skipping...\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if all_data:\n",
    "        # Concatenate all data vertically\n",
    "        long_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Pivot the data to wide format: each zone becomes a column\n",
    "        wide_df = long_df.pivot(\n",
    "            index='Datetime (UTC)', \n",
    "            columns='Zone name', \n",
    "            values='Carbon intensity gCO₂eq/kWh (Life cycle)'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Rename columns to add clarity\n",
    "        for col in wide_df.columns:\n",
    "            if col != 'Datetime (UTC)':\n",
    "                wide_df.rename(columns={col: f\"{col} (gCO₂eq/kWh)\"}, inplace=True)\n",
    "        \n",
    "        # Save to output file\n",
    "        wide_df.to_csv(output_file, index=False)\n",
    "        print(f\"Consolidated data saved to {output_file}\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nSummary statistics:\")\n",
    "        print(f\"Total number of rows (timestamps): {len(wide_df)}\")\n",
    "        \n",
    "        zone_columns = [col for col in wide_df.columns if col != 'Datetime (UTC)']\n",
    "        print(f\"Number of zones (columns): {len(zone_columns)}\")\n",
    "        print(f\"Zones included: {', '.join([col.split(' ')[0] for col in zone_columns])}\")\n",
    "        print(f\"Date range: {wide_df['Datetime (UTC)'].min()} to {wide_df['Datetime (UTC)'].max()}\")\n",
    "        \n",
    "        return wide_df\n",
    "    else:\n",
    "        print(\"No data was processed. Check if input files exist and contain March 24-25 data.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a single CSV file to extract March 24-25 data with life cycle carbon intensity.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Processed dataframe with filtered data\n",
    "    \"\"\"\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert date string to datetime\n",
    "        df['Datetime (UTC)'] = pd.to_datetime(df['Datetime (UTC)'])\n",
    "        \n",
    "        # Filter for March 24-25 only\n",
    "        march_24 = df[(df['Datetime (UTC)'].dt.month == 3) & \n",
    "                         (df['Datetime (UTC)'].dt.day == 24)]\n",
    "        \n",
    "\n",
    "        # Select only the columns we need\n",
    "        columns_to_keep = ['Datetime (UTC)', 'Zone name', 'Carbon intensity gCO₂eq/kWh (Life cycle)']\n",
    "        filtered_df = march_24[columns_to_keep]\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of input CSV files\n",
    "    input_files = [\n",
    "        \"US-NE-ISNE_2024_hourly.csv\",\n",
    "        \"US-NY-NYIS_2024_hourly.csv\",\n",
    "        \"US-SW-AZPS_2024_hourly.csv\",\n",
    "        \"US-CAL-CISO_2024_hourly.csv\",\n",
    "        \"US-CAL-LDWP_2024_hourly.csv\",\n",
    "        \"US-TEX-ERCO_2024_hourly.csv\",\n",
    "        \"US-SE-SEPA_2024_hourly.csv\",\n",
    "        \"US-NW-PGE_2024_hourly.csv\",\n",
    "        \"US-NW-NEVP_2024_hourly.csv\",\n",
    "        \"US-NW-PACE_2024_hourly.csv\",\n",
    "        \"US-MIDW-MISO_2024_hourly.csv\",\n",
    "        \"US-MIDW-LGEE_2024_hourly.csv\"\n",
    "    ]\n",
    "    \n",
    "    # Output file name\n",
    "    output_file = \"consolidated_carbon_intensity_march24.csv\"\n",
    "    \n",
    "    # Process and consolidate the data\n",
    "    wide_df = consolidate_carbon_intensity_data(input_files, output_file)\n",
    "    \n",
    "    # Additional validation\n",
    "    if not wide_df.empty:\n",
    "        # Check for missing values\n",
    "        missing_values = wide_df.isnull().sum()\n",
    "        if missing_values.sum() > 0:\n",
    "            print(\"\\nWarning: Missing values detected in the consolidated data:\")\n",
    "            print(missing_values)\n",
    "            \n",
    "        # Optional: Fill missing values (e.g., with linear interpolation)\n",
    "        filled_df = wide_df.copy()\n",
    "        for col in wide_df.columns:\n",
    "            if col != 'Datetime (UTC)':\n",
    "                # Identify missing values\n",
    "                missing_mask = wide_df[col].isna()\n",
    "                \n",
    "                if missing_mask.any():\n",
    "                    # Create a series to store filled values\n",
    "                    filled_values = wide_df[col].copy()\n",
    "                    \n",
    "                    # For each missing value, compute average of neighboring values\n",
    "                    for idx in missing_mask[missing_mask].index:\n",
    "                        # Find position in the dataframe\n",
    "                        pos = filled_df.index.get_loc(idx)\n",
    "                        neighbors = []\n",
    "                        \n",
    "                        # Try to get value before (if not first row)\n",
    "                        if pos > 0:\n",
    "                            prev_val = wide_df[col].iloc[pos-1]\n",
    "                            if not pd.isna(prev_val):\n",
    "                                neighbors.append(prev_val)\n",
    "                        \n",
    "                        # Try to get value after (if not last row)\n",
    "                        if pos < len(wide_df) - 1:\n",
    "                            next_val = wide_df[col].iloc[pos+1]\n",
    "                            if not pd.isna(next_val):\n",
    "                                neighbors.append(next_val)\n",
    "                        \n",
    "                        # Calculate average if neighbors exist\n",
    "                        if neighbors:\n",
    "                            filled_values.iloc[pos] = sum(neighbors) / len(neighbors)\n",
    "                    \n",
    "                    # Assign filled values back to the dataframe\n",
    "                    filled_df[col] = filled_values\n",
    "                    \n",
    "                    # For any remaining NA values (if both neighbors were NA), use forward/backward fill\n",
    "                    if filled_df[col].isna().any():\n",
    "                        filled_df[col] = filled_df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "            # Save filled version to a separate file\n",
    "            filled_output = \"consolidated_carbon_intensity_march24_filled.csv\"\n",
    "            filled_df.to_csv(filled_output, index=False)\n",
    "            print(f\"\\nFilled data (with neighbor averaging) saved to {filled_output}\")\n",
    "        else:\n",
    "            print(\"\\nNo missing values detected in the consolidated data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7985ba33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
